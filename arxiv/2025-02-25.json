[
    {
        "id": "1",
        "title": "An Agent Framework for Real-Time Financial Information Searching with Large Language Models",
        "author": [
            "Jinzheng Li",
            "Jingshu Zhang",
            "Hongguang Li",
            "Yiqing Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15684",
        "abstract": "Financial decision-making requires processing vast amounts of real-time information while understanding their complex temporal relationships. While traditional search engines excel at providing real-time information access, they often struggle to comprehend sophisticated user intentions and contextual nuances. Conversely, Large Language Models (LLMs) demonstrate reasoning and interaction capabilities but may generate unreliable outputs without access to current data. While recent attempts have been made to combine LLMs with search capabilities, they suffer from (1) restricted access to specialized financial data, (2) static query structures that cannot adapt to dynamic market conditions, and (3) insufficient temporal awareness in result generation. To address these challenges, we present FinSearch, a novel agent-based search framework specifically designed for financial applications that interface with diverse financial data sources including market, stock, and news data. Innovatively, FinSearch comprises four components: (1) an LLM-based multi-step search pre-planner that decomposes user queries into structured sub-queries mapped to specific data sources through a graph representation; (2) a search executor with an LLM-based adaptive query rewriter that executes the searching of each sub-query while dynamically refining the sub-queries in its subsequent node based on intermediate search results; (3) a temporal weighting mechanism that prioritizes information relevance based on the deduced time context from the user's query; (4) an LLM-based response generator that synthesizes results into coherent, contextually appropriate outputs. To evaluate FinSearch, we construct FinSearchBench-24, a benchmark of 1,500 four-choice questions across the stock market, rate changes, monetary policy, and industry developments spanning from June to October 2024.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "2",
        "title": "Level-Navi Agent: A Framework and benchmark for Chinese Web Search Agents",
        "author": [
            "Chuanrui Hu",
            "Shichong Xie",
            "Baoxin Wang",
            "Bin Chen",
            "Xiaofeng Cong",
            "Jun Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15690",
        "abstract": "Large language models (LLMs), adopted to understand human language, drive the development of artificial intelligence (AI) web search agents. Compared to traditional search engines, LLM-powered AI search agents are capable of understanding and responding to complex queries with greater depth, enabling more accurate operations and better context recognition. However, little attention and effort has been paid to the Chinese web search, which results in that the capabilities of open-source models have not been uniformly and fairly evaluated. The difficulty lies in lacking three aspects: an unified agent framework, an accurately labeled dataset, and a suitable evaluation metric. To address these issues, we propose a general-purpose and training-free web search agent by level-aware navigation, Level-Navi Agent, accompanied by a well-annotated dataset (Web24) and a suitable evaluation metric. Level-Navi Agent can think through complex user questions and conduct searches across various levels on the internet to gather information for questions. Meanwhile, we provide a comprehensive evaluation of state-of-the-art LLMs under fair settings. To further facilitate future research, source code is available at Github.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "3",
        "title": "The Synergy of Automated Pipelines with Prompt Engineering and Generative AI in Web Crawling",
        "author": [
            "Chau-Jian Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15691",
        "abstract": "Web crawling is a critical technique for extracting online data, yet it poses challenges due to webpage diversity and anti-scraping mechanisms. This study investigates the integration of generative AI tools Claude AI (Sonnet 3.5) and ChatGPT4.0 with prompt engineering to automate web scraping. Using two prompts, PROMPT I (general inference, tested on Yahoo News) and PROMPT II (element-specific, tested on http://Coupons.com), we evaluate the code quality and performance of AI-generated scripts. Claude AI consistently outperformed ChatGPT-4.0 in script quality and adaptability, as confirmed by predefined evaluation metrics, including functionality, readability, modularity, and robustness. Performance data were collected through manual testing and structured scoring by three evaluators. Visualizations further illustrate Claude AI's superiority. Anti-scraping solutions, including undetected_chromedriver, Selenium, and fake_useragent, were incorporated to enhance performance. This paper demonstrates how generative AI combined with prompt engineering can simplify and improve web scraping workflows.",
        "tags": [
            "ChatGPT"
        ]
    },
    {
        "id": "4",
        "title": "ACL-rlg: A Dataset for Reading List Generation",
        "author": [
            "Julien Aubert-BÃ©duchaud",
            "Florian Boudin",
            "BÃ©atrice Daille",
            "Richard Dufour"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15692",
        "abstract": "Familiarizing oneself with a new scientific field and its existing literature can be daunting due to the large amount of available articles. Curated lists of academic references, or reading lists, compiled by experts, offer a structured way to gain a comprehensive overview of a domain or a specific scientific challenge. In this work, we introduce ACL-rlg, the largest open expert-annotated reading list dataset. We also provide multiple baselines for evaluating reading list generation and formally define it as a retrieval task. Our qualitative study highlights the fact that traditional scholarly search engines and indexing methods perform poorly on this task, and GPT-4o, despite showing better results, exhibits signs of potential data contamination.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "5",
        "title": "Integrating Domain Knowledge into Large Language Models for Enhanced Fashion Recommendations",
        "author": [
            "Zhan Shi",
            "Shanglin Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15696",
        "abstract": "Fashion, deeply rooted in sociocultural dynamics, evolves as individuals emulate styles popularized by influencers and iconic figures. In the quest to replicate such refined tastes using artificial intelligence, traditional fashion ensemble methods have primarily used supervised learning to imitate the decisions of style icons, which falter when faced with distribution shifts, leading to style replication discrepancies triggered by slight variations in input. Meanwhile, large language models (LLMs) have become prominent across various sectors, recognized for their user-friendly interfaces, strong conversational skills, and advanced reasoning capabilities. To address these challenges, we introduce the Fashion Large Language Model (FLLM), which employs auto-prompt generation training strategies to enhance its capacity for delivering personalized fashion advice while retaining essential domain knowledge. Additionally, by integrating a retrieval augmentation technique during inference, the model can better adjust to individual preferences. Our results show that this approach surpasses existing models in accuracy, interpretability, and few-shot learning capabilities.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "6",
        "title": "Sustainable Digitalization of Business with Multi-Agent RAG and LLM",
        "author": [
            "Muhammad Arslan",
            "Saba Munawar",
            "Christophe Cruz"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15700",
        "abstract": "Businesses heavily rely on data sourced from various channels like news articles, financial reports, and consumer reviews to drive their operations, enabling informed decision-making and identifying opportunities. However, traditional manual methods for data extraction are often time-consuming and resource-intensive, prompting the adoption of digital transformation initiatives to enhance efficiency. Yet, concerns persist regarding the sustainability of such initiatives and their alignment with the United Nations (UN)'s Sustainable Development Goals (SDGs). This research aims to explore the integration of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) as a sustainable solution for Information Extraction (IE) and processing. The research methodology involves reviewing existing solutions for business decision-making, noting that many systems require training new machine learning models, which are resource-intensive and have significant environmental impacts. Instead, we propose a sustainable business solution using pre-existing LLMs that can work with diverse datasets. We link domain-specific datasets to tailor LLMs to company needs and employ a Multi-Agent architecture to divide tasks such as information retrieval, enrichment, and classification among specialized agents. This approach optimizes the extraction process and improves overall efficiency. Through the utilization of these technologies, businesses can optimize resource utilization, improve decision-making processes, and contribute to sustainable development goals, thereby fostering environmental responsibility within the corporate sector.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "7",
        "title": "Political Events using RAG with LLMs",
        "author": [
            "Muhammad Arslan",
            "Saba Munawar",
            "Christophe Cruz"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15701",
        "abstract": "In the contemporary digital landscape, media content stands as the foundation for political news analysis, offering invaluable insights sourced from various channels like news articles, social media updates, speeches, and reports. Natural Language Processing (NLP) has revolutionized Political Information Extraction (IE), automating tasks such as Event Extraction (EE) from these diverse media outlets. While traditional NLP methods often necessitate specialized expertise to build rule-based systems or train machine learning models with domain-specific datasets, the emergence of Large Language Models (LLMs) driven by Generative Artificial Intelligence (GenAI) presents a promising alternative. These models offer accessibility, alleviating challenges associated with model construction from scratch and reducing the dependency on extensive datasets during the training phase, thus facilitating rapid implementation. However, challenges persist in handling domain-specific tasks, leading to the development of the Retrieval-Augmented Generation (RAG) framework. RAG enhances LLMs by integrating external data retrieval, enriching their contextual understanding, and expanding their knowledge base beyond pre-existing training data. To illustrate RAG's efficacy, we introduce the Political EE system, specifically tailored to extract political event information from news articles. Understanding these political insights is essential for remaining informed about the latest political advancements, whether on a national or global scale.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "8",
        "title": "Large language models streamline automated systematic review: A preliminary study",
        "author": [
            "Xi Chen",
            "Xue Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15702",
        "abstract": "Large Language Models (LLMs) have shown promise in natural language processing tasks, with the potential to automate systematic reviews. This study evaluates the performance of three state-of-the-art LLMs in conducting systematic review tasks. We assessed GPT-4, Claude-3, and Mistral 8x7B across four systematic review tasks: study design formulation, search strategy development, literature screening, and data extraction. Sourced from a previously published systematic review, we provided reference standard including standard PICO (Population, Intervention, Comparison, Outcome) design, standard eligibility criteria, and data from 20 reference literature. Three investigators evaluated the quality of study design and eligibility criteria using 5-point Liker Scale in terms of accuracy, integrity, relevance, consistency and overall performance. For other tasks, the output is defined as accurate if it is the same as the reference standard. Search strategy performance was evaluated through accuracy and retrieval efficacy. Screening accuracy was assessed for both abstracts screening and full texts screening. Data extraction accuracy was evaluated across 1,120 data points comprising 3,360 individual fields. Claude-3 demonstrated superior overall performance in PICO design. In search strategy formulation, GPT-4 and Claude-3 achieved comparable accuracy, outperforming Mistral. For abstract screening, GPT-4 achieved the highest accuracy, followed by Mistral and Claude-3. In data extraction, GPT-4 significantly outperformed other models. LLMs demonstrate potential for automating systematic review tasks, with GPT-4 showing superior performance in search strategy formulation, literature screening and data extraction. These capabilities make them promising assistive tools for researchers and warrant further development and validation in this field.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "9",
        "title": "EMK-KEN: A High-Performance Approach for Assessing Knowledge Value in Citation Network",
        "author": [
            "Zehui Qu",
            "Chengzhi Liu",
            "Hanwen Cui",
            "Xianping Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15704",
        "abstract": "With the explosive growth of academic literature, effectively evaluating the knowledge value of literature has become quite essential. However, most of the existing methods focus on modeling the entire citation network, which is structurally complex and often suffers from long sequence dependencies when dealing with text embeddings. Thus, they might have low efficiency and poor robustness in different fields. To address these issues, a novel knowledge evaluation method is proposed, called EMK-KEN. The model consists of two modules. Specifically, the first module utilizes MetaFP and Mamba to capture semantic features of node metadata and text embeddings to learn contextual representations of each paper. The second module utilizes KAN to further capture the structural information of citation networks in order to learn the differences in different fields of networks. Extensive experiments based on ten benchmark datasets show that our method outperforms the state-of-the-art competitors in effectiveness and robustness.",
        "tags": [
            "KAN",
            "Mamba"
        ]
    },
    {
        "id": "10",
        "title": "TutorLLM: Customizing Learning Recommendations with Knowledge Tracing and Retrieval-Augmented Generation",
        "author": [
            "Zhaoxing Li",
            "Vahid Yazdanpanah",
            "Jindi Wang",
            "Wen Gu",
            "Lei Shi",
            "Alexandra I. Cristea",
            "Sarah Kiden",
            "Sebastian Stein"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15709",
        "abstract": "The integration of AI in education offers significant potential to enhance learning efficiency. Large Language Models (LLMs), such as ChatGPT, Gemini, and Llama, allow students to query a wide range of topics, providing unprecedented flexibility. However, LLMs face challenges, such as handling varying content relevance and lack of personalization. To address these challenges, we propose TutorLLM, a personalized learning recommender LLM system based on Knowledge Tracing (KT) and Retrieval-Augmented Generation (RAG). The novelty of TutorLLM lies in its unique combination of KT and RAG techniques with LLMs, which enables dynamic retrieval of context-specific knowledge and provides personalized learning recommendations based on the student's personal learning state. Specifically, this integration allows TutorLLM to tailor responses based on individual learning states predicted by the Multi-Features with Latent Relations BERT-based KT (MLFBK) model and to enhance response accuracy with a Scraper model. The evaluation includes user assessment questionnaires and performance metrics, demonstrating a 10\\% improvement in user satisfaction and a 5\\% increase in quiz scores compared to using general LLMs alone.",
        "tags": [
            "BERT",
            "ChatGPT",
            "LLMs",
            "LLaMA",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "11",
        "title": "TrustDataFilter:Leveraging Trusted Knowledge Base Data for More Effective Filtering of Unknown Information",
        "author": [
            "Jinghong Zhang",
            "Yidong Cui",
            "Weiling Wang",
            "Xianyou Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15714",
        "abstract": "With the advancement of technology and changes in the market, the demand for the construction of domain-specific knowledge bases has been increasing, either to improve model performance or to promote enterprise innovation and competitiveness. The construction of domain-specific knowledge bases typically relies on web crawlers or existing industry databases, leading to problems with accuracy and consistency of the data. To address these challenges, we considered the characteristics of domain data, where internal knowledge is interconnected, and proposed the Self-Natural Language Inference Data Filtering (self-nli-TDF) framework. This framework compares trusted filtered knowledge with the data to be filtered, deducing the reasoning relationship between them, thus improving filtering performance. The framework uses plug-and-play large language models for trustworthiness assessment and employs the RoBERTa-MNLI model from the NLI domain for reasoning. We constructed three datasets in the domains of biology, radiation, and science, and conducted experiments using RoBERTa, GPT3.5, and the local Qwen2 model. The experimental results show that this framework improves filter quality, producing more consistent and reliable filtering results.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "12",
        "title": "Open-Source Retrieval Augmented Generation Framework for Retrieving Accurate Medication Insights from Formularies for African Healthcare Workers",
        "author": [
            "Axum AI",
            "J. Owoyemi",
            "S. Abubakar",
            "A. Owoyemi",
            "T.O. Togunwa",
            "F.C. Madubuko",
            "S. Oyatoye",
            "Z. Oyetolu",
            "K. Akyea",
            "A.O. Mohammed",
            "A. Adebakin"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15722",
        "abstract": "Accessing accurate medication insights is vital for enhancing patient safety, minimizing errors, and supporting clinical decision-making. However, healthcare professionals in Africa often rely on manual and time-consuming processes to retrieve drug information, exacerbated by limited access to pharmacists due to brain drain and healthcare disparities. This paper presents \"Drug Insights,\" an open-source Retrieval-Augmented Generation (RAG) chatbot designed to streamline medication lookup for healthcare workers in Africa. By leveraging a corpus of Nigerian pharmaceutical data and advanced AI technologies, including Pinecone databases and GPT models, the system delivers accurate, context-specific responses with minimal hallucination. The chatbot integrates prompt engineering and S-BERT evaluation to optimize retrieval and response generation. Preliminary tests, including pharmacist feedback, affirm the tool's potential to improve drug information access while highlighting areas for enhancement, such as UI/UX refinement and extended corpus integration.",
        "tags": [
            "BERT",
            "GPT",
            "RAG"
        ]
    },
    {
        "id": "13",
        "title": "Instruction-Based Fine-tuning of Open-Source LLMs for Predicting Customer Purchase Behaviors",
        "author": [
            "Halil Ibrahim Ergul",
            "Selim Balcisoy",
            "Burcin Bozkaya"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15724",
        "abstract": "In this study, the performance of various predictive models, including probabilistic baseline, CNN, LSTM, and finetuned LLMs, in forecasting merchant categories from financial transaction data have been evaluated. Utilizing datasets from Bank A for training and Bank B for testing, the superior predictive capabilities of the fine-tuned Mistral Instruct model, which was trained using customer data converted into natural language format have been demonstrated. The methodology of this study involves instruction fine-tuning Mistral via LoRA (LowRank Adaptation of Large Language Models) to adapt its vast pre-trained knowledge to the specific domain of financial transactions. The Mistral model significantly outperforms traditional sequential models, achieving higher F1 scores in the three key merchant categories of bank transaction data (grocery, clothing, and gas stations) that is crucial for targeted marketing campaigns. This performance is attributed to the model's enhanced semantic understanding and adaptability which enables it to better manage minority classes and predict transaction categories with greater accuracy. These findings highlight the potential of LLMs in predicting human behavior.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "14",
        "title": "Town Hall Debate Prompting: Enhancing Logical Reasoning in LLMs through Multi-Persona Interaction",
        "author": [
            "Vivaan Sandwar",
            "Bhav Jain",
            "Rishan Thangaraj",
            "Ishaan Garg",
            "Michael Lam",
            "Kevin Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15725",
        "abstract": "Debate is a commonly used form of human communication catered towards problem-solving because of its efficiency. Debate fundamentally allows multiple viewpoints to be brought up in problem-solving, and for complex problems, each viewpoint opens a new path for problem-solving. In this work, we apply this concept to LLM decision-making by proposing town hall-style debate prompting (THDP), a prompting method that splices a language model into multiple personas that will debate one another to reach a conclusion. Our experimental pipeline varies both the number of personas and the personality types of each persona to find the optimum town hall size and personality for benchmark performance as measured by ZebraLogic bench, a reasoning-intensive benchmark characterized by both multiple-choice and fill-in-the-blank questions. Our experimental results demonstrate that a town hall size of 5 personas with LLM-determined personality types performs optimally on ZebraLogic, achieving a 13\\% improvement over one-shot CoT baselines in per-cell accuracy in GPT-4o, 9% puzzle accuracy increase in Claude 3.5 Sonnet, and an improvement in hard puzzle accuracy from 10-15%.",
        "tags": [
            "GPT",
            "LLMs"
        ]
    },
    {
        "id": "15",
        "title": "Cache-Craft: Managing Chunk-Caches for Efficient Retrieval-Augmented Generation",
        "author": [
            "Shubham Agarwal",
            "Sai Sundaresan",
            "Subrata Mitra",
            "Debabrata Mahapatra",
            "Archit Gupta",
            "Rounak Sharma",
            "Nirmal Joshua Kapu",
            "Tong Yu",
            "Shiv Saini"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15734",
        "abstract": "Retrieval-Augmented Generation (RAG) is often used with Large Language Models (LLMs) to infuse domain knowledge or user-specific information. In RAG, given a user query, a retriever extracts chunks of relevant text from a knowledge base. These chunks are sent to an LLM as part of the input prompt. Typically, any given chunk is repeatedly retrieved across user questions. However, currently, for every question, attention-layers in LLMs fully compute the key values (KVs) repeatedly for the input chunks, as state-of-the-art methods cannot reuse KV-caches when chunks appear at arbitrary locations with arbitrary contexts. Naive reuse leads to output quality degradation. This leads to potentially redundant computations on expensive GPUs and increases latency. In this work, we propose Cache-Craft, a system for managing and reusing precomputed KVs corresponding to the text chunks (we call chunk-caches) in RAG-based systems. We present how to identify chunk-caches that are reusable, how to efficiently perform a small fraction of recomputation to fix the cache to maintain output quality, and how to efficiently store and evict chunk-caches in the hardware for maximizing reuse while masking any overheads. With real production workloads as well as synthetic datasets, we show that Cache-Craft reduces redundant computation by 51% over SOTA prefix-caching and 75% over full recomputation. Additionally, with continuous batching on a real production workload, we get a 1.6X speed up in throughput and a 2X reduction in end-to-end response latency over prefix-caching while maintaining quality, for both the LLaMA-3-8B and LLaMA-3-70B models.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "16",
        "title": "Detecting Content Rating Violations in Android Applications: A Vision-Language Approach",
        "author": [
            "D. Denipitiyage",
            "B. Silva",
            "S. Seneviratne",
            "A. Seneviratne",
            "S. Chawla"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15739",
        "abstract": "Despite regulatory efforts to establish reliable content-rating guidelines for mobile apps, the process of assigning content ratings in the Google Play Store remains self-regulated by the app developers. There is no straightforward method of verifying developer-assigned content ratings manually due to the overwhelming scale or automatically due to the challenging problem of interpreting textual and visual data and correlating them with content ratings. We propose and evaluate a visionlanguage approach to predict the content ratings of mobile game applications and detect content rating violations, using a dataset of metadata of popular Android games. Our method achieves ~6% better relative accuracy compared to the state-of-the-art CLIP-fine-tuned model in a multi-modal setting. Applying our classifier in the wild, we detected more than 70 possible cases of content rating violations, including nine instances with the 'Teacher Approved' badge. Additionally, our findings indicate that 34.5% of the apps identified by our classifier as violating content ratings were removed from the Play Store. In contrast, the removal rate for correctly classified apps was only 27%. This discrepancy highlights the practical effectiveness of our classifier in identifying apps that are likely to be removed based on user complaints.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "17",
        "title": "Detection of LLM-Generated Java Code Using Discretized Nested Bigrams",
        "author": [
            "Timothy Paek",
            "Chilukuri Mohan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15740",
        "abstract": "Large Language Models (LLMs) are currently used extensively to generate code by professionals and students, motivating the development of tools to detect LLM-generated code for applications such as academic integrity and cybersecurity. We address this authorship attribution problem as a binary classification task along with feature identification and extraction. We propose new Discretized Nested Bigram Frequency features on source code groups of various sizes. Compared to prior work, improvements are obtained by representing sparse information in dense membership bins. Experimental evaluation demonstrated that our approach significantly outperformed a commonly used GPT code-detection API and baseline features, with accuracy exceeding 96% compared to 72% and 79% respectively in detecting GPT-rewritten Java code fragments for 976 files with GPT 3.5 and GPT4 using 12 features. We also outperformed three prior works on code author identification in a 40-author dataset. Our approach scales well to larger data sets, and we achieved 99% accuracy and 0.999 AUC for 76,089 files and over 1,000 authors with GPT 4o using 227 features.",
        "tags": [
            "Detection",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "18",
        "title": "On the Effectiveness of Large Language Models in Automating Categorization of Scientific Texts",
        "author": [
            "Gautam Kishore Shahi",
            "Oliver Hummel"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15745",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has led to a multitude of application opportunities. One traditional task for Information Retrieval systems is the summarization and classification of texts, both of which are important for supporting humans in navigating large literature bodies as they e.g. exist with scientific publications. Due to this rapidly growing body of scientific knowledge, recent research has been aiming at building research information systems that not only offer traditional keyword search capabilities, but also novel features such as the automatic detection of research areas that are present at knowledge intensive organizations in academia and industry. To facilitate this idea, we present the results obtained from evaluating a variety of LLMs in their ability to sort scientific publications into hierarchical classifications systems. Using the FORC dataset as ground truth data, we have found that recent LLMs (such as Meta Llama 3.1) are able to reach an accuracy of up to 0.82, which is up to 0.08 better than traditional BERT models.",
        "tags": [
            "BERT",
            "Detection",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "19",
        "title": "CacheMamba: Popularity Prediction for Mobile Edge Caching Networks via Selective State Spaces",
        "author": [
            "Ghazaleh Kianfar",
            "Zohreh Hajiakhondi-Meybodi",
            "Arash Mohammadi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15746",
        "abstract": "Mobile Edge Caching (MEC) plays a pivotal role in mitigating latency in data-intensive services by dynamically caching frequently requested content on edge servers. This capability is critical for applications such as Augmented Reality (AR), Virtual Reality (VR), and Autonomous Vehicles (AV), where efficient content caching and accurate popularity prediction are essential for optimizing performance. In this paper, we explore the problem of popularity prediction in MEC by utilizing historical time-series request data of intended files, formulating this problem as a ranking task. To this aim, we propose CacheMamba model by employing Mamba, a state-space model (SSM)-based architecture, to identify the top-K files with the highest likelihood of being requested. We then benchmark the proposed model against a Transformer-based approach, demonstrating its superior performance in terms of cache-hit rate, Mean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG), and Floating-Point Operations Per Second (FLOPS), particularly when dealing with longer sequences.",
        "tags": [
            "Mamba",
            "Selective State Spaces",
            "Transformer"
        ]
    },
    {
        "id": "20",
        "title": "TCProF:Time-Complexity Prediction SSL Framework",
        "author": [
            "Joonghyuk Hahn",
            "Hyeseon Ahn",
            "Jungin Kim",
            "Soohan Lim",
            "Yo-Sub Han"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15749",
        "abstract": "Time complexity is a theoretic measure to determine the amount of time the algorithm needs for its execution. In reality, developers write algorithms into code snippets within limited resources, making the calculation of a code's time complexity a fundamental task. However, determining the precise time complexity of a code is theoretically undecidable. In response, recent advancements have leaned toward deploying datasets for code time complexity prediction and initiating preliminary experiments for this challenge. We investigate the challenge in low-resource scenarios where only a few labeled instances are given for training. Remarkably, we are the first to introduce TCProF: a Time-Complexity Prediction SSL Framework as an effective solution for code time complexity prediction in low-resource settings. TCProF significantly boosts performance by integrating our augmentation, symbolic modules, and a co-training mechanism, achieving a more than 60% improvement over self-training approaches. We further provide an extensive comparative analysis between TCProF, ChatGPT, and Gemini-Pro, offering a detailed evaluation of our approach.",
        "tags": [
            "ChatGPT"
        ]
    },
    {
        "id": "21",
        "title": "Text2Net: Transforming Plain-text To A Dynamic Interactive Network Simulation Environment",
        "author": [
            "Alireza Marefat",
            "Abbaas Alif Mohamed Nishar",
            "Ashwin Ashok"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15754",
        "abstract": "This paper introduces Text2Net, an innovative text-based network simulation engine that leverages natural language processing (NLP) and large language models (LLMs) to transform plain-text descriptions of network topologies into dynamic, interactive simulations. Text2Net simplifies the process of configuring network simulations, eliminating the need for users to master vendor-specific syntaxes or navigate complex graphical interfaces. Through qualitative and quantitative evaluations, we demonstrate Text2Net's ability to significantly reduce the time and effort required to deploy network scenarios compared to traditional simulators like EVE-NG. By automating repetitive tasks and enabling intuitive interaction, Text2Net enhances accessibility for students, educators, and professionals. The system facilitates hands-on learning experiences for students that bridge the gap between theoretical knowledge and practical application. The results showcase its scalability across various network complexities, marking a significant step toward revolutionizing network education and professional use cases, such as proof-of-concept testing.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "22",
        "title": "LoXR: Performance Evaluation of Locally Executing LLMs on XR Devices",
        "author": [
            "Dawar Khan",
            "Xinyu Liu",
            "Omar Mena",
            "Donggang Jia",
            "Alexandre Kouyoumdjian",
            "Ivan Viola"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15761",
        "abstract": "The deployment of large language models (LLMs) on extended reality (XR) devices has great potential to advance the field of human-AI interaction. In the case of direct, on-device model inference, selecting the appropriate model and device for specific tasks remains challenging. In this paper, we deploy 17 LLMs across four XR devices--Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and Apple Vision Pro, and conduct a comprehensive evaluation. We devise an experimental setup and evaluate performance on four key metrics: performance consistency, processing speed, memory usage, and battery consumption. For each of the 68 model-device pairs, we assess performance under varying string lengths, batch sizes, and thread counts, analyzing the trade-offs for real-time XR applications. We finally propose a unified evaluation method based on the Pareto Optimality theory to select the optimal device-model pairs from the quality and speed objectives. We believe our findings offer valuable insights to guide future optimization efforts for LLM deployment on XR devices. Our evaluation method can be followed as standard groundwork for further research and development in this emerging field. All supplemental materials are available at http://www.nanovis.org/Loxr.html.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "23",
        "title": "Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization",
        "author": [
            "Bowen Pang",
            "Kai Li",
            "Ruifeng She",
            "Feifan Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15763",
        "abstract": "With the development of large language models (LLMs), it has become increasingly important to optimize hardware usage and improve throughput. In this paper, we study the inference optimization of the serving system that deploys LLMs. To optimize system throughput and maximize hardware utilization, we formulate the inference optimization problem as a mixed-integer programming (MIP) model and propose a hybrid offline-online method as solution. The offline method improves large-scale inference systems by introducing a Minimizing Makespan Bin Packing Problem. We further provide a theoretical lower bound computation method. Then, we propose an online sorting and preemptive scheduling method to better utilize hardware. In the online iteration scheduling process, a Lagrangian method is applied to evaluate the cost efficiency of inserting prefill stages versus decode stages at each iteration and dynamically determine when to preempt decoding tasks and insert prefill tasks. Experiments using real-world data from the LLaMA-65B model and the GSM8K dataset demonstrate that system utilization improves from 80.2% to 89.1%, and the total inference time decreases from 201.00 to 190.58 seconds. A 100-cases study shows that our method consistently outperforms the baseline method and improves the utilization rate by 8.0% on average. Finally, we discuss potential future extensions, including stochastic modeling, reinforcement learning-based schedulers, and dynamic decision-making strategies for system throughput and hardware utilization.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "24",
        "title": "Generalized Attention Flow: Feature Attribution for Transformer Models via Maximum Flow",
        "author": [
            "Behrooz Azarkhalili",
            "Maxwell Libbrecht"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15765",
        "abstract": "This paper introduces Generalized Attention Flow (GAF), a novel feature attribution method for Transformer-based models to address the limitations of current approaches. By extending Attention Flow and replacing attention weights with the generalized Information Tensor, GAF integrates attention weights, their gradients, the maximum flow problem, and the barrier method to enhance the performance of feature attributions. The proposed method exhibits key theoretical properties and mitigates the shortcomings of prior techniques that rely solely on simple aggregation of attention weights. Our comprehensive benchmarking on sequence classification tasks demonstrates that a specific variant of GAF consistently outperforms state-of-the-art feature attribution methods in most evaluation settings, providing a more reliable interpretation of Transformer model outputs.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "25",
        "title": "Performance Review on LLM for solving leetcode problems",
        "author": [
            "Lun Wang",
            "Chuanqi Shi",
            "Shaoshui Du",
            "Yiyi Tao",
            "Yixian Shen",
            "Hang Zheng",
            "Xinyu Qiu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15770",
        "abstract": "This paper presents a comprehensive performance evaluation of Large Language Models (LLMs) in solving programming challenges from Leetcode, a widely used platform for algorithm practice and technical interviews. We began by crawling the Leetcode website to collect a diverse set of problems encompassing various difficulty levels and topics. Using this dataset, we generated solutions with multiple LLMs, including GPT-4 and GPT-3.5-turbo (ChatGPT-turbo). The generated solutions were systematically evaluated for correctness and efficiency. We employed the pass@k metric to assess the success rates within a given number of attempts and analyzed the runtime performance of the solutions. Our results highlight the strengths and limitations of current LLMs [10] in code generation and problem-solving tasks, providing insights into their potential applications and areas for improvement in automated programming assistance.",
        "tags": [
            "ChatGPT",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "26",
        "title": "Learning to Reason from Feedback at Test-Time",
        "author": [
            "Yanyang Li",
            "Michael Lyu",
            "Liwei Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15771",
        "abstract": "Solving complex tasks in a single attempt is challenging for large language models (LLMs). Iterative interaction with the environment and feedback is often required to achieve success, making effective feedback utilization a critical topic. Existing approaches either struggle with length generalization or rely on naive retries without leveraging prior information. In this paper, we introduce FTTT, a novel paradigm that formulates feedback utilization as an optimization problem at test time. Additionally, we propose a learnable test-time optimizer, OpTune, to effectively exploit feedback. Experiments on two LLMs across four reasoning datasets demonstrate that FTTT and OpTune achieve superior scalability and performance.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "27",
        "title": "Logic.py: Bridging the Gap between LLMs and Constraint Solvers",
        "author": [
            "Pascal Kesseli",
            "Peter O'Hearn",
            "Ricardo Silveira Cabral"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15776",
        "abstract": "We present a novel approach to formalise and solve search-based problems using large language models, which significantly improves upon previous state-of-the-art results. We demonstrate the efficacy of this approach on the logic puzzles benchmark ZebraLogicBench. Instead of letting the LLM attempt to directly solve the puzzles, our method prompts the model to formalise the problem in a logic-focused domain-specific language (DSL) called http://Logic.py. This formalised representation is then solved using a constraint solver, leveraging the strengths of both the language model and the solver. Our approach achieves a remarkable 65% absolute improvement over the baseline performance of Llama 3.1 70B on ZebraLogicBench, setting a new state-of-the-art with an accuracy of over 90%. This significant advancement demonstrates the potential of combining language models with domain-specific languages and auxiliary tools on traditionally challenging tasks for LLMs.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "28",
        "title": "One for All: A General Framework of LLMs-based Multi-Criteria Decision Making on Human Expert Level",
        "author": [
            "Hui Wang",
            "Fafa Zhang",
            "Chaoxu Mu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15778",
        "abstract": "Multi-Criteria Decision Making~(MCDM) is widely applied in various fields, using quantitative and qualitative analyses of multiple levels and attributes to support decision makers in making scientific and rational decisions in complex scenarios. However, traditional MCDM methods face bottlenecks in high-dimensional problems. Given the fact that Large Language Models~(LLMs) achieve impressive performance in various complex tasks, but limited work evaluates LLMs in specific MCDM problems with the help of human domain experts, we further explore the capability of LLMs by proposing an LLM-based evaluation framework to automatically deal with general complex MCDM problems. Within the framework, we assess the performance of various typical open-source models, as well as commercial models such as Claude and ChatGPT, on 3 important applications, these models can only achieve around 60\\% accuracy rate compared to the evaluation ground truth. Upon incorporation of Chain-of-Thought or few-shot prompting, the accuracy rates rise to around 70\\%, and highly depend on the model. In order to further improve the performance, a LoRA-based fine-tuning technique is employed. The experimental results show that the accuracy rates for different applications improve significantly to around 95\\%, and the performance difference is trivial between different models, indicating that LoRA-based fine-tuned LLMs exhibit significant and stable advantages in addressing MCDM tasks and can provide human-expert-level solutions to a wide range of MCDM challenges.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "29",
        "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer",
        "author": [
            "Euntae Choi",
            "Sumin Song",
            "Woosang Lim",
            "Sungjoo Yoo"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15779",
        "abstract": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training (QAT) approach that first realizes extreme compression of LLMs with W2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP integrates recent rotation techniques with a novel non-uniform weight quantizer design, by quantitatively analyzing the impact of random rotation on 2-bit weight quantization. Our weight quantizer features Learnable Direct Partitioning (LDP), which introduces learnable parameters to directly learn non-uniform intervals jointly with LLM weights. We also present a specialized GPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP can compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and 5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging mobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and MetaMath-7B with no critical problems such as convergence failure and repetition. Code will be made available at blind_review.",
        "tags": [
            "CLIP",
            "LLMs",
            "LLaMA"
        ]
    },
    {
        "id": "30",
        "title": "Self-Supervised Transformers as Iterative Solution Improvers for Constraint Satisfaction",
        "author": [
            "Yudong W. Xu",
            "Wenhao Li",
            "Scott Sanner",
            "Elias B. Khalil"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15794",
        "abstract": "We present a Transformer-based framework for Constraint Satisfaction Problems (CSPs). CSPs find use in many applications and thus accelerating their solution with machine learning is of wide interest. Most existing approaches rely on supervised learning from feasible solutions or reinforcement learning, paradigms that require either feasible solutions to these NP-Complete CSPs or large training budgets and a complex expert-designed reward signal. To address these challenges, we propose ConsFormer, a self-supervised framework that leverages a Transformer as a solution refiner. ConsFormer constructs a solution to a CSP iteratively in a process that mimics local search. Instead of using feasible solutions as labeled data, we devise differentiable approximations to the discrete constraints of a CSP to guide model training. Our model is trained to improve random assignments for a single step but is deployed iteratively at test time, circumventing the bottlenecks of supervised and reinforcement learning. Our method can tackle out-of-distribution CSPs simply through additional iterations.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "31",
        "title": "Lean-ing on Quality: How High-Quality Data Beats Diverse Multilingual Data in AutoFormalization",
        "author": [
            "Willy Chan",
            "Michael Souliman",
            "Jakob Nordhagen",
            "Brando Miranda",
            "Elyas Obbad",
            "Kai Fronsdal Sanmi Koyejo"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15795",
        "abstract": "Autoformalization, the process of transforming informal mathematical language into formal specifications and proofs remains a difficult task for state-of-the-art (large) language models. Existing works point to competing explanations for the performance gap. To this end, we introduce a novel methodology that leverages back-translation with hand-curated prompts to enhance the mathematical capabilities of language models, particularly addressing the challenge posed by the scarcity of labeled data. Specifically, we evaluate three primary variations of this strategy: (1) on-the-fly (online) backtranslation, (2) distilled (offline) backtranslation with few-shot amplification, and (3) line-by-line proof analysis integrated with proof state information. Each variant is designed to optimize data quality over quantity, focusing on the high fidelity of generated proofs rather than sheer data scale. Our findings provide evidence that employing our proposed approaches to generate synthetic data, which prioritizes quality over volume, improves the Autoformalization performance of LLMs as measured by standard benchmarks such as ProofNet. Crucially, our approach outperforms pretrained models using a minimal number of tokens. We also show, through strategic prompting and backtranslation, that our approaches surpass the performance of fine-tuning with extensive multilingual datasets such as MMA on ProofNet with only 1/150th of the tokens. Taken together, our methods show a promising new approach to significantly reduce the resources required to formalize proofs, thereby accelerating AI for math.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "32",
        "title": "Pruning as a Defense: Reducing Memorization in Large Language Models",
        "author": [
            "Mansi Gupta",
            "Nikhar Waghela",
            "Sarthak Gupta",
            "Shourya Goel",
            "Sanjif Shanmugavelu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15796",
        "abstract": "Large language models have been shown to memorize significant portions of their training data, which they can reproduce when appropriately prompted. This work investigates the impact of simple pruning techniques on this behavior. Our findings reveal that pruning effectively reduces the extent of memorization in LLMs, demonstrating its potential as a foundational approach for mitigating membership inference attacks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "33",
        "title": "OCCULT: Evaluating Large Language Models for Offensive Cyber Operation Capabilities",
        "author": [
            "Michael Kouremetis",
            "Marissa Dotter",
            "Alex Byrne",
            "Dan Martin",
            "Ethan Michalak",
            "Gianpaolo Russo",
            "Michael Threet",
            "Guido Zarrella"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15797",
        "abstract": "The prospect of artificial intelligence (AI) competing in the adversarial landscape of cyber security has long been considered one of the most impactful, challenging, and potentially dangerous applications of AI. Here, we demonstrate a new approach to assessing AI's progress towards enabling and scaling real-world offensive cyber operations (OCO) tactics in use by modern threat actors. We detail OCCULT, a lightweight operational evaluation framework that allows cyber security experts to contribute to rigorous and repeatable measurement of the plausible cyber security risks associated with any given large language model (LLM) or AI employed for OCO. We also prototype and evaluate three very different OCO benchmarks for LLMs that demonstrate our approach and serve as examples for building benchmarks under the OCCULT framework. Finally, we provide preliminary evaluation results to demonstrate how this framework allows us to move beyond traditional all-or-nothing tests, such as those crafted from educational exercises like capture-the-flag environments, to contextualize our indicators and warnings in true cyber threat scenarios that present risks to modern infrastructure. We find that there has been significant recent advancement in the risks of AI being used to scale realistic cyber threats. For the first time, we find a model (DeepSeek-R1) is capable of correctly answering over 90% of challenging offensive cyber knowledge tests in our Threat Actor Competency Test for LLMs (TACTL) multiple-choice benchmarks. We also show how Meta's Llama and Mistral's Mixtral model families show marked performance improvements over earlier models against our benchmarks where LLMs act as offensive agents in MITRE's high-fidelity offensive and defensive cyber operations simulation environment, CyberLayer.",
        "tags": [
            "DeepSeek",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "34",
        "title": "Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models",
        "author": [
            "Artyom Kharinaev",
            "Viktor Moskvoretskii",
            "Egor Shvetsov",
            "Kseniia Studenikina",
            "Bykov Mikhail",
            "Evgeny Burnaev"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15799",
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools for addressing modern challenges and enabling practical applications. However, their computational expense remains a significant barrier to widespread adoption. Quantization has emerged as a promising technique to democratize access and enable low resource device deployment. Despite these advancements, the safety and trustworthiness of quantized models remain underexplored, as prior studies often overlook contemporary architectures and rely on overly simplistic benchmarks and evaluations. To address this gap, we introduce OpenSafetyMini, a novel open-ended safety dataset designed to better distinguish between models. We evaluate 4 state-of-the-art quantization techniques across LLaMA and Mistral models using 4 benchmarks, including human evaluations. Our findings reveal that the optimal quantization method varies for 4-bit precision, while vector quantization techniques deliver the best safety and trustworthiness performance at 2-bit precision, providing foundation for future research.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models",
            "Vector Quantization"
        ]
    },
    {
        "id": "35",
        "title": "An explainable transformer circuit for compositional generalization",
        "author": [
            "Cheng Tang",
            "Brenden Lake",
            "Mehrdad Jazayeri"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15801",
        "abstract": "Compositional generalization-the systematic combination of known components into novel structures-remains a core challenge in cognitive science and machine learning. Although transformer-based large language models can exhibit strong performance on certain compositional tasks, the underlying mechanisms driving these abilities remain opaque, calling into question their interpretability. In this work, we identify and mechanistically interpret the circuit responsible for compositional induction in a compact transformer. Using causal ablations, we validate the circuit and formalize its operation using a program-like description. We further demonstrate that this mechanistic understanding enables precise activation edits to steer the model's behavior predictably. Our findings advance the understanding of complex behaviors in transformers and highlight such insights can provide a direct pathway for model control.",
        "tags": [
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "36",
        "title": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference",
        "author": [
            "Bingzhe Zhao",
            "Ke Cheng",
            "Aomufei Yuan",
            "Yuxuan Tian",
            "Ruiguang Zhong",
            "Chengchen Hu",
            "Tong Yang",
            "Lian Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15804",
        "abstract": "KV cache techniques in Transformer models aim to reduce redundant computations at the expense of substantially increased memory usage, making KV cache compression an important and popular research topic. Recently, state-of-the-art KV cache compression methods implement imbalanced, per-head allocation algorithms that dynamically adjust the KV cache budget for each attention head, achieving excellent performance in single-GPU scenarios. However, we observe that such imbalanced compression leads to significant load imbalance when deploying multi-GPU inference, as some GPUs become overburdened while others remain underutilized. In this paper, we propose FairKV, a method designed to ensure fair memory usage among attention heads in systems employing imbalanced KV cache compression. The core technique of FairKV is Fair-Copying, which replicates a small subset of memory-intensive attention heads across GPUs using data parallelism to mitigate load imbalance. Our experiments on popular models, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV increases throughput by 1.66x compared to standard tensor parallelism inference. Our code will be released as open source upon acceptance.",
        "tags": [
            "LLaMA",
            "Transformer"
        ]
    },
    {
        "id": "37",
        "title": "A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos",
        "author": [
            "Yang Yao",
            "Xuan Tong",
            "Ruofan Wang",
            "Yixu Wang",
            "Lujundong Li",
            "Liang Liu",
            "Yan Teng",
            "Yingchun Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15806",
        "abstract": "Large Reasoning Models (LRMs) have significantly advanced beyond traditional Large Language Models (LLMs) with their exceptional logical reasoning capabilities, yet these improvements introduce heightened safety risks. When subjected to jailbreak attacks, their ability to generate more targeted and organized content can lead to greater harm. Although some studies claim that reasoning enables safer LRMs against existing LLM attacks, they overlook the inherent flaws within the reasoning process itself. To address this gap, we propose the first jailbreak attack targeting LRMs, exploiting their unique vulnerabilities stemming from the advanced reasoning capabilities. Specifically, we introduce a Chaos Machine, a novel component to transform attack prompts with diverse one-to-one mappings. The chaos mappings iteratively generated by the machine are embedded into the reasoning chain, which strengthens the variability and complexity and also promotes a more robust attack. Based on this, we construct the Mousetrap framework, which makes attacks projected into nonlinear-like low sample spaces with mismatched generalization enhanced. Also, due to the more competing objectives, LRMs gradually maintain the inertia of unpredictable iterative reasoning and fall into our trap. Success rates of the Mousetrap attacking o1-mini, claude-sonnet and gemini-thinking are as high as 96%, 86% and 98% respectively on our toxic dataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench, attacking claude-sonnet, well-known for its safety, Mousetrap can astonishingly achieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This paper contains inappropriate, offensive and harmful content.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "38",
        "title": "Zero-Shot Commonsense Validation and Reasoning with Large Language Models: An Evaluation on SemEval-2020 Task 4 Dataset",
        "author": [
            "Rawand Alfugaha",
            "Mohammad AL-Smadi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15810",
        "abstract": "This study evaluates the performance of Large Language Models (LLMs) on SemEval-2020 Task 4 dataset, focusing on commonsense validation and explanation. Our methodology involves evaluating multiple LLMs, including LLaMA3-70B, Gemma2-9B, and Mixtral-8x7B, using zero-shot prompting techniques. The models are tested on two tasks: Task A (Commonsense Validation), where models determine whether a statement aligns with commonsense knowledge, and Task B (Commonsense Explanation), where models identify the reasoning behind implausible statements. Performance is assessed based on accuracy, and results are compared to fine-tuned transformer-based models. The results indicate that larger models outperform previous models and perform closely to human evaluation for Task A, with LLaMA3-70B achieving the highest accuracy of 98.40% in Task A whereas, lagging behind previous models with 93.40% in Task B. However, while models effectively identify implausible statements, they face challenges in selecting the most relevant explanation, highlighting limitations in causal and inferential reasoning.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "39",
        "title": "Spiking Point Transformer for Point Cloud Classification",
        "author": [
            "Peixi Wu",
            "Bosong Chai",
            "Hebei Li",
            "Menghua Zheng",
            "Yansong Peng",
            "Zeyu Wang",
            "Xuan Nie",
            "Yueyi Zhang",
            "Xiaoyan Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15811",
        "abstract": "Spiking Neural Networks (SNNs) offer an attractive and energy-efficient alternative to conventional Artificial Neural Networks (ANNs) due to their sparse binary activation. When SNN meets Transformer, it shows great potential in 2D image processing. However, their application for 3D point cloud remains underexplored. To this end, we present Spiking Point Transformer (SPT), the first transformer-based SNN framework for point cloud classification. Specifically, we first design Queue-Driven Sampling Direct Encoding for point cloud to reduce computational costs while retaining the most effective support points at each time step. We introduce the Hybrid Dynamics Integrate-and-Fire Neuron (HD-IF), designed to simulate selective neuron activation and reduce over-reliance on specific artificial neurons. SPT attains state-of-the-art results on three benchmark datasets that span both real-world and synthetic datasets in the SNN domain. Meanwhile, the theoretical energy consumption of SPT is at least 6.4$\\times$ less than its ANN counterpart.",
        "tags": [
            "3D",
            "Transformer"
        ]
    },
    {
        "id": "40",
        "title": "InsightVision: A Comprehensive, Multi-Level Chinese-based Benchmark for Evaluating Implicit Visual Semantics in Large Vision Language Models",
        "author": [
            "Xiaofei Yin",
            "Yijie Hong",
            "Ya Guo",
            "Yi Tu",
            "Weiqiang Wang",
            "Gongshen Liu",
            "Huijia zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15812",
        "abstract": "In the evolving landscape of multimodal language models, understanding the nuanced meanings conveyed through visual cues - such as satire, insult, or critique - remains a significant challenge. Existing evaluation benchmarks primarily focus on direct tasks like image captioning or are limited to a narrow set of categories, such as humor or satire, for deep semantic understanding. To address this gap, we introduce, for the first time, a comprehensive, multi-level Chinese-based benchmark designed specifically for evaluating the understanding of implicit meanings in images. This benchmark is systematically categorized into four subtasks: surface-level content understanding, symbolic meaning interpretation, background knowledge comprehension, and implicit meaning comprehension. We propose an innovative semi-automatic method for constructing datasets, adhering to established construction protocols. Using this benchmark, we evaluate 15 open-source large vision language models (LVLMs) and GPT-4o, revealing that even the best-performing model lags behind human performance by nearly 14% in understanding implicit meaning. Our findings underscore the intrinsic challenges current LVLMs face in grasping nuanced visual semantics, highlighting significant opportunities for future research and development in this domain. We will publicly release our InsightVision dataset, code upon acceptance of the paper.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "41",
        "title": "Slamming: Training a Speech Language Model on One GPU in a Day",
        "author": [
            "Gallil Maimon",
            "Avishai Elmakies",
            "Yossi Adi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15814",
        "abstract": "We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples at - https://pages.cs.huji.ac.il/adiyoss-lab/slamming .",
        "tags": [
            "SLAM"
        ]
    },
    {
        "id": "42",
        "title": "Theoretical Physics Benchmark (TPBench) -- a Dataset and Study of AI Reasoning Capabilities in Theoretical Physics",
        "author": [
            "Daniel J.H. Chung",
            "Zhiqi Gao",
            "Yurii Kvasiuk",
            "Tianyi Li",
            "Moritz MÃ¼nchmeyer",
            "Maja Rudolph",
            "Frederic Sala",
            "Sai Chaitanya Tadepalli"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15815",
        "abstract": "We introduce a benchmark to evaluate the capability of AI to solve problems in theoretical physics, focusing on high-energy theory and cosmology. The first iteration of our benchmark consists of 57 problems of varying difficulty, from undergraduate to research level. These problems are novel in the sense that they do not come from public problem collections. We evaluate our data set on various open and closed language models, including o3-mini, o1, DeepSeek-R1, GPT-4o and versions of Llama and Qwen. While we find impressive progress in model performance with the most recent models, our research-level difficulty problems are mostly unsolved. We address challenges of auto-verifiability and grading, and discuss common failure modes. While currently state-of-the art models are still of limited use for researchers, our results show that AI assisted theoretical physics research may become possible in the near future. We discuss the main obstacles towards this goal and possible strategies to overcome them. The public problems and solutions, results for various models, and updates to the data set and score distribution, are available on the website of the dataset http://tpbench.org.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "43",
        "title": "Towards Robust ESG Analysis Against Greenwashing Risks: Aspect-Action Analysis with Cross-Category Generalization",
        "author": [
            "Keane Ong",
            "Rui Mao",
            "Deeksha Varshney",
            "Erik Cambria",
            "Gianmarco Mengaldo"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15821",
        "abstract": "Sustainability reports are key for evaluating companies' environmental, social and governance, ESG performance, but their content is increasingly obscured by greenwashing - sustainability claims that are misleading, exaggerated, and fabricated. Yet, existing NLP approaches for ESG analysis lack robustness against greenwashing risks, often extracting insights that reflect misleading or exaggerated sustainability claims rather than objective ESG performance. To bridge this gap, we introduce A3CG - Aspect-Action Analysis with Cross-Category Generalization, as a novel dataset to improve the robustness of ESG analysis amid the prevalence of greenwashing. By explicitly linking sustainability aspects with their associated actions, A3CG facilitates a more fine-grained and transparent evaluation of sustainability claims, ensuring that insights are grounded in verifiable actions rather than vague or misleading rhetoric. Additionally, A3CG emphasizes cross-category generalization. This ensures robust model performance in aspect-action analysis even when companies change their reports to selectively favor certain sustainability areas. Through experiments on A3CG, we analyze state-of-the-art supervised models and LLMs, uncovering their limitations and outlining key directions for future research.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "44",
        "title": "InductionBench: LLMs Fail in the Simplest Complexity Class",
        "author": [
            "Wenyue Hua",
            "Tyler Wong",
            "Sun Fei",
            "Liangming Pan",
            "Adam Jardine",
            "William Yang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15823",
        "abstract": "Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities. Coda and data are available https://github.com/Wenyueh/inductive_reasoning_benchmark.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "45",
        "title": "CoME: An Unlearning-based Approach to Conflict-free Model Editing",
        "author": [
            "Dahyun Jung",
            "Jaehyung Seo",
            "Jaewook Lee",
            "Chanjun Park",
            "Heuiseok Lim"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15826",
        "abstract": "Large language models (LLMs) often retain outdated or incorrect information from pre-training, which undermines their reliability. While model editing methods have been developed to address such errors without full re-training, they frequently suffer from knowledge conflicts, where outdated information interferes with new knowledge. In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features. Through experiments on GPT-J and LLaMA-3 using Counterfact and ZsRE datasets, we demonstrate that CoME improves both editing accuracy and model reliability when applied to existing editing methods. Our results highlight that the targeted removal of outdated knowledge is crucial for enhancing model editing effectiveness and maintaining the model's generative performance.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "46",
        "title": "A Stronger Mixture of Low-Rank Experts for Fine-Tuning Foundation Models",
        "author": [
            "Mengyang Sun",
            "Yihao Wang",
            "Tao Feng",
            "Dan Zhang",
            "Yifan Zhu",
            "Jie Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15828",
        "abstract": "In order to streamline the fine-tuning of foundation models, Low-Rank Adapters (LoRAs) have been substantially adopted across various fields, including instruction tuning and domain adaptation. The underlying concept of LoRA involves decomposing a full-rank matrix into the product of two lower-rank matrices, which reduces storage consumption and accelerates the training process. Furthermore, to address the limited expressive capacity of LoRA, the Mixture-of-Expert (MoE) has been introduced for incorporating multiple LoRA adapters. The integration of LoRA experts leads to a visible improvement across several downstream scenes. However, the mixture of LoRAs (MoE-LoRA) still exhibits its low robustness during tuning and inferring. Inspired by the Riemannian Preconditioners which train LoRA as a sub-space projector, we propose a new training strategy for MoE-LoRA, to stabilize and boost its feature learning procedure by multi-space projections. Examinations on SGD and AdamW optimizers demonstrate the effectiveness of our methodology. Source code is available at https://github.com/THUDM/MoELoRA_Riemannian.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "47",
        "title": "DeepRTL: Bridging Verilog Understanding and Generation with a Unified Representation Model",
        "author": [
            "Yi Liu",
            "Changran Xu",
            "Yunhao Zhou",
            "Zeju Li",
            "Qiang Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15832",
        "abstract": "Recent advancements in large language models (LLMs) have shown significant potential for automating hardware description language (HDL) code generation from high-level natural language instructions. While fine-tuning has improved LLMs' performance in hardware design tasks, prior efforts have largely focused on Verilog generation, overlooking the equally critical task of Verilog understanding. Furthermore, existing models suffer from weak alignment between natural language descriptions and Verilog code, hindering the generation of high-quality, synthesizable designs. To address these issues, we present DeepRTL, a unified representation model that excels in both Verilog understanding and generation. Based on CodeT5+, DeepRTL is fine-tuned on a comprehensive dataset that aligns Verilog code with rich, multi-level natural language descriptions. We also introduce the first benchmark for Verilog understanding and take the initiative to apply embedding similarity and GPT Score to evaluate the models' understanding capabilities. These metrics capture semantic similarity more accurately than traditional methods like BLEU and ROUGE, which are limited to surface-level n-gram overlaps. By adapting curriculum learning to train DeepRTL, we enable it to significantly outperform GPT-4 in Verilog understanding tasks, while achieving performance on par with OpenAI's o1-preview model in Verilog generation tasks.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "48",
        "title": "Pragmatic Reasoning improves LLM Code Generation",
        "author": [
            "Zhuchen Cao",
            "Sven Apel",
            "Adish Singla",
            "Vera Demberg"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15835",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive potential in translating natural language (NL) instructions into program code. However, user instructions often contain inherent ambiguities, making it challenging for LLMs to generate code that accurately reflects the user's true intent. To address this challenge, researchers have proposed to produce multiple candidates of the program code and then rerank them to identify the best solution. In this paper, we propose CodeRSA, a novel code candidate reranking mechanism built upon the Rational Speech Act (RSA) framework, designed to guide LLMs toward more comprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using one of the latest LLMs on a popular code generation dataset. Our experiment results show that CodeRSA consistently outperforms common baselines, surpasses the state-of-the-art approach in most cases, and demonstrates robust overall performance. These findings underscore the effectiveness of integrating pragmatic reasoning into code candidate reranking, offering a promising direction for enhancing code generation quality in LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "49",
        "title": "Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models",
        "author": [
            "Haokun Chen",
            "Sebastian Szyller",
            "Weilin Xu",
            "Nageen Himayat"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15836",
        "abstract": "Large language models (LLMs) have become increasingly popular. Their emergent capabilities can be attributed to their massive training datasets. However, these datasets often contain undesirable or inappropriate content, e.g., harmful texts, personal information, and copyrighted material. This has promoted research into machine unlearning that aims to remove information from trained models. In particular, approximate unlearning seeks to achieve information removal by strategically editing the model rather than complete model retraining.\nRecent work has shown that soft token attacks (STA) can successfully extract purportedly unlearned information from LLMs, thereby exposing limitations in current unlearning methodologies. In this work, we reveal that STAs are an inadequate tool for auditing unlearning. Through systematic evaluation on common unlearning benchmarks (Who Is Harry Potter? and TOFU), we demonstrate that such attacks can elicit any information from the LLM, regardless of (1) the deployed unlearning algorithm, and (2) whether the queried content was originally present in the training corpus. Furthermore, we show that STA with just a few soft tokens (1-10) can elicit random strings over 400-characters long. Thus showing that STAs are too powerful, and misrepresent the effectiveness of the unlearning methods.\nOur work highlights the need for better evaluation baselines, and more appropriate auditing tools for assessing the effectiveness of unlearning in LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "50",
        "title": "Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents",
        "author": [
            "Axel Backlund",
            "Lukas Petersson"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15840",
        "abstract": "While Large Language Models (LLMs) can exhibit impressive proficiency in isolated, short-term tasks, they often fail to maintain coherent performance over longer time horizons. In this paper, we present Vending-Bench, a simulated environment designed to specifically test an LLM-based agent's ability to manage a straightforward, long-running business scenario: operating a vending machine. Agents must balance inventories, place orders, set prices, and handle daily fees - tasks that are each simple but collectively, over long horizons (>20M tokens per run) stress an LLM's capacity for sustained, coherent decision-making. Our experiments reveal high variance in performance across multiple LLMs: Claude 3.5 Sonnet and o3-mini manage the machine well in most runs and turn a profit, but all models have runs that derail, either through misinterpreting delivery schedules, forgetting orders, or descending into tangential \"meltdown\" loops from which they rarely recover. We find no clear correlation between failures and the point at which the model's context window becomes full, suggesting that these breakdowns do not stem from memory limits. Apart from highlighting the high variance in performance over long time horizons, Vending-Bench also tests models' ability to acquire capital, a necessity in many hypothetical dangerous AI scenarios. We hope the benchmark can help in preparing for the advent of stronger AI systems.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "51",
        "title": "Hallucination Detection in Large Language Models with Metamorphic Relations",
        "author": [
            "Borui Yang",
            "Md Afif Al Mamun",
            "Jie M. Zhang",
            "Gias Uddin"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15844",
        "abstract": "Large Language Models (LLMs) are prone to hallucinations, e.g., factually incorrect information, in their responses. These hallucinations present challenges for LLM-based applications that demand high factual accuracy. Existing hallucination detection methods primarily depend on external resources, which can suffer from issues such as low availability, incomplete coverage, privacy concerns, high latency, low reliability, and poor scalability. There are also methods depending on output probabilities, which are often inaccessible for closed-source LLMs like GPT models. This paper presents MetaQA, a self-contained hallucination detection approach that leverages metamorphic relation and prompt mutation. Unlike existing methods, MetaQA operates without any external resources and is compatible with both open-source and closed-source LLMs. MetaQA is based on the hypothesis that if an LLM's response is a hallucination, the designed metamorphic relations will be violated. We compare MetaQA with the state-of-the-art zero-resource hallucination detection method, SelfCheckGPT, across multiple datasets, and on two open-source and two closed-source LLMs. Our results reveal that MetaQA outperforms SelfCheckGPT in terms of precision, recall, and f1 score. For the four LLMs we study, MetaQA outperforms SelfCheckGPT with a superiority margin ranging from 0.041 - 0.113 (for precision), 0.143 - 0.430 (for recall), and 0.154 - 0.368 (for F1-score). For instance, with Mistral-7B, MetaQA achieves an average F1-score of 0.435, compared to SelfCheckGPT's F1-score of 0.205, representing an improvement rate of 112.2%. MetaQA also demonstrates superiority across all different categories of questions.",
        "tags": [
            "Detection",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "52",
        "title": "Verify when Uncertain: Beyond Self-Consistency in Black Box Hallucination Detection",
        "author": [
            "Yihao Xue",
            "Kristjan Greenewald",
            "Youssef Mroueh",
            "Baharan Mirzasoleiman"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15845",
        "abstract": "Large Language Models (LLMs) suffer from hallucination problems, which hinder their reliability in sensitive applications. In the black-box setting, several self-consistency-based techniques have been proposed for hallucination detection. We empirically study these techniques and show that they achieve performance close to that of a supervised (still black-box) oracle, suggesting little room for improvement within this paradigm. To address this limitation, we explore cross-model consistency checking between the target model and an additional verifier LLM. With this extra information, we observe improved oracle performance compared to purely self-consistency-based methods. We then propose a budget-friendly, two-stage detection algorithm that calls the verifier model only for a subset of cases. It dynamically switches between self-consistency and cross-consistency based on an uncertainty interval of the self-consistency classifier. We provide a geometric interpretation of consistency-based hallucination detection methods through the lens of kernel mean embeddings, offering deeper theoretical insights. Extensive experiments show that this approach maintains high detection performance while significantly reducing computational cost.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "53",
        "title": "Control Illusion: The Failure of Instruction Hierarchies in Large Language Models",
        "author": [
            "Yilin Geng",
            "Haonan Li",
            "Honglin Mu",
            "Xudong Han",
            "Timothy Baldwin",
            "Omri Abend",
            "Eduard Hovy",
            "Lea Frermann"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15851",
        "abstract": "Large language models (LLMs) are increasingly deployed with hierarchical instruction schemes, where certain instructions (e.g., system-level directives) are expected to take precedence over others (e.g., user messages). Yet, we lack a systematic understanding of how effectively these hierarchical control mechanisms work. We introduce a systematic evaluation framework based on constraint prioritization to assess how well LLMs enforce instruction hierarchies. Our experiments across six state-of-the-art LLMs reveal that models struggle with consistent instruction prioritization, even for simple formatting conflicts. We find that the widely-adopted system/user prompt separation fails to establish a reliable instruction hierarchy, and models exhibit strong inherent biases toward certain constraint types regardless of their priority designation. While controlled prompt engineering and model fine-tuning show modest improvements, our results indicate that instruction hierarchy enforcement is not robustly realized, calling for deeper architectural innovations beyond surface-level modifications.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "54",
        "title": "Enhancing Domain-Specific Retrieval-Augmented Generation: Synthetic Data Generation and Evaluation using Reasoning Models",
        "author": [
            "Aryan Jadon",
            "Avinash Patil",
            "Shashank Kumar"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15854",
        "abstract": "Retrieval-Augmented Generation (RAG) systems face significant performance gaps when applied to technical domains requiring precise information extraction from complex documents. Current evaluation methodologies relying on document-level metrics inadequately capture token-resolution retrieval accuracy that is critical for domain-related documents. We propose a framework combining granular evaluation metrics with synthetic data generation to optimize domain-specific RAG performance. First, we introduce token-aware metrics Precision $\\Omega$ and Intersection-over-Union (IoU) that quantify context preservation versus information density trade-offs inherent in technical texts. Second, we develop a reasoning model-driven pipeline using instruction-tuned LLMs (DeepSeek-R1, DeepSeek-R1 distilled variants, and Phi-4) to generate context-anchored QA pairs with discontinuous reference spans across three specialized corpora: SEC 10-K filings (finance), biomedical abstracts (PubMed), and APT threat reports (cybersecurity).\nOur empirical analysis reveals critical insights: smaller chunks (less than 10 tokens) improve precision by 31-42% (IoU = 0.071 vs. baseline 0.053) at recall costs (-18%), while domain-specific embedding strategies yield 22% variance in optimal chunk sizing (5-20 tokens). The DeepSeek-R1-Distill-Qwen-32B model demonstrates superior concept alignment (+14% mean IoU over alternatives), though no configuration universally dominates. Financial texts favor larger chunks for risk factor coverage (Recall = 0.81 at size = 20), whereas cybersecurity content benefits from atomic segmentation, Precision $\\Omega = 0.28$ at size = 5.\nOur code is available on https://github.com/aryan-jadon/Synthetic-Data-Generation-and-Evaluation-using-Reasoning-Model",
        "tags": [
            "DeepSeek",
            "LLMs",
            "Qwen",
            "RAG",
            "Segmentation"
        ]
    },
    {
        "id": "55",
        "title": "PPC-GPT: Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation",
        "author": [
            "Tao Fan",
            "Guoqiang Ma",
            "Yuanfeng Song",
            "Lixin Fan",
            "Kai Chen",
            "Qiang Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15857",
        "abstract": "Compressing Large Language Models (LLMs) into task-specific Small Language Models (SLMs) encounters two significant challenges: safeguarding domain-specific knowledge privacy and managing limited resources. To tackle these challenges, we propose PPC-GPT, a innovative privacy-preserving federated framework specifically designed for compressing LLMs into task-specific SLMs via pruning and Chain-of-Thought (COT) distillation. PPC-GPT works on a server-client federated architecture, where the client sends differentially private (DP) perturbed task-specific data to the server's LLM. The LLM then generates synthetic data along with their corresponding rationales. This synthetic data is subsequently used for both LLM pruning and retraining processes. Additionally, we harness COT knowledge distillation, leveraging the synthetic data to further improve the retraining of structurally-pruned SLMs. Our experimental results demonstrate the effectiveness of PPC-GPT across various text generation tasks. By compressing LLMs into task-specific SLMs, PPC-GPT not only achieves competitive performance but also prioritizes data privacy protection.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "56",
        "title": "AI Governance InternationaL Evaluation Index (AGILE Index)",
        "author": [
            "Yi Zeng",
            "Enmeng Lu",
            "Xin Guan",
            "Cunqing Huangfu",
            "Zizhe Ruan",
            "Ammar Younas"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15859",
        "abstract": "The rapid advancement of Artificial Intelligence (AI) technology is profoundly transforming human society and concurrently presenting a series of ethical, legal, and social issues. The effective governance of AI has become a crucial global concern. Since 2022, the extensive deployment of generative AI, particularly large language models, marked a new phase in AI governance. Continuous efforts are being made by the international community in actively addressing the novel challenges posed by these AI developments. As consensus on international governance continues to be established and put into action, the practical importance of conducting a global assessment of the state of AI governance is progressively coming to light. In this context, we initiated the development of the AI Governance InternationaL Evaluation Index (AGILE Index). Adhering to the design principle, \"the level of governance should match the level of development,\" the inaugural evaluation of the AGILE Index commences with an exploration of four foundational pillars: the development level of AI, the AI governance environment, the AI governance instruments, and the AI governance effectiveness. It covers 39 indicators across 18 dimensions to comprehensively assess the AI governance level of 14 representative countries globally. The index is utilized to delve into the status of AI governance to date in 14 countries for the first batch of evaluation. The aim is to depict the current state of AI governance in these countries through data scoring, assist them in identifying their governance stage and uncovering governance issues, and ultimately offer insights for the enhancement of their AI governance systems.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "57",
        "title": "Synthetic vs. Gold: The Role of LLM-Generated Labels and Data in Cyberbullying Detection",
        "author": [
            "Arefeh Kazemi",
            "Sri Balaaji Natarajan Kalaivendan",
            "Joachim Wagner",
            "Hamza Qadeer",
            "Brian Davis"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15860",
        "abstract": "This study investigates the role of LLM-generated synthetic data in cyberbullying detection. We conduct a series of experiments where we replace some or all of the authentic data with synthetic data, or augment the authentic data with synthetic data. We find that synthetic cyberbullying data can be the basis for training a classifier for harm detection that reaches performance close to that of a classifier trained with authentic data. Combining authentic with synthetic data shows improvements over the baseline of training on authentic data alone for the test data for all three LLMs tried. These results highlight the viability of synthetic data as a scalable, ethically viable alternative in cyberbullying detection while emphasizing the critical impact of LLM selection on performance outcomes.",
        "tags": [
            "Detection",
            "LLMs"
        ]
    },
    {
        "id": "58",
        "title": "Generative AI Framework for 3D Object Generation in Augmented Reality",
        "author": [
            "Majid Behravan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15869",
        "abstract": "This thesis presents a framework that integrates state-of-the-art generative AI models for real-time creation of three-dimensional (3D) objects in augmented reality (AR) environments. The primary goal is to convert diverse inputs, such as images and speech, into accurate 3D models, enhancing user interaction and immersion. Key components include advanced object detection algorithms, user-friendly interaction techniques, and robust AI models like Shap-E for 3D generation. Leveraging Vision Language Models (VLMs) and Large Language Models (LLMs), the system captures spatial details from images and processes textual information to generate comprehensive 3D objects, seamlessly integrating virtual objects into real-world environments. The framework demonstrates applications across industries such as gaming, education, retail, and interior design. It allows players to create personalized in-game assets, customers to see products in their environments before purchase, and designers to convert real-world objects into 3D models for real-time visualization. A significant contribution is democratizing 3D model creation, making advanced AI tools accessible to a broader audience, fostering creativity and innovation. The framework addresses challenges like handling multilingual inputs, diverse visual data, and complex environments, improving object detection and model generation accuracy, as well as loading 3D models in AR space in real-time. In conclusion, this thesis integrates generative AI and AR for efficient 3D model generation, enhancing accessibility and paving the way for innovative applications and improved user interactions in AR environments.",
        "tags": [
            "3D",
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "59",
        "title": "MutaGReP: Execution-Free Repository-Grounded Plan Search for Code-Use",
        "author": [
            "Zaid Khan",
            "Ali Farhadi",
            "Ranjay Krishna",
            "Luca Weihs",
            "Mohit Bansal",
            "Tanmay Gupta"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15872",
        "abstract": "When a human requests an LLM to complete a coding task using functionality from a large code repository, how do we provide context from the repo to the LLM? One approach is to add the entire repo to the LLM's context window. However, most tasks involve only fraction of symbols from a repo, longer contexts are detrimental to the LLM's reasoning abilities, and context windows are not unlimited. Alternatively, we could emulate the human ability to navigate a large repo, pick out the right functionality, and form a plan to solve the task. We propose MutaGReP (Mutation-guided Grounded Repository Plan Search), an approach to search for plans that decompose a user request into natural language steps grounded in the codebase. MutaGReP performs neural tree search in plan space, exploring by mutating plans and using a symbol retriever for grounding. On the challenging LongCodeArena benchmark, our plans use less than 5% of the 128K context window for GPT-4o but rival the coding performance of GPT-4o with a context window filled with the repo. Plans produced by MutaGReP allow Qwen 2.5 Coder 32B and 72B to match the performance of GPT-4o with full repo context and enable progress on the hardest LongCodeArena tasks. Project page: http://zaidkhan.me/MutaGReP",
        "tags": [
            "GPT",
            "Qwen"
        ]
    },
    {
        "id": "60",
        "title": "DOEI: Dual Optimization of Embedding Information for Attention-Enhanced Class Activation Maps",
        "author": [
            "Hongjie Zhu",
            "Zeyu Zhang",
            "Guansong Pang",
            "Xu Wang",
            "Shimin Wen",
            "Yu Bai",
            "Daji Ergu",
            "Ying Cai",
            "Yang Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15885",
        "abstract": "Weakly supervised semantic segmentation (WSSS) typically utilizes limited semantic annotations to obtain initial Class Activation Maps (CAMs). However, due to the inadequate coupling between class activation responses and semantic information in high-dimensional space, the CAM is prone to object co-occurrence or under-activation, resulting in inferior recognition accuracy. To tackle this issue, we propose DOEI, Dual Optimization of Embedding Information, a novel approach that reconstructs embedding representations through semantic-aware attention weight matrices to optimize the expression capability of embedding information. Specifically, DOEI amplifies tokens with high confidence and suppresses those with low confidence during the class-to-patch interaction. This alignment of activation responses with semantic information strengthens the propagation and decoupling of target features, enabling the generated embeddings to more accurately represent target features in high-level semantic space. In addition, we propose a hybrid-feature alignment module in DOEI that combines RGB values, embedding-guided features, and self-attention weights to increase the reliability of candidate tokens. Comprehensive experiments show that DOEI is an effective plug-and-play module that empowers state-of-the-art visual transformer-based WSSS models to significantly improve the quality of CAMs and segmentation performance on popular benchmarks, including PASCAL VOC (+3.6%, +1.5%, +1.2% mIoU) and MS COCO (+1.2%, +1.6% mIoU). Code will be available at https://github.com/AIGeeksGroup/DOEI.",
        "tags": [
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "61",
        "title": "A Close Look at Decomposition-based XAI-Methods for Transformer Language Models",
        "author": [
            "Leila Arras",
            "Bruno Puri",
            "Patrick Kahardipraja",
            "Sebastian Lapuschkin",
            "Wojciech Samek"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15886",
        "abstract": "Various XAI attribution methods have been recently proposed for the transformer architecture, allowing for insights into the decision-making process of large language models by assigning importance scores to input tokens and intermediate representations. One class of methods that seems very promising in this direction includes decomposition-based approaches, i.e., XAI-methods that redistribute the model's prediction logit through the network, as this value is directly related to the prediction. In the previous literature we note though that two prominent methods of this category, namely ALTI-Logit and LRP, have not yet been analyzed in juxtaposition and hence we propose to close this gap by conducting a careful quantitative evaluation w.r.t. ground truth annotations on a subject-verb agreement task, as well as various qualitative inspections, using BERT, GPT-2 and LLaMA-3 as a testbed. Along the way we compare and extend the ALTI-Logit and LRP methods, including the recently proposed AttnLRP variant, from an algorithmic and implementation perspective. We further incorporate in our benchmark two widely-used gradient-based attribution techniques. Finally, we make our carefullly constructed benchmark dataset for evaluating attributions on language models, as well as our code, publicly available in order to foster evaluation of XAI-methods on a well-defined common ground.",
        "tags": [
            "BERT",
            "GPT",
            "LLaMA",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "62",
        "title": "Understanding and Evaluating Hallucinations in 3D Visual Language Models",
        "author": [
            "Ruiying Peng",
            "Kaiyuan Li",
            "Weichen Zhang",
            "Chen Gao",
            "Xinlei Chen",
            "Yong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15888",
        "abstract": "Recently, 3D-LLMs, which combine point-cloud encoders with large models, have been proposed to tackle complex tasks in embodied intelligence and scene understanding. In addition to showing promising results on 3D tasks, we found that they are significantly affected by hallucinations. For instance, they may generate objects that do not exist in the scene or produce incorrect relationships between objects. To investigate this issue, this work presents the first systematic study of hallucinations in 3D-LLMs. We begin by quickly evaluating hallucinations in several representative 3D-LLMs and reveal that they are all significantly affected by hallucinations. We then define hallucinations in 3D scenes and, through a detailed analysis of datasets, uncover the underlying causes of these hallucinations. We find three main causes: (1) Uneven frequency distribution of objects in the dataset. (2) Strong correlations between objects. (3) Limited diversity in object attributes. Additionally, we propose new evaluation metrics for hallucinations, including Random Point Cloud Pair and Opposite Question Evaluations, to assess whether the model generates responses based on visual information and aligns it with the text's meaning.",
        "tags": [
            "3D",
            "LLMs"
        ]
    },
    {
        "id": "63",
        "title": "RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers",
        "author": [
            "Min Zhao",
            "Guande He",
            "Yixiao Chen",
            "Hongzhou Zhu",
            "Chongxuan Li",
            "Jun Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15894",
        "abstract": "Recent advancements in video generation have enabled models to synthesize high-quality, minute-long videos. However, generating even longer videos with temporal coherence remains a major challenge, and existing length extrapolation methods lead to temporal repetition or motion deceleration. In this work, we systematically analyze the role of frequency components in positional embeddings and identify an intrinsic frequency that primarily governs extrapolation behavior. Based on this insight, we propose RIFLEx, a minimal yet effective approach that reduces the intrinsic frequency to suppress repetition while preserving motion consistency, without requiring any additional modifications. RIFLEx offers a true free lunch--achieving high-quality $2\\times$ extrapolation on state-of-the-art video diffusion transformers in a completely training-free manner. Moreover, it enhances quality and enables $3\\times$ extrapolation by minimal fine-tuning without long videos. Project page and codes: \\href{https://riflex-video.github.io/}{https://riflex-video.github.io/.}",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "64",
        "title": "IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector",
        "author": [
            "Zheng Chen",
            "Yushi Feng",
            "Changyang He",
            "Yue Deng",
            "Hongxi Pu",
            "Bo Li"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15902",
        "abstract": "Large Language Models (LLMs) have attained human-level fluency in text generation, which complicates the distinguishing between human-written and LLM-generated texts. This increases the risk of misuse and highlights the need for reliable detectors. Yet, existing detectors exhibit poor robustness on out-of-distribution (OOD) data and attacked data, which is critical for real-world scenarios. Also, they struggle to provide explainable evidence to support their decisions, thus undermining the reliability. In light of these challenges, we propose IPAD (Inverse Prompt for AI Detection), a novel framework consisting of a Prompt Inverter that identifies predicted prompts that could have generated the input text, and a Distinguisher that examines how well the input texts align with the predicted prompts. We develop and examine two versions of Distinguishers. Empirical evaluations demonstrate that both Distinguishers perform significantly better than the baseline methods, with version2 outperforming baselines by 9.73% on in-distribution data (F1-score) and 12.65% on OOD data (AUROC). Furthermore, a user study is conducted to illustrate that IPAD enhances the AI detection trustworthiness by allowing users to directly examine the decision-making evidence, which provides interpretable support for its state-of-the-art detection results.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "65",
        "title": "Graph Attention Convolutional U-NET: A Semantic Segmentation Model for Identifying Flooded Areas",
        "author": [
            "Muhammad Umair Danish",
            "Madhushan Buwaneswaran",
            "Tehara Fonseka",
            "Katarina Grolinger"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15907",
        "abstract": "The increasing impact of human-induced climate change and unplanned urban constructions has increased flooding incidents in recent years. Accurate identification of flooded areas is crucial for effective disaster management and urban planning. While few works have utilized convolutional neural networks and transformer-based semantic segmentation techniques for identifying flooded areas from aerial footage, recent developments in graph neural networks have created improvement opportunities. This paper proposes an innovative approach, the Graph Attention Convolutional U-NET (GAC-UNET) model, based on graph neural networks for automated identification of flooded areas. The model incorporates a graph attention mechanism and Chebyshev layers into the U-Net architecture. Furthermore, this paper explores the applicability of transfer learning and model reprogramming to enhance the accuracy of flood area segmentation models. Empirical results demonstrate that the proposed GAC-UNET model, outperforms other approaches with 91\\% mAP, 94\\% dice score, and 89\\% IoU, providing valuable insights for informed decision-making and better planning of future infrastructures in flood-prone areas.",
        "tags": [
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "66",
        "title": "LLMs in Mobile Apps: Practices, Challenges, and Opportunities",
        "author": [
            "Kimberly Hau",
            "Safwat Hassan",
            "Shurui Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15908",
        "abstract": "The integration of AI techniques has become increasingly popular in software development, enhancing performance, usability, and the availability of intelligent features. With the rise of large language models (LLMs) and generative AI, developers now have access to a wealth of high-quality open-source models and APIs from closed-source providers, enabling easier experimentation and integration of LLMs into various systems. This has also opened new possibilities in mobile application (app) development, allowing for more personalized and intelligent apps. However, integrating LLM into mobile apps might present unique challenges for developers, particularly regarding mobile device constraints, API management, and code infrastructure. In this project, we constructed a comprehensive dataset of 149 LLM-enabled Android apps and conducted an exploratory analysis to understand how LLMs are deployed and used within mobile apps. This analysis highlights key characteristics of the dataset, prevalent integration strategies, and common challenges developers face. Our findings provide valuable insights for future research and tooling development aimed at enhancing LLM-enabled mobile apps.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "67",
        "title": "Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models",
        "author": [
            "Zheyuan Liu",
            "Guangyao Dou",
            "Xiangchi Yuan",
            "Chunhui Zhang",
            "Zhaoxuan Tan",
            "Meng Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15910",
        "abstract": "Generative models such as Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) trained on massive datasets can lead them to memorize and inadvertently reveal sensitive information, raising ethical and privacy concerns. While some prior works have explored this issue in the context of LLMs, it presents a unique challenge for MLLMs due to the entangled nature of knowledge across modalities, making comprehensive unlearning more difficult. To address this challenge, we propose Modality Aware Neuron Unlearning (MANU), a novel unlearning framework for MLLMs designed to selectively clip neurons based on their relative importance to the targeted forget data, curated for different modalities. Specifically, MANU consists of two stages: important neuron selection and selective pruning. The first stage identifies and collects the most influential neurons across modalities relative to the targeted forget knowledge, while the second stage is dedicated to pruning those selected neurons. MANU effectively isolates and removes the neurons that contribute most to the forget data within each modality, while preserving the integrity of retained knowledge. Our experiments conducted across various MLLM architectures illustrate that MANU can achieve a more balanced and comprehensive unlearning in each modality without largely affecting the overall model utility.",
        "tags": [
            "CLIP",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "68",
        "title": "InSlicing: Interpretable Learning-Assisted Network Slice Configuration in Open Radio Access Networks",
        "author": [
            "Ming Zhao",
            "Yuru Zhang",
            "Qiang Liu",
            "Ahan Kak",
            "Nakjung Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15918",
        "abstract": "Network slicing is a key technology enabling the flexibility and efficiency of 5G networks, offering customized services for diverse applications. However, existing methods face challenges in adapting to dynamic network environments and lack interpretability in performance models. In this paper, we propose a novel interpretable network slice configuration algorithm (\\emph{InSlicing}) in open radio access networks, by integrating Kolmogorov-Arnold Networks (KANs) and hybrid optimization process. On the one hand, we use KANs to approximate and learn the unknown performance function of individual slices, which converts the blackbox optimization problem. On the other hand, we solve the converted problem with a genetic method for global search and incorporate a trust region for gradient-based local refinement. With the extensive evaluation, we show that our proposed algorithm achieves high interpretability while reducing 25+\\% operation cost than existing solutions.",
        "tags": [
            "Kolmogorov-Arnold Networks"
        ]
    },
    {
        "id": "69",
        "title": "Self-Taught Agentic Long Context Understanding",
        "author": [
            "Yufan Zhuang",
            "Xiaodong Yu",
            "Jialian Wu",
            "Ximeng Sun",
            "Ze Wang",
            "Jiang Liu",
            "Yusheng Su",
            "Jingbo Shang",
            "Zicheng Liu",
            "Emad Barsoum"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15920",
        "abstract": "Answering complex, long-context questions remains a major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a framework designed to enhance an LLM's understanding of such queries by integrating targeted self-clarification with contextual grounding within an agentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC), where models refine their understanding through self-generated clarification questions and corresponding contextual groundings. By scaling inference as a tree search where each node represents a CoC step, we achieve 97.8% answer recall on NarrativeQA with a search depth of up to three and a branching factor of eight. To amortize the high cost of this search process to training, we leverage the preference pairs for each step obtained by the CoC workflow and perform two-stage model finetuning: (1) supervised finetuning to learn effective decomposition strategies, and (2) direct preference optimization to enhance reasoning quality. This enables AgenticLU models to generate clarifications and retrieve relevant context effectively and efficiently in a single inference pass. Extensive experiments across seven long-context tasks demonstrate that AgenticLU significantly outperforms state-of-the-art prompting methods and specialized long-context LLMs, achieving robust multi-hop reasoning while sustaining consistent performance as context length grows.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "70",
        "title": "Improving Consistency in Large Language Models through Chain of Guidance",
        "author": [
            "Harsh Raj",
            "Vipul Gupta",
            "Domenic Rosati",
            "Subhabrata Majumdar"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15924",
        "abstract": "Consistency is a fundamental dimension of trustworthiness in Large Language Models (LLMs). For humans to be able to trust LLM-based applications, their outputs should be consistent when prompted with inputs that carry the same meaning or intent. Despite this need, there is no known mechanism to control and guide LLMs to be more consistent at inference time. In this paper, we introduce a novel alignment strategy to maximize semantic consistency in LLM outputs. Our proposal is based on Chain of Guidance (CoG), a multistep prompting technique that generates highly consistent outputs from LLMs. For closed-book question-answering (Q&A) tasks, when compared to direct prompting, the outputs generated using CoG show improved consistency. While other approaches like template-based responses and majority voting may offer alternative paths to consistency, our work focuses on exploring the potential of guided prompting. We use synthetic data sets comprised of consistent input-output pairs to fine-tune LLMs to produce consistent and correct outputs. Our fine-tuned models are more than twice as consistent compared to base models and show strong generalization capabilities by producing consistent outputs over datasets not used in the fine-tuning process.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "71",
        "title": "Dissecting Human Body Representations in Deep Networks Trained for Person Identification",
        "author": [
            "Thomas M Metz",
            "Matthew Q Hill",
            "Blake Myers",
            "Veda Nandan Gandi",
            "Rahul Chilakapati",
            "Alice J O'Toole"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15934",
        "abstract": "Long-term body identification algorithms have emerged recently with the increased availability of high-quality training data. We seek to fill knowledge gaps about these models by analyzing body image embeddings from four body identification networks trained with 1.9 million images across 4,788 identities and 9 databases. By analyzing a diverse range of architectures (ViT, SWIN-ViT, CNN, and linguistically primed CNN), we first show that the face contributes to the accuracy of body identification algorithms and that these algorithms can identify faces to some extent -- with no explicit face training. Second, we show that representations (embeddings) generated by body identification algorithms encode information about gender, as well as image-based information including view (yaw) and even the dataset from which the image originated. Third, we demonstrate that identification accuracy can be improved without additional training by operating directly and selectively on the learned embedding space. Leveraging principal component analysis (PCA), identity comparisons were consistently more accurate in subspaces that eliminated dimensions that explained large amounts of variance. These three findings were surprisingly consistent across architectures and test datasets. This work represents the first analysis of body representations produced by long-term re-identification networks trained on challenging unconstrained datasets.",
        "tags": [
            "ViT"
        ]
    },
    {
        "id": "72",
        "title": "Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs",
        "author": [
            "Shane Bergsma",
            "Nolan Dey",
            "Gurpreet Gosal",
            "Gavia Gray",
            "Daria Soboleva",
            "Joel Hestness"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15938",
        "abstract": "LLMs are commonly trained with a learning rate (LR) warmup, followed by cosine decay to 10% of the maximum (10x decay). In a large-scale empirical study, we show that under an optimal peak LR, a simple linear decay-to-zero (D2Z) schedule consistently outperforms other schedules when training at compute-optimal dataset sizes. D2Z is superior across a range of model sizes, batch sizes, datasets, and vocabularies. Benefits increase as dataset size increases. Leveraging a novel interpretation of AdamW as an exponential moving average of weight updates, we show how linear D2Z optimally balances the demands of early training (moving away from initial conditions) and late training (averaging over more updates in order to mitigate gradient noise). In experiments, a 610M-parameter model trained for 80 tokens-per-parameter (TPP) using D2Z achieves lower loss than when trained for 200 TPP using 10x decay, corresponding to an astonishing 60% compute savings. Models such as Llama2-7B, trained for 286 TPP with 10x decay, could likely have saved a majority of compute by training with D2Z.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "73",
        "title": "\"Kya family planning after marriage hoti hai?\": Integrating Cultural Sensitivity in an LLM Chatbot for Reproductive Health",
        "author": [
            "Roshini Deva",
            "Dhruv Ramani",
            "Tanvi Divate",
            "Suhani Jalota",
            "Azra Ismail"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15939",
        "abstract": "Access to sexual and reproductive health information remains a challenge in many communities globally, due to cultural taboos and limited availability of healthcare providers. Public health organizations are increasingly turning to Large Language Models (LLMs) to improve access to timely and personalized information. However, recent HCI scholarship indicates that significant challenges remain in incorporating context awareness and mitigating bias in LLMs. In this paper, we study the development of a culturally-appropriate LLM-based chatbot for reproductive health with underserved women in urban India. Through user interactions, focus groups, and interviews with multiple stakeholders, we examine the chatbot's response to sensitive and highly contextual queries on reproductive health. Our findings reveal strengths and limitations of the system in capturing local context, and complexities around what constitutes \"culture\". Finally, we discuss how local context might be better integrated, and present a framework to inform the design of culturally-sensitive chatbots for community health.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "74",
        "title": "Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models",
        "author": [
            "Lior Belenki",
            "Alekh Agarwal",
            "Tianze Shi",
            "Kristina Toutanova"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15950",
        "abstract": "We propose a method to optimize language model pre-training data mixtures through efficient approximation of the cross-entropy loss corresponding to each candidate mixture via a Mixture of Data Experts (MDE). We use this approximation as a source of additional features in a regression model, trained from observations of model loss for a small number of mixtures.\nExperiments with Transformer decoder-only language models in the range of 70M to 1B parameters on the SlimPajama dataset show that our method achieves significantly better performance than approaches that train regression models using only the mixture rates as input features. Combining this improved optimization method with an objective that takes into account cross-entropy on end task data leads to superior performance on few-shot downstream evaluations.\nWe also provide theoretical insights on why aggregation of data expert predictions can provide good approximations to model losses for data mixtures.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "75",
        "title": "Human Motion Prediction, Reconstruction, and Generation",
        "author": [
            "Canxuan Gang",
            "Yiran Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15956",
        "abstract": "This report reviews recent advancements in human motion prediction, reconstruction, and generation. Human motion prediction focuses on forecasting future poses and movements from historical data, addressing challenges like nonlinear dynamics, occlusions, and motion style variations. Reconstruction aims to recover accurate 3D human body movements from visual inputs, often leveraging transformer-based architectures, diffusion models, and physical consistency losses to handle noise and complex poses. Motion generation synthesizes realistic and diverse motions from action labels, textual descriptions, or environmental constraints, with applications in robotics, gaming, and virtual avatars. Additionally, text-to-motion generation and human-object interaction modeling have gained attention, enabling fine-grained and context-aware motion synthesis for augmented reality and robotics. This review highlights key methodologies, datasets, challenges, and future research directions driving progress in these fields.",
        "tags": [
            "3D",
            "Diffusion",
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "76",
        "title": "R$^3$Mem: Bridging Memory Retention and Retrieval via Reversible Compression",
        "author": [
            "Xiaoqiang Wang",
            "Suyuchen Wang",
            "Yun Zhu",
            "Bang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15957",
        "abstract": "Memory plays a key role in enhancing LLMs' performance when deployed to real-world applications. Existing solutions face trade-offs: explicit memory designs based on external storage require complex management and incur storage overhead, while implicit memory designs that store information via parameters struggle with reliable retrieval. In this paper, we propose R$^3$Mem, a memory network that optimizes both information Retention and Retrieval through Reversible context compression. Specifically, R$^3$Mem employs virtual memory tokens to compress and encode infinitely long histories, further enhanced by a hierarchical compression strategy that refines information from document- to entity-level for improved assimilation across granularities. For retrieval, R$^3$Mem employs a reversible architecture, reconstructing raw data by invoking the model backward with compressed information. Implemented via parameter-efficient fine-tuning, it can integrate seamlessly with any Transformer-based model. Experiments demonstrate that our memory design achieves state-of-the-art performance in long-context language modeling and retrieval-augmented generation tasks. It also significantly outperforms conventional memory modules in long-horizon interaction tasks like conversational agents, showcasing its potential for next-generation retrieval systems.",
        "tags": [
            "LLMs",
            "Transformer"
        ]
    },
    {
        "id": "77",
        "title": "Accountability in Code Review: The Role of Intrinsic Drivers and the Impact of LLMs",
        "author": [
            "Adam Alami",
            "Victor Vadmand Jensen",
            "Neil A. Ernst"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15963",
        "abstract": "Accountability is an innate part of social systems. It maintains stability and ensures positive pressure on individuals' decision-making. As actors in a social system, software developers are accountable to their team and organization for their decisions. However, the drivers of accountability and how it changes behavior in software development are less understood. In this study, we look at how the social aspects of code review affect software engineers' sense of accountability for code quality. Since software engineering (SE) is increasingly involving Large Language Models (LLM) assistance, we also evaluate the impact on accountability when introducing LLM-assisted code reviews. We carried out a two-phased sequential qualitative study (interviews -> focus groups). In Phase I (16 interviews), we sought to investigate the intrinsic drivers of software engineers influencing their sense of accountability for code quality, relying on self-reported claims. In Phase II, we tested these traits in a more natural setting by simulating traditional peer-led reviews with focus groups and then LLM-assisted review sessions. We found that there are four key intrinsic drivers of accountability for code quality: personal standards, professional integrity, pride in code quality, and maintaining one's reputation. In a traditional peer-led review, we observed a transition from individual to collective accountability when code reviews are initiated. We also found that the introduction of LLM-assisted reviews disrupts this accountability process, challenging the reciprocity of accountability taking place in peer-led evaluations, i.e., one cannot be accountable to an LLM. Our findings imply that the introduction of AI into SE must preserve social integrity and collective accountability mechanisms.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "78",
        "title": "Forgotten Polygons: Multimodal Large Language Models are Shape-Blind",
        "author": [
            "William Rudman",
            "Michal Golovanesky",
            "Amir Bar",
            "Vedant Palit",
            "Yann LeCun",
            "Carsten Eickhoff",
            "Ritambhara Singh"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15969",
        "abstract": "Despite strong performance on vision-language tasks, Multimodal Large Language Models (MLLMs) struggle with mathematical problem-solving, with both open-source and state-of-the-art models falling short of human performance on visual-math benchmarks. To systematically examine visual-mathematical reasoning in MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test multi-step reasoning, and (3) explore a potential solution to improve visual reasoning capabilities. Our findings reveal fundamental shortcomings in shape recognition, with top models achieving under 50% accuracy in identifying regular polygons. We analyze these failures through the lens of dual-process theory and show that MLLMs rely on System 1 (intuitive, memorized associations) rather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count the sides of both familiar and novel shapes, suggesting they have neither learned the concept of sides nor effectively process visual inputs. Finally, we propose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances multi-step mathematical reasoning by explicitly referencing visual annotations in diagrams, boosting GPT-4o's accuracy on an irregular polygon side-counting task from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMs remains an open problem, and visually-guided prompting is essential for successfully engaging visual reasoning. Code available at: https://github.com/rsinghlab/Shape-Blind.",
        "tags": [
            "GPT",
            "Large Language Models"
        ]
    },
    {
        "id": "79",
        "title": "Multi-Agent Multimodal Models for Multicultural Text to Image Generation",
        "author": [
            "Parth Bhalerao",
            "Mounika Yalamarty",
            "Brian Trinh",
            "Oana Ignat"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15972",
        "abstract": "Large Language Models (LLMs) demonstrate impressive performance across various multimodal tasks. However, their effectiveness in cross-cultural contexts remains limited due to the predominantly Western-centric nature of existing data and models. Meanwhile, multi-agent models have shown strong capabilities in solving complex tasks. In this paper, we evaluate the performance of LLMs in a multi-agent interaction setting for the novel task of multicultural image generation. Our key contributions are: (1) We introduce MosAIG, a Multi-Agent framework that enhances multicultural Image Generation by leveraging LLMs with distinct cultural personas; (2) We provide a dataset of 9,000 multicultural images spanning five countries, three age groups, two genders, 25 historical landmarks, and five languages; and (3) We demonstrate that multi-agent interactions outperform simple, no-agent models across multiple evaluation metrics, offering valuable insights for future research. Our dataset and models are available at https://github.com/OanaIgnat/MosAIG.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Text-to-Image"
        ]
    },
    {
        "id": "80",
        "title": "Sparsity May Be All You Need: Sparse Random Parameter Adaptation",
        "author": [
            "Jesus Rios",
            "Pierre Dognin",
            "Ronny Luss",
            "Karthikeyan N. Ramamurthy"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15975",
        "abstract": "Full fine-tuning of large language models for alignment and task adaptation has become prohibitively expensive as models have grown in size. Parameter-Efficient Fine-Tuning (PEFT) methods aim at significantly reducing the computational and memory resources needed for fine-tuning these models by only training on a small number of parameters instead of all model parameters. Currently, the most popular PEFT method is the Low-Rank Adaptation (LoRA), which freezes the parameters of the model to be fine-tuned and introduces a small set of trainable parameters in the form of low-rank matrices. We propose simply reducing the number of trainable parameters by randomly selecting a small proportion of the model parameters to train on. In this paper, we compare the efficiency and performance of our proposed approach with PEFT methods, including LoRA, as well as full parameter fine-tuning.",
        "tags": [
            "Large Language Models",
            "LoRA",
            "Low-Rank Adaptation"
        ]
    },
    {
        "id": "81",
        "title": "Visual Zero-Shot E-Commerce Product Attribute Value Extraction",
        "author": [
            "Jiaying Gong",
            "Ming Cheng",
            "Hongda Shen",
            "Pierre-Yves Vandenbussche",
            "Janet Jenq",
            "Hoda Eldardiry"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15979",
        "abstract": "Existing zero-shot product attribute value (aspect) extraction approaches in e-Commerce industry rely on uni-modal or multi-modal models, where the sellers are asked to provide detailed textual inputs (product descriptions) for the products. However, manually providing (typing) the product descriptions is time-consuming and frustrating for the sellers. Thus, we propose a cross-modal zero-shot attribute value generation framework (ViOC-AG) based on CLIP, which only requires product images as the inputs. ViOC-AG follows a text-only training process, where a task-customized text decoder is trained with the frozen CLIP text encoder to alleviate the modality gap and task disconnection. During the zero-shot inference, product aspects are generated by the frozen CLIP image encoder connected with the trained task-customized text decoder. OCR tokens and outputs from a frozen prompt-based LLM correct the decoded outputs for out-of-domain attribute values. Experiments show that ViOC-AG significantly outperforms other fine-tuned vision-language models for zero-shot attribute value extraction.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "82",
        "title": "Mean-Shift Distillation for Diffusion Mode Seeking",
        "author": [
            "Vikas Thamizharasan",
            "Nikitas Chatzis",
            "Iliyan Georgiev",
            "Matthew Fisher",
            "Difan Liu",
            "Nanxuan Zhao",
            "Evangelos Kalogerakis",
            "Michal Lukac"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15989",
        "abstract": "We present mean-shift distillation, a novel diffusion distillation technique that provides a provably good proxy for the gradient of the diffusion output distribution. This is derived directly from mean-shift mode seeking on the distribution, and we show that its extrema are aligned with the modes. We further derive an efficient product distribution sampling procedure to evaluate the gradient. Our method is formulated as a drop-in replacement for score distillation sampling (SDS), requiring neither model retraining nor extensive modification of the sampling procedure. We show that it exhibits superior mode alignment as well as improved convergence in both synthetic and practical setups, yielding higher-fidelity results when applied to both text-to-image and text-to-3D applications with Stable Diffusion.",
        "tags": [
            "3D",
            "Diffusion",
            "Text-to-3D",
            "Text-to-Image"
        ]
    },
    {
        "id": "83",
        "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
        "author": [
            "Jingbo Yang",
            "Bairu Hou",
            "Wei Wei",
            "Yujia Bao",
            "Shiyu Chang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16002",
        "abstract": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we propose a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation of LLMs when using KV caches computed independently for each document, KVLink introduces three key components: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, using trainable special tokens to restore self-attention across independently encoded documents, and applying mixed-data fine-tuning to enhance performance while preserving the model's original capabilities. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 90% compared to standard LLM inference, making it a scalable and efficient solution for context reuse.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "84",
        "title": "Fast Converging Parallel Offline-Online Iterative Multiscale Mixed Methods",
        "author": [
            "Dilong Zhou",
            "Rafael T. Guiraldello",
            "Felipe Pereira"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16024",
        "abstract": "In this work, we build upon the recently introduced Multiscale Robin Coupled Method with Oversampling and Smoothing (MRCM-OS) to develop two highly efficient iterative multiscale methods. The MRCM-OS methodology demonstrated the ability to achieve flux error magnitudes on the order of $10^{-4}$ in a challenging industry benchmark, namely the SPE10 permeability field. The two newly proposed iterative procedures, through the construction of online informed spaces, significantly enhance the solution accuracy, reaching flux error magnitudes of order $10^{-10}$ for a reduced number of steps.\nThe proposed methods are based on the construction of online informed spaces, which are iteratively refined to improve solution accuracy. Following an initial offline stage, where known boundary conditions are applied to construct multiscale basis functions, the informed spaces are updated through iterative procedures that utilize boundary conditions defined by the most recently computed solution variables. Two distinct approaches are introduced, each leveraging this framework to deliver efficient and accurate iterative solutions.\nA series of numerical simulations, conducted on the SPE10 benchmark, demonstrates the very rapid convergence of the iterative solutions. These results highlight the computational efficiency and competitiveness of the two proposed methods, which are thoroughly compared to each other and to an existing multiscale iterative method from the literature.",
        "tags": [
            "FLUX"
        ]
    },
    {
        "id": "85",
        "title": "FeatSharp: Your Vision Model Features, Sharper",
        "author": [
            "Mike Ranzinger",
            "Greg Heinrich",
            "Pavlo Molchanov",
            "Jan Kautz",
            "Bryan Catanzaro",
            "Andrew Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16025",
        "abstract": "The feature maps of vision encoders are fundamental to myriad modern AI tasks, ranging from core perception algorithms (e.g. semantic segmentation, object detection, depth perception, etc.) to modern multimodal understanding in vision-language models (VLMs). Currently, in computer vision, the frontier of general purpose vision backbones are Vision Transformers (ViT), typically trained using contrastive loss (e.g. CLIP). A key problem with most off-the-shelf ViTs, particularly CLIP, is that these models are inflexibly low resolution. Most run at 224x224px, while the \"high resolution\" versions are around 378-448px, but still inflexible. We introduce a novel method to coherently and cheaply upsample the feature maps of low-res vision encoders while picking up on fine-grained details that would otherwise be lost due to resolution. We demonstrate the effectiveness of this approach on core perception tasks as well as within agglomerative model (RADIO) training as a way of providing richer targets for distillation.",
        "tags": [
            "CLIP",
            "Detection",
            "Segmentation",
            "ViT"
        ]
    },
    {
        "id": "86",
        "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
        "author": [
            "Qianqi Yan",
            "Yue Fan",
            "Hongquan Li",
            "Shan Jiang",
            "Yang Zhao",
            "Xinze Guan",
            "Ching-Chen Kuo",
            "Xin Eric Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16033",
        "abstract": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting inconsistencies confined to a single modality, particularly in text, but struggle with cross-modal conflicts and complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "87",
        "title": "Single-Channel EEG Tokenization Through Time-Frequency Modeling",
        "author": [
            "Jathurshan Pradeepkumar",
            "Xihao Piao",
            "Zheng Chen",
            "Jimeng Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16060",
        "abstract": "We introduce TFM-Tokenizer, a novel tokenization framework tailored for EEG analysis that transforms continuous, noisy brain signals into a sequence of discrete, well-represented tokens for various EEG tasks. Conventional approaches typically rely on continuous embeddings and inter-channel dependencies, which are limited in capturing inherent EEG features such as temporally unpredictable patterns and diverse oscillatory waveforms. In contrast, we hypothesize that critical time-frequency features can be effectively captured from a single channel. By learning tokens that encapsulate these intrinsic patterns within a single channel, our approach yields a scalable tokenizer adaptable across diverse EEG settings. We integrate the TFM-Tokenizer with a transformer-based TFM-Encoder, leveraging established pretraining techniques from natural language processing, such as masked token prediction, followed by downstream fine-tuning for various EEG tasks. Experiments across four EEG datasets show that TFM-Token outperforms state-of-the-art methods. On TUEV, our approach improves balanced accuracy and Cohen's Kappa by 5% over baselines. Comprehensive analysis of the learned tokens demonstrates their ability to capture class-distinctive features, enhance frequency representation, and ability to encode time-frequency motifs into distinct tokens, improving interpretability.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "88",
        "title": "Creative Blends of Visual Concepts",
        "author": [
            "Zhida Sun",
            "Zhenyao Zhang",
            "Yue Zhang",
            "Min Lu",
            "Dani Lischinski",
            "Daniel Cohen-Or",
            "Hui Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16062",
        "abstract": "Visual blends combine elements from two distinct visual concepts into a single, integrated image, with the goal of conveying ideas through imaginative and often thought-provoking visuals. Communicating abstract concepts through visual blends poses a series of conceptual and technical challenges. To address these challenges, we introduce Creative Blends, an AI-assisted design system that leverages metaphors to visually symbolize abstract concepts by blending disparate objects. Our method harnesses commonsense knowledge bases and large language models to align designers' conceptual intent with expressive concrete objects. Additionally, we employ generative text-to-image techniques to blend visual elements through their overlapping attributes. A user study (N=24) demonstrated that our approach reduces participants' cognitive load, fosters creativity, and enhances the metaphorical richness of visual blend ideation. We explore the potential of our method to expand visual blends to include multiple object blending and discuss the insights gained from designing with generative AI.",
        "tags": [
            "Large Language Models",
            "Text-to-Image"
        ]
    },
    {
        "id": "89",
        "title": "Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents",
        "author": [
            "Patrick Tser Jern Kon",
            "Jiachen Liu",
            "Qiuyi Ding",
            "Yiming Qiu",
            "Zhenning Yang",
            "Yibo Huang",
            "Jayanth Srinivasa",
            "Myungjin Lee",
            "Mosharaf Chowdhury",
            "Ang Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16069",
        "abstract": "Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results. Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigorous experimentation remains a significant challenge. To address this gap, we propose Curie, an AI agent framework designed to embed rigor into the experimentation process through three key components: an intra-agent rigor module to enhance reliability, an inter-agent rigor module to maintain methodical control, and an experiment knowledge module to enhance interpretability. To evaluate Curie, we design a novel experimental benchmark composed of 46 questions across four computer science domains, derived from influential research papers, and widely adopted open-source projects. Compared to the strongest baseline tested, we achieve a 3.4$\\times$ improvement in correctly answering experimental http://questions.Curie is open-sourced at https://github.com/Just-Curieous/Curie.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "90",
        "title": "Stealing Training Data from Large Language Models in Decentralized Training through Activation Inversion Attack",
        "author": [
            "Chenxi Dai",
            "Lin Lu",
            "Pan Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16086",
        "abstract": "Decentralized training has become a resource-efficient framework to democratize the training of large language models (LLMs). However, the privacy risks associated with this framework, particularly due to the potential inclusion of sensitive data in training datasets, remain unexplored. This paper identifies a novel and realistic attack surface: the privacy leakage from training data in decentralized training, and proposes \\textit{activation inversion attack} (AIA) for the first time. AIA first constructs a shadow dataset comprising text labels and corresponding activations using public datasets. Leveraging this dataset, an attack model can be trained to reconstruct the training data from activations in victim decentralized training. We conduct extensive experiments on various LLMs and publicly available datasets to demonstrate the susceptibility of decentralized training to AIA. These findings highlight the urgent need to enhance security measures in decentralized training to mitigate privacy risks in training LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "91",
        "title": "Echo: A Large Language Model with Temporal Episodic Memory",
        "author": [
            "WenTao Liu",
            "Ruohua Zhang",
            "Aimin Zhou",
            "Feng Gao",
            "JiaLi Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16090",
        "abstract": "Research on large language models (LLMs) has shown remarkable performance in domains such as mathematics, programming, and literary creation. However, most studies have focused on semantic memory-based question answering, neglecting LLMs' potential to handle episodic memory (EM)-related queries. This oversight has led to suboptimal performance in applications requiring EM, including emotional companionship, personal AI assistants, and AI teachers. To address this gap, we introduce Echo, a LLM enhanced with temporal episodic memory. We propose a Multi-Agent Data Generation Framework that guides the model in generating multi-turn, complex scenario episodic memory dialogue data (EM-Train). Temporal information is innovatively incorporated into the LLM training process, and Echo is trained using the EM-Train. Furthermore, We develop an EM-Test benchmark specifically designed to evaluate LLMs' episodic memory capabilities. The EM-Test assesses performance across various time spans and difficulty levels, providing a comprehensive evaluation of multi-turn episodic memory dialogues. Our experiments demonstrate that Echo significantly outperforms state-of-the-art LLMs on EM-Test. Additionally, a qualitative analysis reveals Echo's potential to exhibit human-like episodic memory capabilities. We will open-source all datasets, code, and model weights.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "92",
        "title": "LitLinker: Supporting the Ideation of Interdisciplinary Contexts with Large Language Models for Teaching Literature in Elementary Schools",
        "author": [
            "Haoxiang Fan",
            "Changshuang Zhou",
            "Hao Yu",
            "Xueyang Wu",
            "Jiangyu Gu",
            "Zhenhui Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16097",
        "abstract": "Teaching literature under interdisciplinary contexts (e.g., science, art) that connect reading materials has become popular in elementary schools. However, constructing such contexts is challenging as it requires teachers to explore substantial amounts of interdisciplinary content and link it to the reading materials. In this paper, we develop LitLinker via an iterative design process involving 13 teachers to facilitate the ideation of interdisciplinary contexts for teaching literature. Powered by a large language model (LLM), LitLinker can recommend interdisciplinary topics and contextualize them with the literary elements (e.g., paragraphs, viewpoints) in the reading materials. A within-subjects study (N=16) shows that compared to an LLM chatbot, LitLinker can improve the integration depth of different subjects and reduce workload in this ideation task. Expert interviews (N=9) also demonstrate LitLinker's usefulness for supporting the ideation of interdisciplinary contexts for teaching literature. We conclude with concerns and design considerations for supporting interdisciplinary teaching with LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "93",
        "title": "Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals",
        "author": [
            "Linda Zeng",
            "Rithwik Gupta",
            "Divij Motwani",
            "Diji Yang",
            "Yi Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16101",
        "abstract": "Retrieval-augmented generation (RAG) has shown impressive capabilities in mitigating hallucinations in large language models (LLMs). However, LLMs struggle to handle misleading retrievals and often fail to maintain their own reasoning when exposed to conflicting or selectively-framed evidence, making them vulnerable to real-world misinformation. In such real-world retrieval scenarios, misleading and conflicting information is rampant, particularly in the political domain, where evidence is often selectively framed, incomplete, or polarized. However, existing RAG benchmarks largely assume a clean retrieval setting, where models succeed by accurately retrieving and generating answers from gold-standard documents. This assumption fails to align with real-world conditions, leading to an overestimation of RAG system performance. To bridge this gap, we introduce RAGuard, a fact-checking dataset designed to evaluate the robustness of RAG systems against misleading retrievals. Unlike prior benchmarks that rely on synthetic noise, our dataset constructs its retrieval corpus from Reddit discussions, capturing naturally occurring misinformation. It categorizes retrieved evidence into three types: supporting, misleading, and irrelevant, providing a realistic and challenging testbed for assessing how well RAG systems navigate different retrieval information. Our benchmark experiments reveal that when exposed to misleading retrievals, all tested LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no retrieval at all), highlighting their susceptibility to noisy environments. To the best of our knowledge, RAGuard is the first benchmark to systematically assess RAG robustness against misleading evidence. We expect this benchmark will drive future research toward improving RAG systems beyond idealized datasets, making them more reliable for real-world applications.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "94",
        "title": "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming",
        "author": [
            "Rui Li",
            "Peiyi Wang",
            "Jingyuan Ma",
            "Di Zhang",
            "Lei Sha",
            "Zhifang Sui"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16109",
        "abstract": "Large Language Models (LLMs) have gained increasing attention for their remarkable capacity, alongside concerns about safety arising from their potential to produce harmful content. Red teaming aims to find prompts that could elicit harmful responses from LLMs, and is essential to discover and mitigate safety risks before real-world deployment. However, manual red teaming is both time-consuming and expensive, rendering it unscalable. In this paper, we propose RTPE, a scalable evolution framework to evolve red teaming prompts across both breadth and depth dimensions, facilitating the automatic generation of numerous high-quality and diverse red teaming prompts. Specifically, in-breadth evolving employs a novel enhanced in-context learning method to create a multitude of quality prompts, whereas in-depth evolving applies customized transformation operations to enhance both content and form of prompts, thereby increasing diversity. Extensive experiments demonstrate that RTPE surpasses existing representative automatic red teaming methods on both attack success rate and diversity. In addition, based on 4,800 red teaming prompts created by RTPE, we further provide a systematic analysis of 8 representative LLMs across 8 sensitive topics.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "95",
        "title": "ZIA: A Theoretical Framework for Zero-Input AI",
        "author": [
            "Aditi De",
            "NeuroBits Labs"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16124",
        "abstract": "Zero-Input AI (ZIA) introduces a novel framework for human-computer interaction by enabling proactive intent prediction without explicit user commands. It integrates gaze tracking, bio-signals (EEG, heart rate), and contextual data (time, location, usage history) into a multi-modal model for real-time inference, targeting <100 ms latency. The proposed architecture employs a transformer-based model with cross-modal attention, variational Bayesian inference for uncertainty estimation, and reinforcement learning for adaptive optimization. To support deployment on edge devices (CPUs, TPUs, NPUs), ZIA utilizes quantization, weight pruning, and linear attention to reduce complexity from quadratic to linear with sequence length. Theoretical analysis establishes an information-theoretic bound on prediction error and demonstrates how multi-modal fusion improves accuracy over single-modal approaches. Expected performance suggests 85-90% accuracy with EEG integration and 60-100 ms inference latency. ZIA provides a scalable, privacy-preserving framework for accessibility, healthcare, and consumer applications, advancing AI toward anticipatory intelligence.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "96",
        "title": "Chain-of-Description: What I can understand, I can put into words",
        "author": [
            "Jiaxin Guo",
            "Daimeng Wei",
            "Zongyao Li",
            "Hengchao Shang",
            "Yuanchang Luo",
            "Hao Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16137",
        "abstract": "In this paper, we propose a novel strategy defined as Chain-of-Description (CoD) Prompting, tailored for Multi-Modal Large Language Models. This approach involves having the model first provide a detailed description of the multi-modal input before generating an answer to the question. When applied to models such as Qwen2-Audio, Qwen2-VL, and Qwen2.5-VL, CoD Prompting significantly enhances performance compared to standard prompting methods. This is demonstrated by nearly a 4\\% improvement in the speech category of the audio benchmark AIR-Bench-Chat and a 5.3\\% improvement in the hard-level portion of the vision benchmark MMMU\\_Pro. Our ablation study further validates the effectiveness of CoD Prompting.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "97",
        "title": "The Law of Knowledge Overshadowing: Towards Understanding, Predicting, and Preventing LLM Hallucination",
        "author": [
            "Yuji Zhang",
            "Sha Li",
            "Cheng Qian",
            "Jiateng Liu",
            "Pengfei Yu",
            "Chi Han",
            "Yi R. Fung",
            "Kathleen McKeown",
            "Chengxiang Zhai",
            "Manling Li",
            "Heng Ji"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16143",
        "abstract": "Hallucination is a persistent challenge in large language models (LLMs), where even with rigorous quality control, models often generate distorted facts. This paradox, in which error generation continues despite high-quality training data, calls for a deeper understanding of the underlying LLM mechanisms. To address it, we propose a novel concept: knowledge overshadowing, where model's dominant knowledge can obscure less prominent knowledge during text generation, causing the model to fabricate inaccurate details. Building on this idea, we introduce a novel framework to quantify factual hallucinations by modeling knowledge overshadowing. Central to our approach is the log-linear law, which predicts that the rate of factual hallucination increases linearly with the logarithmic scale of (1) Knowledge Popularity, (2) Knowledge Length, and (3) Model Size. The law provides a means to preemptively quantify hallucinations, offering foresight into their occurrence even before model training or inference. Built on overshadowing effect, we propose a new decoding strategy CoDa, to mitigate hallucinations, which notably enhance model factuality on Overshadow (27.9%), MemoTrap (13.1%) and NQ-Swap (18.3%). Our findings not only deepen understandings of the underlying mechanisms behind hallucinations but also provide actionable insights for developing more predictable and controllable language models.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "98",
        "title": "Number Representations in LLMs: A Computational Parallel to Human Perception",
        "author": [
            "H.V. AlquBoj",
            "Hilal AlQuabeh",
            "Velibor Bojkovic",
            "Tatsuya Hiraoka",
            "Ahmed Oumar El-Shangiti",
            "Munachiso Nwadike",
            "Kentaro Inui"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16147",
        "abstract": "Humans are believed to perceive numbers on a logarithmic mental number line, where smaller values are represented with greater resolution than larger ones. This cognitive bias, supported by neuroscience and behavioral studies, suggests that numerical magnitudes are processed in a sublinear fashion rather than on a uniform linear scale. Inspired by this hypothesis, we investigate whether large language models (LLMs) exhibit a similar logarithmic-like structure in their internal numerical representations. By analyzing how numerical values are encoded across different layers of LLMs, we apply dimensionality reduction techniques such as PCA and PLS followed by geometric regression to uncover latent structures in the learned embeddings. Our findings reveal that the model's numerical representations exhibit sublinear spacing, with distances between values aligning with a logarithmic scale. This suggests that LLMs, much like humans, may encode numbers in a compressed, non-uniform manner.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "99",
        "title": "ZiGong 1.0: A Large Language Model for Financial Credit",
        "author": [
            "Yu Lei",
            "Zixuan Wang",
            "Chu Liu",
            "Tongyao Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16159",
        "abstract": "Large Language Models (LLMs) have demonstrated strong performance across various general Natural Language Processing (NLP) tasks. However, their effectiveness in financial credit assessment applications remains suboptimal, primarily due to the specialized financial expertise required for these tasks. To address this limitation, we propose ZiGong, a Mistral-based model enhanced through multi-task supervised fine-tuning. To specifically combat model hallucination in financial contexts, we introduce a novel data pruning methodology. Our approach utilizes a proxy model to score training samples, subsequently combining filtered data with original datasets for model training. This data refinement strategy effectively reduces hallucinations in LLMs while maintaining reliability in downstream financial applications. Experimental results show our method significantly enhances model robustness and prediction accuracy in real-world financial scenarios.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "100",
        "title": "OmniParser V2: Structured-Points-of-Thought for Unified Visual Text Parsing and Its Generality to Multimodal Large Language Models",
        "author": [
            "Wenwen Yu",
            "Zhibo Yang",
            "Jianqiang Wan",
            "Sibo Song",
            "Jun Tang",
            "Wenqing Cheng",
            "Yuliang Liu",
            "Xiang Bai"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16161",
        "abstract": "Visually-situated text parsing (VsTP) has recently seen notable advancements, driven by the growing demand for automated document understanding and the emergence of large language models capable of processing document-based questions. While various methods have been proposed to tackle the complexities of VsTP, existing solutions often rely on task-specific architectures and objectives for individual tasks. This leads to modal isolation and complex workflows due to the diversified targets and heterogeneous schemas. In this paper, we introduce OmniParser V2, a universal model that unifies VsTP typical tasks, including text spotting, key information extraction, table recognition, and layout analysis, into a unified framework. Central to our approach is the proposed Structured-Points-of-Thought (SPOT) prompting schemas, which improves model performance across diverse scenarios by leveraging a unified encoder-decoder architecture, objective, and input\\&output representation. SPOT eliminates the need for task-specific architectures and loss functions, significantly simplifying the processing pipeline. Our extensive evaluations across four tasks on eight different datasets show that OmniParser V2 achieves state-of-the-art or competitive results in VsTP. Additionally, we explore the integration of SPOT within a multimodal large language model structure, further enhancing text localization and recognition capabilities, thereby confirming the generality of SPOT prompting technique. The code is available at \\href{https://github.com/AlibabaResearch/AdvancedLiterateMachinery}{AdvancedLiterateMachinery}.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "101",
        "title": "Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs under Noisy Observations",
        "author": [
            "Chunyang Li",
            "Weiqi Wang",
            "Tianshi Zheng",
            "Yangqiu Song"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16169",
        "abstract": "Inductive reasoning, a cornerstone of human cognition, enables generalization from limited data but hasn't yet been fully achieved by large language models (LLMs). While modern LLMs excel at reasoning tasks, their ability to maintain stable and consistent rule abstraction under imperfect observations remains underexplored. To fill this gap, in this work, we introduce Robust Rule Induction, a task that evaluates LLMs' capability in inferring rules from data that are fused with noisy examples. To address this task, we further propose Sample-steered Rule Refinement (SRR), a method enhancing reasoning stability via observation diversification and execution-guided feedback. Experiments across arithmetic, cryptography, and list functions reveal: (1) SRR outperforms other methods with minimal performance degradation under noise; (2) Despite slight accuracy variation, LLMs exhibit instability under noise (e.g., 0% accuracy change with only 70% consistent score); (3) Counterfactual task gaps highlight LLMs' reliance on memorized patterns over genuine abstraction. Our findings challenge LLMs' reasoning robustness, revealing susceptibility to hypothesis drift and pattern overfitting, while providing empirical evidence critical for developing human-like inductive systems. Code and data are available at \\href{https://github.com/lcy2723/Robust-Rule-Induction}{https://github.com/lcy2723/Robust-Rule-Induction}.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "102",
        "title": "EPERM: An Evidence Path Enhanced Reasoning Model for Knowledge Graph Question and Answering",
        "author": [
            "Xiao Long",
            "Liansheng Zhuang",
            "Aodi Li",
            "Minghong Yao",
            "Shafei Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16171",
        "abstract": "Due to the remarkable reasoning ability, Large language models (LLMs) have demonstrated impressive performance in knowledge graph question answering (KGQA) tasks, which find answers to natural language questions over knowledge graphs (KGs). To alleviate the hallucinations and lack of knowledge issues of LLMs, existing methods often retrieve the question-related information from KGs to enrich the input context. However, most methods focus on retrieving the relevant information while ignoring the importance of different types of knowledge in reasoning, which degrades their performance. To this end, this paper reformulates the KGQA problem as a graphical model and proposes a three-stage framework named the Evidence Path Enhanced Reasoning Model (EPERM) for KGQA. In the first stage, EPERM uses the fine-tuned LLM to retrieve a subgraph related to the question from the original knowledge graph. In the second stage, EPERM filters out the evidence paths that faithfully support the reasoning of the questions, and score their importance in reasoning. Finally, EPERM uses the weighted evidence paths to reason the final answer. Since considering the importance of different structural information in KGs for reasoning, EPERM can improve the reasoning ability of LLMs in KGQA tasks. Extensive experiments on benchmark datasets demonstrate that EPERM achieves superior performances in KGQA tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "103",
        "title": "Maybe I Should Not Answer That, but... Do LLMs Understand The Safety of Their Inputs?",
        "author": [
            "Maciej ChrabÄszcz",
            "Filip Szatkowski",
            "Bartosz WÃ³jcik",
            "Jan DubiÅski",
            "Tomasz TrzciÅski"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16174",
        "abstract": "Ensuring the safety of the Large Language Model (LLM) is critical, but currently used methods in most cases sacrifice the model performance to obtain increased safety or perform poorly on data outside of their adaptation distribution. We investigate existing methods for such generalization and find them insufficient. Surprisingly, while even plain LLMs recognize unsafe prompts, they may still generate unsafe responses. To avoid performance degradation and preserve safe performance, we advocate for a two-step framework, where we first identify unsafe prompts via a lightweight classifier, and apply a \"safe\" model only to such prompts. In particular, we explore the design of the safety detector in more detail, investigating the use of different classifier architectures and prompting techniques. Interestingly, we find that the final hidden state for the last token is enough to provide robust performance, minimizing false positives on benign data while performing well on malicious prompt detection. Additionally, we show that classifiers trained on the representations from different model layers perform comparably on the latest model layers, indicating that safety representation is present in the LLMs' hidden states at most model stages. Our work is a step towards efficient, representation-based safety mechanisms for LLMs.",
        "tags": [
            "Detection",
            "LLMs"
        ]
    },
    {
        "id": "104",
        "title": "Mojito: LLM-Aided Motion Instructor with Jitter-Reduced Inertial Tokens",
        "author": [
            "Ziwei Shan",
            "Yaoyu He",
            "Chengfeng Zhao",
            "Jiashen Du",
            "Jingyan Zhang",
            "Qixuan Zhang",
            "Jingyi Yu",
            "Lan Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16175",
        "abstract": "Human bodily movements convey critical insights into action intentions and cognitive processes, yet existing multimodal systems primarily focused on understanding human motion via language, vision, and audio, which struggle to capture the dynamic forces and torques inherent in 3D motion. Inertial measurement units (IMUs) present a promising alternative, offering lightweight, wearable, and privacy-conscious motion sensing. However, processing of streaming IMU data faces challenges such as wireless transmission instability, sensor noise, and drift, limiting their utility for long-term real-time motion capture (MoCap), and more importantly, online motion analysis. To address these challenges, we introduce Mojito, an intelligent motion agent that integrates inertial sensing with large language models (LLMs) for interactive motion capture and behavioral analysis.",
        "tags": [
            "3D",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "105",
        "title": "BiDeV: Bilateral Defusing Verification for Complex Claim Fact-Checking",
        "author": [
            "Yuxuan Liu",
            "Hongda Sun",
            "Wenya Guo",
            "Xinyan Xiao",
            "Cunli Mao",
            "Zhengtao Yu",
            "Rui Yan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16181",
        "abstract": "Complex claim fact-checking performs a crucial role in disinformation detection. However, existing fact-checking methods struggle with claim vagueness, specifically in effectively handling latent information and complex relations within claims. Moreover, evidence redundancy, where nonessential information complicates the verification process, remains a significant issue. To tackle these limitations, we propose Bilateral Defusing Verification (BiDeV), a novel fact-checking working-flow framework integrating multiple role-played LLMs to mimic the human-expert fact-checking process. BiDeV consists of two main modules: Vagueness Defusing identifies latent information and resolves complex relations to simplify the claim, and Redundancy Defusing eliminates redundant content to enhance the evidence quality. Extensive experimental results on two widely used challenging fact-checking benchmarks (Hover and Feverous-s) demonstrate that our BiDeV can achieve the best performance under both gold and open settings. This highlights the effectiveness of BiDeV in handling complex claims and ensuring precise fact-checking",
        "tags": [
            "Detection",
            "LLMs"
        ]
    },
    {
        "id": "106",
        "title": "IPO: Your Language Model is Secretly a Preference Classifier",
        "author": [
            "Shivank Garg",
            "Ayush Singh",
            "Shweta Singh",
            "Paras Chopra"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16182",
        "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. While it enables LLMs to achieve human-level alignment, it often incurs significant computational and financial costs due to its reliance on training external reward models or human-labeled preferences. In this work, we propose \\textbf{Implicit Preference Optimization (IPO)}, an alternative approach that leverages generative LLMs as preference classifiers, thereby reducing the dependence on external human feedback or reward models to obtain preferences. We conduct a comprehensive evaluation on the preference classification ability of LLMs using RewardBench, assessing models across different sizes, architectures, and training levels to validate our hypothesis. Furthermore, we investigate the self-improvement capabilities of LLMs by generating multiple responses for a given instruction and employing the model itself as a preference classifier for Direct Preference Optimization (DPO)-based training. Our findings demonstrate that models trained through IPO achieve performance comparable to those utilizing state-of-the-art reward models for obtaining preferences.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "107",
        "title": "An Autonomous Network Orchestration Framework Integrating Large Language Models with Continual Reinforcement Learning",
        "author": [
            "Masoud Shokrnezhad",
            "Tarik Taleb"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16198",
        "abstract": "6G networks aim to achieve global coverage, massive connectivity, and ultra-stringent requirements. Space-Air-Ground Integrated Networks (SAGINs) and Semantic Communication (SemCom) are essential for realizing these goals, yet they introduce considerable complexity in resource orchestration. Drawing inspiration from research in robotics, a viable solution to manage this complexity is the application of Large Language Models (LLMs). Although the use of LLMs in network orchestration has recently gained attention, existing solutions have not sufficiently addressed LLM hallucinations or their adaptation to network dynamics. To address this gap, this paper proposes a framework called Autonomous Reinforcement Coordination (ARC) for a SemCom-enabled SAGIN. This framework employs an LLM-based Retrieval-Augmented Generator (RAG) monitors services, users, and resources and processes the collected data, while a Hierarchical Action Planner (HAP) orchestrates resources. ARC decomposes orchestration into two tiers, utilizing LLMs for high-level planning and Reinforcement Learning (RL) agents for low-level decision-making, in alignment with the Mixture of Experts (MoE) concept. The LLMs utilize Chain-of-Thought (CoT) reasoning for few-shot learning, empowered by contrastive learning, while the RL agents employ replay buffer management for continual learning, thereby achieving efficiency, accuracy, and adaptability. Simulations are provided to demonstrate the effectiveness of ARC, along with a comprehensive discussion on potential future research directions to enhance and upgrade ARC.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG",
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "108",
        "title": "LLMKey: LLM-Powered Wireless Key Generation Scheme for Next-Gen IoV Systems",
        "author": [
            "Huanqi Yang",
            "Weitao Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16199",
        "abstract": "Wireless key generation holds significant promise for establishing cryptographic keys in Next-Gen Internet of Vehicles (IoV) systems. However, existing approaches often face inefficiencies and performance limitations caused by frequent channel probing and ineffective quantization. To address these challenges, this paper introduces LLMKey, a novel key generation system designed to enhance efficiency and security. We identify excessive channel probing and suboptimal quantization as critical bottlenecks in current methods. To mitigate these issues, we propose an innovative large language model (LLM)-based channel probing technique that leverages the capabilities of LLMs to reduce probing rounds while preserving crucial channel information. Instead of conventional quantization, LLMKey adopts a perturbed compressed sensing-based key delivery mechanism, improving both robustness and security. Extensive evaluations are conducted in four real-world scenarios, encompassing V2I (Vehicle-to-Infrastructure) and V2V (Vehicle-to-Vehicle) settings in both urban and rural environments. The results show that LLMKey achieves an average key agreement rate of 98.78\\%, highlighting its effectiveness and reliability across diverse conditions.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "109",
        "title": "Improving Speech Enhancement by Cross- and Sub-band Processing with State Space Model",
        "author": [
            "Jizhen Li",
            "Weiping Tu",
            "Yuhong Yang",
            "Xinmeng Xu",
            "Yiqun Zhang",
            "Yanzhen Ren"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16207",
        "abstract": "Recently, the state space model (SSM) represented by Mamba has shown remarkable performance in long-term sequence modeling tasks, including speech enhancement. However, due to substantial differences in sub-band features, applying the same SSM to all sub-bands limits its inference capability. Additionally, when processing each time frame of the time-frequency representation, the SSM may forget certain high-frequency information of low energy, making the restoration of structure in the high-frequency bands challenging. For this reason, we propose Cross- and Sub-band Mamba (CSMamba). To assist the SSM in handling different sub-band features flexibly, we propose a band split block that splits the full-band into four sub-bands with different widths based on their information similarity. We then allocate independent weights to each sub-band, thereby reducing the inference burden on the SSM. Furthermore, to mitigate the forgetting of low-energy information in the high-frequency bands by the SSM, we introduce a spectrum restoration block that enhances the representation of the cross-band features from multiple perspectives. Experimental results on the DNS Challenge 2021 dataset demonstrate that CSMamba outperforms several state-of-the-art (SOTA) speech enhancement methods in three objective evaluation metrics with fewer parameters.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "110",
        "title": "SalM$2$: An Extremely Lightweight Saliency Mamba Model for Real-Time Cognitive Awareness of Driver Attention",
        "author": [
            "Chunyu Zhao",
            "Wentao Mu",
            "Xian Zhou",
            "Wenbo Liu",
            "Fei Yan",
            "Tao Deng"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16214",
        "abstract": "Driver attention recognition in driving scenarios is a popular direction in traffic scene perception technology. It aims to understand human driver attention to focus on specific targets/objects in the driving scene. However, traffic scenes contain not only a large amount of visual information but also semantic information related to driving tasks. Existing methods lack attention to the actual semantic information present in driving scenes. Additionally, the traffic scene is a complex and dynamic process that requires constant attention to objects related to the current driving task. Existing models, influenced by their foundational frameworks, tend to have large parameter counts and complex structures. Therefore, this paper proposes a real-time saliency Mamba network based on the latest Mamba framework. As shown in Figure 1, our model uses very few parameters (0.08M, only 0.09~11.16% of other models), while maintaining SOTA performance or achieving over 98% of the SOTA model's performance.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "111",
        "title": "Flow-based linear embedding for Bayesian filtering of nonlinear stochastic dynamical systems",
        "author": [
            "Xintong Wang",
            "Xiaofei Guan",
            "Ling Guo",
            "Hao Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16232",
        "abstract": "Bayesian filtering for high-dimensional nonlinear stochastic dynamical systems is a fundamental yet challenging problem in many fields of science and engineering. Existing methods face significant obstacles: Gaussian-based filters struggle with non-Gaussian distributions, sequential Monte Carlo methods are computationally intensive and prone to particle degeneracy in high dimensions, and deep learning approaches often fail to balance accuracy and efficiency in complex filtering tasks. To address these challenges, we propose a flow-based Bayesian filter (FBF) that integrates normalizing flows to construct a latent linear state-space model with Gaussian filtering distributions. This framework enables efficient density estimation and sampling through invertible transformations provided by normalizing flows, which can be learned directly from data, thereby eliminating the need for prior knowledge of system dynamics or observation models. Numerical experiments demonstrate the advantages of FBF in terms of both accuracy and efficiency.",
        "tags": [
            "Normalizing Flows"
        ]
    },
    {
        "id": "112",
        "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
        "author": [
            "Yifu Ding",
            "Wentao Jiang",
            "Shunyu Liu",
            "Yongcheng Jing",
            "Jinyang Guo",
            "Yingjie Wang",
            "Jing Zhang",
            "Zengmao Wang",
            "Ziwei Liu",
            "Bo Du",
            "Xianglong Liu",
            "Dacheng Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16235",
        "abstract": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by structuring problem-solving as a spanning tree. However, recent methods focus on search accuracy while overlooking computational efficiency. The challenges of accelerating the ToT lie in the frequent switching of reasoning focus, and the redundant exploration of suboptimal solutions. To alleviate this dilemma, we propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework that aims to dynamically optimize the reasoning path in inference. It includes the Parallelism Streamline in the generation phase to build up a flexible and adaptive parallelism with arbitrary paths by fine-grained cache management and alignment. Meanwhile, the Search and Transition Mechanism filters potential candidates to dynamically maintain the reasoning focus on more possible solutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with Math500 and GSM8K datasets show that DPTS significantly improves efficiency by 2-4x on average while maintaining or even surpassing existing reasoning algorithms in accuracy, making ToT-based reasoning more scalable and computationally efficient.",
        "tags": [
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "113",
        "title": "Reproducibility Study of Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation",
        "author": [
            "Jose L. Garcia",
            "Karolina Hajkova",
            "Maria Marchenko",
            "Carlos Miguel PatiÃ±o"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16242",
        "abstract": "This paper presents a reproducibility study and extension of \"Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation.\" We validate the original findings using a range of open-weight models (1.5B-70B parameters) and GPT-4o Mini while introducing several novel contributions. We analyze the Pareto front of the games, propose a communication-free baseline to test whether successful negotiations are possible without agent interaction, evaluate recent small language models' performance, analyze structural information leakage in model responses, and implement an inequality metric to assess negotiation fairness. Our results demonstrate that smaller models (<10B parameters) struggle with format adherence and coherent responses, but larger open-weight models can approach proprietary model performance. Additionally, in many scenarios, single-agent approaches can achieve comparable results to multi-agent negotiations, challenging assumptions about the necessity of agent communication to perform well on the benchmark. This work also provides insights into the accessibility, fairness, environmental impact, and privacy considerations of LLM-based negotiation systems.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "114",
        "title": "Linear Attention for Efficient Bidirectional Sequence Modeling",
        "author": [
            "Arshia Afzal",
            "Elias Abad Rocamora",
            "Leyla Naz Candogan",
            "Pol Puigdemont",
            "Francesco Tonin",
            "Yongtao Wu",
            "Mahsa Shoaran",
            "Volkan Cevher"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16249",
        "abstract": "Transformers with linear attention enable fast and parallel training. Moreover, they can be formulated as Recurrent Neural Networks (RNNs), for efficient linear-time inference. While extensively evaluated in causal sequence modeling, they have yet to be extended to the bidirectional setting. This work introduces the LION framework, establishing new theoretical foundations for linear transformers in bidirectional sequence modeling. LION constructs a bidirectional RNN equivalent to full Linear Attention. This extends the benefits of linear transformers: parallel training, and efficient inference, into the bidirectional setting. Using LION, we cast three linear transformers to their bidirectional form: LION-LIT, the bidirectional variant corresponding to (Katharopoulos et al., 2020); LION-D, extending RetNet (Sun et al., 2023); and LION-S, a linear transformer with a stable selective mask inspired by selectivity of SSMs (Dao & Gu, 2024). Replacing the attention block with LION (-LIT, -D, -S) achieves performance on bidirectional tasks that approaches that of Transformers and State-Space Models (SSMs), while delivering significant improvements in training speed. Our implementation is available in http://github.com/LIONS-EPFL/LION.",
        "tags": [
            "RNN",
            "SSMs",
            "State Space Models",
            "Transformer"
        ]
    },
    {
        "id": "115",
        "title": "ThinkBench: Dynamic Out-of-Distribution Evaluation for Robust LLM Reasoning",
        "author": [
            "Shulin Huang",
            "Linyi Yang",
            "Yan Song",
            "Shuang Chen",
            "Leyang Cui",
            "Ziyu Wan",
            "Qingcheng Zeng",
            "Ying Wen",
            "Kun Shao",
            "Weinan Zhang",
            "Jun Wang",
            "Yue Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16268",
        "abstract": "Evaluating large language models (LLMs) poses significant challenges, particularly due to issues of data contamination and the leakage of correct answers. To address these challenges, we introduce ThinkBench, a novel evaluation framework designed to evaluate LLMs' reasoning capability robustly. ThinkBench proposes a dynamic data generation method for constructing out-of-distribution (OOD) datasets and offers an OOD dataset that contains 2,912 samples drawn from reasoning tasks. ThinkBench unifies the evaluation of reasoning models and non-reasoning models. We evaluate 16 LLMs and 4 PRMs under identical experimental conditions and show that most of the LLMs' performance are far from robust and they face a certain level of data leakage. By dynamically generating OOD datasets, ThinkBench effectively provides a reliable evaluation of LLMs and reduces the impact of data contamination.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "116",
        "title": "Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation",
        "author": [
            "Kartik Gupta"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16274",
        "abstract": "The Qwen 2.5 3B base model was fine-tuned to generate contextually rich and engaging movie dialogue, leveraging the Cornell Movie-Dialog Corpus, a curated dataset of movie conversations. Due to the limitations in GPU computing and VRAM, the training process began with the 0.5B model progressively scaling up to the 1.5B and 3B versions as efficiency improvements were implemented. The Qwen 2.5 series, developed by Alibaba Group, stands at the forefront of small open-source pre-trained models, particularly excelling in creative tasks compared to alternatives like Meta's Llama 3.2 and Google's Gemma. Results demonstrate the ability of small models to produce high-quality, realistic dialogue, offering a promising approach for real-time, context-sensitive conversation generation.",
        "tags": [
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "117",
        "title": "Beyond Trusting Trust: Multi-Model Validation for Robust Code Generation",
        "author": [
            "Bradley McDanel"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16279",
        "abstract": "This paper explores the parallels between Thompson's \"Reflections on Trusting Trust\" and modern challenges in LLM-based code generation. We examine how Thompson's insights about compiler backdoors take on new relevance in the era of large language models, where the mechanisms for potential exploitation are even more opaque and difficult to analyze. Building on this analogy, we discuss how the statistical nature of LLMs creates novel security challenges in code generation pipelines. As a potential direction forward, we propose an ensemble-based validation approach that leverages multiple independent models to detect anomalous code patterns through cross-model consensus. This perspective piece aims to spark discussion about trust and validation in AI-assisted software development.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "118",
        "title": "Human Preferences in Large Language Model Latent Space: A Technical Analysis on the Reliability of Synthetic Data in Voting Outcome Prediction",
        "author": [
            "Sarah Ball",
            "Simeon Allmendinger",
            "Frauke Kreuter",
            "Niklas KÃ¼hl"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16280",
        "abstract": "Generative AI (GenAI) is increasingly used in survey contexts to simulate human preferences. While many research endeavors evaluate the quality of synthetic GenAI data by comparing model-generated responses to gold-standard survey results, fundamental questions about the validity and reliability of using LLMs as substitutes for human respondents remain. Our study provides a technical analysis of how demographic attributes and prompt variations influence latent opinion mappings in large language models (LLMs) and evaluates their suitability for survey-based predictions. Using 14 different models, we find that LLM-generated data fails to replicate the variance observed in real-world human responses, particularly across demographic subgroups. In the political space, persona-to-party mappings exhibit limited differentiation, resulting in synthetic data that lacks the nuanced distribution of opinions found in survey data. Moreover, we show that prompt sensitivity can significantly alter outputs for some models, further undermining the stability and predictiveness of LLM-based simulations. As a key contribution, we adapt a probe-based methodology that reveals how LLMs encode political affiliations in their latent space, exposing the systematic distortions introduced by these models. Our findings highlight critical limitations in AI-generated survey data, urging caution in its use for public opinion research, social science experimentation, and computational behavioral modeling.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "119",
        "title": "Interrogating LLM design under a fair learning doctrine",
        "author": [
            "Johnny Tian-Zheng Wei",
            "Maggie Wang",
            "Ameya Godbole",
            "Jonathan H. Choi",
            "Robin Jia"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16290",
        "abstract": "The current discourse on large language models (LLMs) and copyright largely takes a \"behavioral\" perspective, focusing on model outputs and evaluating whether they are substantially similar to training data. However, substantial similarity is difficult to define algorithmically and a narrow focus on model outputs is insufficient to address all copyright risks. In this interdisciplinary work, we take a complementary \"structural\" perspective and shift our focus to how LLMs are trained. We operationalize a notion of \"fair learning\" by measuring whether any training decision substantially affected the model's memorization. As a case study, we deconstruct Pythia, an open-source LLM, and demonstrate the use of causal and correlational analyses to make factual determinations about Pythia's training decisions. By proposing a legal standard for fair learning and connecting memorization analyses to this standard, we identify how judges may advance the goals of copyright law through adjudication. Finally, we discuss how a fair learning standard might evolve to enhance its clarity by becoming more rule-like and incorporating external technical guidelines.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "120",
        "title": "TimePFN: Effective Multivariate Time Series Forecasting with Synthetic Data",
        "author": [
            "Ege Onur Taga",
            "M. Emrullah Ildiz",
            "Samet Oymak"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16294",
        "abstract": "The diversity of time series applications and scarcity of domain-specific data highlight the need for time-series models with strong few-shot learning capabilities. In this work, we propose a novel training scheme and a transformer-based architecture, collectively referred to as TimePFN, for multivariate time-series (MTS) forecasting. TimePFN is based on the concept of Prior-data Fitted Networks (PFN), which aims to approximate Bayesian inference. Our approach consists of (1) generating synthetic MTS data through diverse Gaussian process kernels and the linear coregionalization method, and (2) a novel MTS architecture capable of utilizing both temporal and cross-channel dependencies across all input patches. We evaluate TimePFN on several benchmark datasets and demonstrate that it outperforms the existing state-of-the-art models for MTS forecasting in both zero-shot and few-shot settings. Notably, fine-tuning TimePFN with as few as 500 data points nearly matches full dataset training error, and even 50 data points yield competitive results. We also find that TimePFN exhibits strong univariate forecasting performance, attesting to its generalization ability. Overall, this work unlocks the power of synthetic data priors for MTS forecasting and facilitates strong zero- and few-shot forecasting performance.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "121",
        "title": "DualNeRF: Text-Driven 3D Scene Editing via Dual-Field Representation",
        "author": [
            "Yuxuan Xiong",
            "Yue Shi",
            "Yishun Dou",
            "Bingbing Ni"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16302",
        "abstract": "Recently, denoising diffusion models have achieved promising results in 2D image generation and editing. Instruct-NeRF2NeRF (IN2N) introduces the success of diffusion into 3D scene editing through an \"Iterative dataset update\" (IDU) strategy. Though achieving fascinating results, IN2N suffers from problems of blurry backgrounds and trapping in local optima. The first problem is caused by IN2N's lack of efficient guidance for background maintenance, while the second stems from the interaction between image editing and NeRF training during IDU. In this work, we introduce DualNeRF to deal with these problems. We propose a dual-field representation to preserve features of the original scene and utilize them as additional guidance to the model for background maintenance during IDU. Moreover, a simulated annealing strategy is embedded into IDU to endow our model with the power of addressing local optima issues. A CLIP-based consistency indicator is used to further improve the editing quality by filtering out low-quality edits. Extensive experiments demonstrate that our method outperforms previous methods both qualitatively and quantitatively.",
        "tags": [
            "3D",
            "CLIP",
            "Diffusion",
            "Image Editing",
            "NeRF"
        ]
    },
    {
        "id": "122",
        "title": "Iterative Auto-Annotation for Scientific Named Entity Recognition Using BERT-Based Models",
        "author": [
            "Kartik Gupta"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16312",
        "abstract": "This paper presents an iterative approach to performing Scientific Named Entity Recognition (SciNER) using BERT-based models. We leverage transfer learning to fine-tune pretrained models with a small but high-quality set of manually annotated data. The process is iteratively refined by using the fine-tuned model to auto-annotate a larger dataset, followed by additional rounds of fine-tuning. We evaluated two models, dslim/bert-large-NER and bert-largecased, and found that bert-large-cased consistently outperformed the former. Our approach demonstrated significant improvements in prediction accuracy and F1 scores, especially for less common entity classes. Future work could include pertaining with unlabeled data, exploring more powerful encoders like RoBERTa, and expanding the scope of manual annotations. This methodology has broader applications in NLP tasks where access to labeled data is limited.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "123",
        "title": "Dynamic Coalition Structure Detection in Natural Language-based Interactions",
        "author": [
            "Abhishek N. Kulkarni",
            "Andy Liu",
            "Jean-Raphael Gaglione",
            "Daniel Fried",
            "Ufuk Topcu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16339",
        "abstract": "In strategic multi-agent sequential interactions, detecting dynamic coalition structures is crucial for understanding how self-interested agents coordinate to influence outcomes. However, natural-language-based interactions introduce unique challenges to coalition detection due to ambiguity over intents and difficulty in modeling players' subjective perspectives. We propose a new method that leverages recent advancements in large language models and game theory to predict dynamic multilateral coalition formation in Diplomacy, a strategic multi-agent game where agents negotiate coalitions using natural language. The method consists of two stages. The first stage extracts the set of agreements discussed by two agents in their private dialogue, by combining a parsing-based filtering function with a fine-tuned language model trained to predict player intents. In the second stage, we define a new metric using the concept of subjective rationalizability from hypergame theory to evaluate the expected value of an agreement for each player. We then compute this metric for each agreement identified in the first stage by assessing the strategic value of the agreement for both players and taking into account the subjective belief of one player that the second player would honor the agreement. We demonstrate that our method effectively detects potential coalition structures in online Diplomacy gameplay by assigning high values to agreements likely to be honored and low values to those likely to be violated. The proposed method provides foundational insights into coalition formation in multi-agent environments with language-based negotiation and offers key directions for future research on the analysis of complex natural language-based interactions between agents.",
        "tags": [
            "Detection",
            "Large Language Models"
        ]
    },
    {
        "id": "124",
        "title": "Exploring Sentiment Manipulation by LLM-Enabled Intelligent Trading Agents",
        "author": [
            "David Byrd"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16343",
        "abstract": "Companies across all economic sectors continue to deploy large language models at a rapid pace. Reinforcement learning is experiencing a resurgence of interest due to its association with the fine-tuning of language models from human feedback. Tool-chain language models control task-specific agents; if the converse has not already appeared, it soon will. In this paper, we present what we believe is the first investigation of an intelligent trading agent based on continuous deep reinforcement learning that also controls a large language model with which it can post to a social media feed observed by other traders. We empirically investigate the performance and impact of such an agent in a simulated financial market, finding that it learns to optimize its total reward, and thereby augment its profit, by manipulating the sentiment of the posts it produces. The paper concludes with discussion, limitations, and suggestions for future work.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "125",
        "title": "LegalBench.PT: A Benchmark for Portuguese Law",
        "author": [
            "Beatriz Canaverde",
            "Telmo Pessoa Pires",
            "Leonor Melo Ribeiro",
            "AndrÃ© F. T. Martins"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16357",
        "abstract": "The recent application of LLMs to the legal field has spurred the creation of benchmarks across various jurisdictions and languages. However, no benchmark has yet been specifically designed for the Portuguese legal system. In this work, we present http://LegalBench.PT, the first comprehensive legal benchmark covering key areas of Portuguese law. To develop http://LegalBench.PT, we first collect long-form questions and answers from real law exams, and then use GPT-4o to convert them into multiple-choice, true/false, and matching formats. Once generated, the questions are filtered and processed to improve the quality of the dataset. To ensure accuracy and relevance, we validate our approach by having a legal professional review a sample of the generated questions. Although the questions are synthetically generated, we show that their basis in human-created exams and our rigorous filtering and processing methods applied result in a reliable benchmark for assessing LLMs' legal knowledge and reasoning abilities. Finally, we evaluate the performance of leading LLMs on http://LegalBench.PT and investigate potential biases in GPT-4o's responses. We also assess the performance of Portuguese lawyers on a sample of questions to establish a baseline for model comparison and validate the benchmark.",
        "tags": [
            "GPT",
            "LLMs"
        ]
    },
    {
        "id": "126",
        "title": "Wrong Answers Can Also Be Useful: PlausibleQA -- A Large-Scale QA Dataset with Answer Plausibility Scores",
        "author": [
            "Jamshid Mozafari",
            "Abdelrahman Abdallah",
            "Bhawna Piryani",
            "Adam Jatowt"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16358",
        "abstract": "Large Language Models (LLMs) are revolutionizing information retrieval, with chatbots becoming an important source for answering user queries. As by their design, LLMs prioritize generating correct answers, the value of highly plausible yet incorrect answers (candidate answers) tends to be overlooked. However, such answers can still prove useful, for example, they can play a crucial role in tasks like Multiple-Choice Question Answering (MCQA) and QA Robustness Assessment (QARA). Existing QA datasets primarily focus on correct answers without explicit consideration of the plausibility of other candidate answers, limiting opportunity for more nuanced evaluations of models. To address this gap, we introduce PlausibleQA, a large-scale dataset comprising 10,000 questions and 100,000 candidate answers, each annotated with plausibility scores and justifications for their selection. Additionally, the dataset includes 900,000 justifications for pairwise comparisons between candidate answers, further refining plausibility assessments. We evaluate PlausibleQA through human assessments and empirical experiments, demonstrating its utility in MCQA and QARA analysis. Our findings show that plausibility-aware approaches are effective for MCQA distractor generation and QARA. We release PlausibleQA as a resource for advancing QA research and enhancing LLM performance in distinguishing plausible distractors from correct answers.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "127",
        "title": "Audio Visual Segmentation Through Text Embeddings",
        "author": [
            "Kyungbok Lee",
            "You Zhang",
            "Zhiyao Duan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16359",
        "abstract": "The goal of Audio-Visual Segmentation (AVS) is to localize and segment the sounding source objects from the video frames. Researchers working on AVS suffer from limited datasets because hand-crafted annotation is expensive. Recent works attempt to overcome the challenge of limited data by leveraging the segmentation foundation model, SAM, prompting it with audio to enhance its ability to segment sounding source objects. While this approach alleviates the model's burden on understanding visual modality by utilizing pre-trained knowledge of SAM, it does not address the fundamental challenge of the limited dataset for learning audio-visual relationships. To address these limitations, we propose \\textbf{AV2T-SAM}, a novel framework that bridges audio features with the text embedding space of pre-trained text-prompted SAM. Our method leverages multimodal correspondence learned from rich text-image paired datasets to enhance audio-visual alignment. Furthermore, we introduce a novel feature, $\\mathbf{\\textit{\\textbf{f}}_{CLIP} \\odot \\textit{\\textbf{f}}_{CLAP}}$, which emphasizes shared semantics of audio and visual modalities while filtering irrelevant noise. Experiments on the AVSBench dataset demonstrate state-of-the-art performance on both datasets of AVSBench. Our approach outperforms existing methods by effectively utilizing pretrained segmentation models and cross-modal semantic alignment.",
        "tags": [
            "CLIP",
            "SAM",
            "Segmentation"
        ]
    },
    {
        "id": "128",
        "title": "A generative approach to LLM harmfulness detection with special red flag tokens",
        "author": [
            "Sophie Xhonneux",
            "David Dobre",
            "Mehrnaz Mohfakhami",
            "Leo Schwinn",
            "Gauthier Gidel"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16366",
        "abstract": "Most safety training methods for large language models (LLMs) based on fine-tuning rely on dramatically changing the output distribution of the model when faced with a harmful request, shifting it from an unsafe answer to a refusal to respond. These methods inherently compromise model capabilities and might make auto-regressive models vulnerable to attacks that make likely an initial token of affirmative response. To avoid that, we propose to expand the model's vocabulary with a special token we call red flag token (<rf>) and propose to fine-tune the model to generate this token at any time harmful content is generated or about to be generated. This novel safety training method effectively augments LLMs into generative classifiers of harmfulness at all times during the conversation. This method offers several advantages: it enables the model to explicitly learn the concept of harmfulness while marginally affecting the generated distribution, thus maintaining the model's utility. It also evaluates each generated answer rather than just the input prompt and provides a stronger defence against sampling-based attacks. In addition, it simplifies the evaluation of the model's robustness and reduces correlated failures when combined with a classifier. We further show an increased robustness to long contexts, and supervised fine-tuning attacks.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "129",
        "title": "Concept Corrector: Erase concepts on the fly for text-to-image diffusion models",
        "author": [
            "Zheling Meng",
            "Bo Peng",
            "Xiaochuan Jin",
            "Yueming Lyu",
            "Wei Wang",
            "Jing Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16368",
        "abstract": "Text-to-image diffusion models have demonstrated the underlying risk of generating various unwanted content, such as sexual elements. To address this issue, the task of concept erasure has been introduced, aiming to erase any undesired concepts that the models can generate. Previous methods, whether training-based or training-free, have primarily focused on the input side, i.e. texts. However, they often suffer from incomplete erasure due to limitations in the generalization from limited prompts to diverse image content. In this paper, motivated by the notion that concept erasure on the output side, i.e. generated images, may be more direct and effective, we propose to check concepts based on intermediate-generated images and correct them in the remainder of the generation process. Two key challenges are identified, i.e. determining the presence of target concepts during generation and replacing them on the fly. Leveraging the generation mechanism of diffusion models, we present the Concept Corrector, which incorporates the Generation Check Mechanism and the Concept Removal Attention. This method can identify the generated features associated with target concepts and replace them using pre-defined negative prompts, thereby achieving concept erasure. It requires no changes to model parameters and only relies on a given concept name and its replacement content. To the best of our knowledge, this is the first erasure method based on intermediate-generated images. The experiments on various concepts demonstrate its impressive erasure performance. Code: https://github.com/RichardSunnyMeng/ConceptCorrector.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "130",
        "title": "Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines",
        "author": [
            "Saurabh Srivastava",
            "Sweta Pati",
            "Ziyu Yao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16377",
        "abstract": "In this work, we study the effect of annotation guidelines -- textual descriptions of event types and arguments, when instruction-tuning large language models for event extraction. We conducted a series of experiments with both human-provided and machine-generated guidelines in both full- and low-data settings. Our results demonstrate the promise of annotation guidelines when there is a decent amount of training data and highlight its effectiveness in improving cross-schema generalization and low-frequency event-type performance.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "131",
        "title": "Finding Influential Cores via Normalized Ricci Flows in Directed and Undirected Hypergraphs with Applications",
        "author": [
            "Prithviraj Sengupta",
            "Nazanin Azarhooshang",
            "Reka Albert",
            "Bhaskar DasGupta"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16382",
        "abstract": "Many biological and social systems are naturally represented as edge-weighted directed or undirected hypergraphs since they exhibit group interactions involving three or more system units as opposed to pairwise interactions that can be incorporated in graph-theoretic representations. However, finding influential cores in hypergraphs is still not as extensively studied as their graph-theoretic counter-parts. To this end, we develop and implement a hypergraph-curvature guided discrete time diffusion process with suitable topological surgeries and edge-weight re-normalization procedures for both undirected and directed weighted hypergraphs to find influential cores. We successfully apply our framework for directed hypergraphs to seven metabolic hypergraphs and our framework for undirected hypergraphs to two social (co-authorship) hypergraphs to find influential cores, thereby demonstrating the practical feasibility of our approach. In addition, we prove a theorem showing that a certain edge weight re-normalization procedure in a prior research work for Ricci flows for edge-weighted graphs has the undesirable outcome of modifying the edge-weights to negative numbers, thereby rendering the procedure impossible to use. To the best of our knowledge, this seems to be one of the first articles that formulates algorithmic approaches for finding core(s) of (weighted or unweighted) directed hypergraphs.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "132",
        "title": "Toward a Flexible Framework for Linear Representation Hypothesis Using Maximum Likelihood Estimation",
        "author": [
            "Trung Nguyen",
            "Yan Leng"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16385",
        "abstract": "Linear representation hypothesis posits that high-level concepts are encoded as linear directions in the representation spaces of LLMs. Park et al. (2024) formalize this notion by unifying multiple interpretations of linear representation, such as 1-dimensional subspace representation and interventions, using a causal inner product. However, their framework relies on single-token counterfactual pairs and cannot handle ambiguous contrasting pairs, limiting its applicability to complex or context-dependent concepts. We introduce a new notion of binary concepts as unit vectors in a canonical representation space, and utilize LLMs' (neural) activation differences along with maximum likelihood estimation (MLE) to compute concept directions (i.e., steering vectors). Our method, Sum of Activation-base Normalized Difference (SAND), formalizes the use of activation differences modeled as samples from a von Mises-Fisher (vMF) distribution, providing a principled approach to derive concept directions. We extend the applicability of Park et al. (2024) by eliminating the dependency on unembedding representations and single-token pairs. Through experiments with LLaMA models across diverse concepts and benchmarks, we demonstrate that our lightweight approach offers greater flexibility, superior performance in activation engineering tasks like monitoring and manipulation.",
        "tags": [
            "LLMs",
            "LLaMA"
        ]
    },
    {
        "id": "133",
        "title": "An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science",
        "author": [
            "Qiuhai Zeng",
            "Claire Jin",
            "Xinyue Wang",
            "Yuhan Zheng",
            "Qunhua Li"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16395",
        "abstract": "Large Language Models (LLMs) have demonstrated potential for data science tasks via code generation. However, the exploratory nature of data science, alongside the stochastic and opaque outputs of LLMs, raise concerns about their reliability. While prior work focuses on benchmarking LLM accuracy, reproducibility remains underexplored, despite being critical to establishing trust in LLM-driven analysis.\nWe propose a novel analyst-inspector framework to automatically evaluate and enforce the reproducibility of LLM-generated data science workflows - the first rigorous approach to the best of our knowledge. Defining reproducibility as the sufficiency and completeness of workflows for reproducing functionally equivalent code, this framework enforces computational reproducibility principles, ensuring transparent, well-documented LLM workflows while minimizing reliance on implicit model assumptions.\nUsing this framework, we systematically evaluate five state-of-the-art LLMs on 1,032 data analysis tasks across three diverse benchmark datasets. We also introduce two novel reproducibility-enhancing prompting strategies. Our results show that higher reproducibility strongly correlates with improved accuracy and reproducibility-enhancing prompts are effective, demonstrating structured prompting's potential to enhance automated data science workflows and enable transparent, robust AI-driven analysis. Our code is publicly available.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "134",
        "title": "Ensemble ToT of LLMs and Its Application to Automatic Grading System for Supporting Self-Learning",
        "author": [
            "Yuki Ito",
            "Qiang Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16399",
        "abstract": "Providing students with detailed and timely grading feedback is essential for self-learning. While existing LLM-based grading systems are promising, most of them rely on one single model, which limits their performance. To address this, we propose Ensemble Tree-of-Thought (ToT), a framework that enhances LLM outputs by integrating multiple models. Using this framework, we develop a grading system. Ensemble ToT follows three steps: (1) analyzing LLM performance, (2) generating candidate answers, and (3) refining them into a final result. Based on this, our grading system first evaluates the grading tendencies of LLMs, then generates multiple results, and finally integrates them via a simulated debate. Experimental results demonstrate our approach's ability to provide accurate and explainable grading by effectively coordinating multiple LLMs.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "135",
        "title": "Navigation-GPT: A Robust and Adaptive Framework Utilizing Large Language Models for Navigation Applications",
        "author": [
            "Feng Ma",
            "Xiu-min Wang",
            "Chen Chen",
            "Xiao-bin Xu",
            "Xin-ping Yan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16402",
        "abstract": "Existing navigation decision support systems often perform poorly when handling non-predefined navigation scenarios. Leveraging the generalization capabilities of large language model (LLM) in handling unknown scenarios, this research proposes a dual-core framework for LLM applications to address this issue. Firstly, through ReAct-based prompt engineering, a larger LLM core decomposes intricate navigation tasks into manageable sub-tasks, which autonomously invoke corresponding external tools to gather relevant information, using this feedback to mitigate the risk of LLM hallucinations. Subsequently, a fine-tuned and compact LLM core, acting like a first-mate is designed to process such information and unstructured external data, then to generates context-aware recommendations, ultimately delivering lookout insights and navigation hints that adhere to the International Regulations for Preventing Collisions at Sea (COLREGs) and other rules. Extensive experiments demonstrate the proposed framework not only excels in traditional ship collision avoidance tasks but also adapts effectively to unstructured, non-predefined, and unpredictable scenarios. A comparative analysis with DeepSeek-R1, GPT-4o and other SOTA models highlights the efficacy and rationality of the proposed framework. This research bridges the gap between conventional navigation systems and LLMs, offering a framework to enhance safety and operational efficiency across diverse navigation applications.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "136",
        "title": "M4SC: An MLLM-based Multi-modal, Multi-task and Multi-user Semantic Communication System",
        "author": [
            "Feibo Jiang",
            "Siwei Tu",
            "Li Dong",
            "Kezhi Wang",
            "Kun Yang",
            "Cunhua Pan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16418",
        "abstract": "Multi-modal Large Language Models (MLLMs) are capable of precisely extracting high-level semantic information from multi-modal data, enabling multi-task understanding and generation. This capability facilitates more efficient and intelligent data transmission in semantic communications. In this paper, we design a tailored MLLM for semantic communication and propose an MLLM-based Multi-modal, Multi-task and Multi-user Semantic Communication (M4SC) system. First, we utilize the Kolmogorov-Arnold Network (KAN) to achieve multi-modal alignment in MLLMs, thereby enhancing the accuracy of semantics representation in the semantic space across different modalities. Next, we introduce a multi-task fine-tuning approach based on task instruction following, which leverages a unified task instruction template to describe various semantic communication tasks, improving the MLLM's ability to follow instructions across multiple tasks. Additionally, by designing a semantic sharing mechanism, we transmit the public and private semantic information of multiple users separately, thus increasing the efficiency of semantic communication. Finally, we employ a joint KAN-LLM-channel coding strategy to comprehensively enhance the performance of the semantic communication system in complex communication environments. Experimental results validate the effectiveness and robustness of the proposed M4SC in multi-modal, multi-task, and multi-user scenarios.",
        "tags": [
            "KAN",
            "Large Language Models"
        ]
    },
    {
        "id": "137",
        "title": "High-resolution Rainy Image Synthesis: Learning from Rendering",
        "author": [
            "Kaibin Zhou",
            "Shengjie Zhao",
            "Hao Deng",
            "Lin Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16421",
        "abstract": "Currently, there are few effective methods for synthesizing a mass of high-resolution rainy images in complex illumination conditions. However, these methods are essential for synthesizing large-scale high-quality paired rainy-clean image datasets, which can train deep learning-based single image rain removal models capable of generalizing to various illumination conditions. Therefore, we propose a practical two-stage learning-from-rendering pipeline for high-resolution rainy image synthesis. The pipeline combines the benefits of the realism of rendering-based methods and the high-efficiency of learning-based methods, providing the possibility of creating large-scale high-quality paired rainy-clean image datasets. In the rendering stage, we use a rendering-based method to create a High-resolution Rainy Image (HRI) dataset, which contains realistic high-resolution paired rainy-clean images of multiple scenes and various illumination conditions. In the learning stage, to learn illumination information from background images for high-resolution rainy image generation, we propose a High-resolution Rainy Image Generation Network (HRIGNet). HRIGNet is designed to introduce a guiding diffusion model in the Latent Diffusion Model, which provides additional guidance information for high-resolution image synthesis. In our experiments, HRIGNet is able to synthesize high-resolution rainy images up to 2048x1024 resolution. Rain removal experiments on real dataset validate that our method can help improve the robustness of deep derainers to real rainy images. To make our work reproducible, source codes and the dataset have been released at https://kb824999404.github.io/HRIG/.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "138",
        "title": "Unified Prompt Attack Against Text-to-Image Generation Models",
        "author": [
            "Duo Peng",
            "Qiuhong Ke",
            "Mark He Huang",
            "Ping Hu",
            "Jun Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16423",
        "abstract": "Text-to-Image (T2I) models have advanced significantly, but their growing popularity raises security concerns due to their potential to generate harmful images. To address these issues, we propose UPAM, a novel framework to evaluate the robustness of T2I models from an attack perspective. Unlike prior methods that focus solely on textual defenses, UPAM unifies the attack on both textual and visual defenses. Additionally, it enables gradient-based optimization, overcoming reliance on enumeration for improved efficiency and effectiveness. To handle cases where T2I models block image outputs due to defenses, we introduce Sphere-Probing Learning (SPL) to enable optimization even without image results. Following SPL, our model bypasses defenses, inducing the generation of harmful content. To ensure semantic alignment with attacker intent, we propose Semantic-Enhancing Learning (SEL) for precise semantic control. UPAM also prioritizes the naturalness of adversarial prompts using In-context Naturalness Enhancement (INE), making them harder for human examiners to detect. Additionally, we address the issue of iterative queries--common in prior methods and easily detectable by API defenders--by introducing Transferable Attack Learning (TAL), allowing effective attacks with minimal queries. Extensive experiments validate UPAM's superiority in effectiveness, efficiency, naturalness, and low query detection rates.",
        "tags": [
            "Detection",
            "Text-to-Image"
        ]
    },
    {
        "id": "139",
        "title": "Lightweight Vision Model-based Multi-user Semantic Communication Systems",
        "author": [
            "Feibo Jiang",
            "Siwei Tu",
            "Li Dong",
            "Kezhi Wang",
            "Kun Yang",
            "Ruiqi Liu",
            "Cunhua Pan",
            "Jiangzhou Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16424",
        "abstract": "Semantic Communication (SemCom) is a promising new paradigm for next-generation communication systems, emphasizing the transmission of core information, particularly in environments characterized by uncertainty, noise, and bandwidth constraints. However, existing image SemCom systems face several challenges, such as inefficient knowledge base construction, insufficient semantic encoding, and lack of multi-user semantic sharing. To address these issues, we propose a Lightweight Vision Model-based Multi-user Semantic Communication System (LVM-MSC). First, we construct a Lightweight Knowledge Base (LKB) based on the fast Segment Anything Model (SAM). LKB incorporates the extensive image knowledge of the SAM model while significantly reducing the number of parameters through its convolutional architecture. Next, we design an Efficient Semantic Codec (ESC) based on the Masked AutoEncoder (MAE) architecture. ESC enhances semantic compression at both the pixel and semantic levels and implements lightweight semantic decoding tailored for user devices. Furthermore, we propose a Multi-user Semantic Sharing (MSS) transmission for the multi-user SemCom. By calculating the similarity of semantic information among different users in the sharing semantic space, we unify the transmissions of similar semantic information through broadcasting, further improving the transmission efficiency. Finally, simulation results demonstrate the feasibility and effectiveness of the proposed LVM-MSC system.",
        "tags": [
            "SAM",
            "Segment Anything"
        ]
    },
    {
        "id": "140",
        "title": "Visual Reasoning Evaluation of Grok, Deepseek Janus, Gemini, Qwen, Mistral, and ChatGPT",
        "author": [
            "Nidhal Jegham",
            "Marwan Abdelatti",
            "Abdeltawab Hendawi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16428",
        "abstract": "Traditional evaluations of multimodal large language models (LLMs) have been limited by their focus on single-image reasoning, failing to assess crucial aspects like contextual understanding, reasoning stability, and uncertainty calibration. This study addresses these limitations by introducing a novel benchmark that integrates multi-image reasoning tasks with rejection-based evaluation and positional bias detection. To evaluate these dimensions, we further introduce entropy as a novel metric for quantifying reasoning consistency across reordered answer variants. We applied this benchmark to assess Grok 3, ChatGPT-4o, ChatGPT-o1, Gemini 2.0 Flash Experimental, DeepSeek Janus models, Qwen2.5-VL-72B-Instruct, QVQ-72B-Preview, and Pixtral 12B across eight visual reasoning tasks, including difference spotting and diagram interpretation. Our findings reveal ChatGPT-o1 leading in overall accuracy (82.5\\%) and rejection accuracy (70.0\\%), closely followed by Gemini 2.0 Flash Experimental (70.8\\%). QVQ-72B-Preview demonstrated superior rejection accuracy (85.5\\%). Notably, Pixtral 12B (51.7\\%) showed promise in specific domains, while Janus models exhibited challenges in bias and uncertainty calibration, reflected in low rejection accuracies and high entropy scores. High entropy scores in Janus models (Janus 7B: 0.8392, Janus 1B: 0.787) underscore their susceptibility to positional bias and unstable reasoning, contrasting with the low entropy and robust reasoning of ChatGPT models. The study further demonstrates that model size is not the sole determinant of performance, as evidenced by Grok 3 underperformance despite its substantial parameter count. By employing multi-image contexts, rejection mechanisms, and entropy-based consistency metrics, this benchmark sets a new standard for evaluating multimodal LLMs, enabling a more robust and reliable assessment of next-generation AI systems.",
        "tags": [
            "ChatGPT",
            "DeepSeek",
            "Detection",
            "LLMs",
            "Large Language Models",
            "Qwen"
        ]
    },
    {
        "id": "141",
        "title": "Sequence-level Large Language Model Training with Contrastive Preference Optimization",
        "author": [
            "Zhili Feng",
            "Dhananjay Ram",
            "Cole Hawkins",
            "Aditya Rawal",
            "Jinman Zhao",
            "Sheng Zha"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16433",
        "abstract": "The next token prediction loss is the dominant self-supervised training objective for large language models and has achieved promising results in a variety of downstream tasks. However, upon closer investigation of this objective, we find that it lacks an understanding of sequence-level signals, leading to a mismatch between training and inference processes. To bridge this gap, we introduce a contrastive preference optimization (CPO) procedure that can inject sequence-level information into the language model at any training stage without expensive human labeled data. Our experiments show that the proposed objective surpasses the next token prediction in terms of win rate in the instruction-following and text generation tasks.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "142",
        "title": "VisFactor: Benchmarking Fundamental Visual Cognition in Multimodal Large Language Models",
        "author": [
            "Jen-Tse Huang",
            "Dasen Dai",
            "Jen-Yuan Huang",
            "Youliang Yuan",
            "Xiaoyuan Liu",
            "Wenxuan Wang",
            "Wenxiang Jiao",
            "Pinjia He",
            "Zhaopeng Tu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16435",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in multimodal understanding; however, their fundamental visual cognitive abilities remain largely underexplored. To bridge this gap, we introduce VisFactor, a novel benchmark derived from the Factor-Referenced Cognitive Test (FRCT), a well-established psychometric assessment of human cognition. VisFactor digitalizes vision-related FRCT subtests to systematically evaluate MLLMs across essential visual cognitive tasks including spatial reasoning, perceptual speed, and pattern recognition. We present a comprehensive evaluation of state-of-the-art MLLMs, such as GPT-4o, Gemini-Pro, and Qwen-VL, using VisFactor under diverse prompting strategies like Chain-of-Thought and Multi-Agent Debate. Our findings reveal a concerning deficiency in current MLLMs' fundamental visual cognition, with performance frequently approaching random guessing and showing only marginal improvements even with advanced prompting techniques. These results underscore the critical need for focused research to enhance the core visual reasoning capabilities of MLLMs. To foster further investigation in this area, we release our VisFactor benchmark at https://github.com/CUHK-ARISE/VisFactor.",
        "tags": [
            "GPT",
            "Large Language Models",
            "Qwen"
        ]
    },
    {
        "id": "143",
        "title": "Compression Scaling Laws:Unifying Sparsity and Quantization",
        "author": [
            "Elias Frantar",
            "Utku Evci",
            "Wonpyo Park",
            "Neil Houlsby",
            "Dan Alistarh"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16440",
        "abstract": "We investigate how different compression techniques -- such as weight and activation quantization, and weight sparsity -- affect the scaling behavior of large language models (LLMs) during pretraining. Building on previous work showing that weight sparsity acts as a constant multiplier on model size in scaling laws, we demonstrate that this \"effective parameter\" scaling pattern extends to quantization as well. Specifically, we establish that weight-only quantization achieves strong parameter efficiency multipliers, while full quantization of both weights and activations shows diminishing returns at lower bitwidths. Our results suggest that different compression techniques can be unified under a common scaling law framework, enabling principled comparison and combination of these methods.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "144",
        "title": "Iterative Flow Matching -- Path Correction and Gradual Refinement for Enhanced Generative Modeling",
        "author": [
            "Eldad Haber",
            "Shadab Ahamed",
            "Md. Shahriar Rahim Siddiqui",
            "Niloufar Zakariaei",
            "Moshe Eliasof"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16445",
        "abstract": "Generative models for image generation are now commonly used for a wide variety of applications, ranging from guided image generation for entertainment to solving inverse problems. Nonetheless, training a generator is a non-trivial feat that requires fine-tuning and can lead to so-called hallucinations, that is, the generation of images that are unrealistic. In this work, we explore image generation using flow matching. We explain and demonstrate why flow matching can generate hallucinations, and propose an iterative process to improve the generation process. Our iterative process can be integrated into virtually $\\textit{any}$ generative modeling technique, thereby enhancing the performance and robustness of image synthesis systems.",
        "tags": [
            "Flow Matching"
        ]
    },
    {
        "id": "145",
        "title": "An alternating approach for reconstructing the initial value and source term in a time-fractional diffusion-wave equation",
        "author": [
            "Yun Zhang",
            "Xiaoli Feng",
            "Xiongbin Yan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16453",
        "abstract": "This paper is dedicated to addressing the simultaneous inversion problem involving the initial value and space-dependent source term in a time-fractional diffusion-wave equation. Firstly, we establish the uniqueness of the inverse problem by leveraging the asymptotic expansion of Mittag-Leffler functions. Subsequently, we decompose the inverse problem into two subproblems and introduce an alternating iteration reconstruction method, complemented by a regularization strategy. Additionally, a comprehensive convergence analysis for this method is provided. To solve the inverse problem numerically, we introduce two semidiscrete schemes based on standard Galerkin method and lumped mass method, respectively. Furthermore, we establish error estimates that are associated with the noise level, iteration step, regularization parameter, and spatial discretization parameter. Finally, we present several numerical experiments in both one-dimensional and two-dimensional cases to validate the theoretical results and demonstrate the effectiveness of our proposed method.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "146",
        "title": "MAPN: Enhancing Heterogeneous Sparse Graph Representation by Mamba-based Asynchronous Aggregation",
        "author": [
            "Xuqi Mao",
            "Zhenying He",
            "X. Sean Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16454",
        "abstract": "Graph neural networks (GNNs) have become the state of the art for various graph-related tasks and are particularly prominent in heterogeneous graphs (HetGs). However, several issues plague this paradigm: first, the difficulty in fully utilizing long-range information, known as over-squashing; second, the tendency for excessive message-passing layers to produce indistinguishable representations, referred to as over-smoothing; and finally, the inadequacy of conventional MPNNs to train effectively on large sparse graphs. To address these challenges in deep neural networks for large-scale heterogeneous graphs, this paper introduces the Mamba-based Asynchronous Propagation Network (MAPN), which enhances the representation of heterogeneous sparse graphs. MAPN consists of two primary components: node sequence generation and semantic information aggregation. Node sequences are initially generated based on meta-paths through random walks, which serve as the foundation for a spatial state model that extracts essential information from nodes at various distances. It then asynchronously aggregates semantic information across multiple hops and layers, effectively preserving unique node characteristics and mitigating issues related to deep network degradation. Extensive experiments across diverse datasets demonstrate the effectiveness of MAPN in graph embeddings for various downstream tasks underscoring its substantial benefits for graph representation in large sparse heterogeneous graphs.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "147",
        "title": "Towards Fully-Automated Materials Discovery via Large-Scale Synthesis Dataset and Expert-Level LLM-as-a-Judge",
        "author": [
            "Heegyu Kim",
            "Taeyang Jeon",
            "Seungtaek Choi",
            "Jihoon Hong",
            "Dongwon Jeon",
            "Sungbum Cho",
            "Ga-Yeon Baek",
            "Kyung-Won Kwak",
            "Dong-Hee Lee",
            "Sun-Jin Choi",
            "Jisu Bae",
            "Chihoon Lee",
            "Yunseo Kim",
            "Jinsung Park",
            "Hyunsouk Cho"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16457",
        "abstract": "Materials synthesis is vital for innovations such as energy storage, catalysis, electronics, and biomedical devices. Yet, the process relies heavily on empirical, trial-and-error methods guided by expert intuition. Our work aims to support the materials science community by providing a practical, data-driven resource. We have curated a comprehensive dataset of 17K expert-verified synthesis recipes from open-access literature, which forms the basis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an end-to-end framework that supports research in large language models applied to synthesis prediction. It encompasses key tasks, including raw materials and equipment prediction, synthesis procedure generation, and characterization outcome forecasting. We propose an LLM-as-a-Judge framework that leverages large language models for automated evaluation, demonstrating strong statistical agreement with expert assessments. Overall, our contributions offer a supportive foundation for exploring the capabilities of LLMs in predicting and guiding materials synthesis, ultimately paving the way for more efficient experimental design and accelerated innovation in materials science.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "148",
        "title": "TerEffic: Highly Efficient Ternary LLM Inference on FPGA",
        "author": [
            "Chenyang Yin",
            "Zhenyu Bai",
            "Pranav Venkatram",
            "Shivam Aggarwal",
            "Zhaoying Li",
            "Tulika Mitra"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16473",
        "abstract": "Large Language Model (LLM) deployment on edge devices is typically constrained by the need for off-chip memory access, leading to high power consumption and limited throughput. Ternary quantization for LLMs is promising in maintaining model accuracy while reducing memory footprint. However, existing accelerators have not exploited this potential for on-chip inference. We present TerEffic, an FPGA-based accelerator that carefully co-designs memory architecture and computational units to unlock highly efficient LLM inference with fully on-chip execution. Through weight compression, custom computational units, and memory hierarchy optimization, we achieve unprecedented efficiency by eliminating off-chip memory bandwidth bottlenecks. We propose two architectural variants: a fully on-chip design for smaller models and an HBM-assisted design for larger ones. When evaluated on a 370M parameter model with single-batch inference, our on-chip design achieves 12,700 tokens/sec (149 times higher than NVIDIA's Jetson Orin Nano) with a power efficiency of 467 tokens/sec/W (19 times better than Jetson Orin Nano). The HBM-assisted design provides 521 tokens/sec on a 2.7B parameter model (2 times higher than NVIDIA's A100) with 33W power consumption, achieving a power efficiency of 16 tokens/sec/W (8 times better than A100).",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "149",
        "title": "Dragen3D: Multiview Geometry Consistent 3D Gaussian Generation with Drag-Based Control",
        "author": [
            "Jinbo Yan",
            "Alan Zhao",
            "Yixin Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16475",
        "abstract": "Single-image 3D generation has emerged as a prominent research topic, playing a vital role in virtual reality, 3D modeling, and digital content creation. However, existing methods face challenges such as a lack of multi-view geometric consistency and limited controllability during the generation process, which significantly restrict their usability. % To tackle these challenges, we introduce Dragen3D, a novel approach that achieves geometrically consistent and controllable 3D generation leveraging 3D Gaussian Splatting (3DGS). We introduce the Anchor-Gaussian Variational Autoencoder (Anchor-GS VAE), which encodes a point cloud and a single image into anchor latents and decode these latents into 3DGS, enabling efficient latent-space generation. To enable multi-view geometry consistent and controllable generation, we propose a Seed-Point-Driven strategy: first generate sparse seed points as a coarse geometry representation, then map them to anchor latents via the Seed-Anchor Mapping Module. Geometric consistency is ensured by the easily learned sparse seed points, and users can intuitively drag the seed points to deform the final 3DGS geometry, with changes propagated through the anchor latents. To the best of our knowledge, we are the first to achieve geometrically controllable 3D Gaussian generation and editing without relying on 2D diffusion priors, delivering comparable 3D generation quality to state-of-the-art methods.",
        "tags": [
            "3D",
            "Diffusion",
            "Gaussian Splatting",
            "VAE"
        ]
    },
    {
        "id": "150",
        "title": "A Split-Window Transformer for Multi-Model Sequence Spammer Detection using Multi-Model Variational Autoencoder",
        "author": [
            "Zhou Yang",
            "Yucai Pang",
            "Hongbo Yin",
            "Yunpeng Xiao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16483",
        "abstract": "This paper introduces a new Transformer, called MS$^2$Dformer, that can be used as a generalized backbone for multi-modal sequence spammer detection. Spammer detection is a complex multi-modal task, thus the challenges of applying Transformer are two-fold. Firstly, complex multi-modal noisy information about users can interfere with feature mining. Secondly, the long sequence of users' historical behaviors also puts a huge GPU memory pressure on the attention computation. To solve these problems, we first design a user behavior Tokenization algorithm based on the multi-modal variational autoencoder (MVAE). Subsequently, a hierarchical split-window multi-head attention (SW/W-MHA) mechanism is proposed. The split-window strategy transforms the ultra-long sequences hierarchically into a combination of intra-window short-term and inter-window overall attention. Pre-trained on the public datasets, MS$^2$Dformer's performance far exceeds the previous state of the art. The experiments demonstrate MS$^2$Dformer's ability to act as a backbone.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "151",
        "title": "A Fine-Tuning Approach for T5 Using Knowledge Graphs to Address Complex Tasks",
        "author": [
            "Xiaoxuan Liao",
            "Binrong Zhu",
            "Jacky He",
            "Guiran Liu",
            "Hongye Zheng",
            "Jia Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16484",
        "abstract": "With the development of deep learning technology, large language models have achieved remarkable results in many natural language processing tasks. However, these models still have certain limitations in handling complex reasoning tasks and understanding rich background knowledge. To solve this problem, this study proposed a T5 model fine-tuning method based on knowledge graphs, which enhances the model's reasoning ability and context understanding ability by introducing external knowledge graphs. We used the SQuAD1.1 dataset for experiments. The experimental results show that the T5 model based on knowledge graphs is significantly better than other baseline models in reasoning accuracy, context understanding, and the ability to handle complex problems. At the same time, we also explored the impact of knowledge graphs of different scales on model performance and found that as the scale of the knowledge graph increases, the performance of the model gradually improves. Especially when dealing with complex problems, the introduction of knowledge graphs greatly improves the reasoning ability of the T5 model. Ablation experiments further verify the importance of entity and relationship embedding in the model and prove that a complete knowledge graph is crucial to improving the various capabilities of the T5 model. In summary, this study provides an effective method to enhance the reasoning and understanding capabilities of large language models and provides new directions for future research.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "152",
        "title": "MQADet: A Plug-and-Play Paradigm for Enhancing Open-Vocabulary Object Detection via Multimodal Question Answering",
        "author": [
            "Caixiong Li",
            "Xiongwei Zhao",
            "Jinhang Zhang",
            "Xing Zhang",
            "Zhou Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16486",
        "abstract": "Open-vocabulary detection (OVD) is a challenging task to detect and classify objects from an unrestricted set of categories, including those unseen during training. Existing open-vocabulary detectors are limited by complex visual-textual misalignment and long-tailed category imbalances, leading to suboptimal performance in challenging scenarios. To address these limitations, we introduce \\textbf{MQADet}, a universal paradigm for enhancing existing open-vocabulary detectors by leveraging the cross-modal reasoning capabilities of multimodal large language models (MLLMs). MQADet functions as a plug-and-play solution that integrates seamlessly with pre-trained object detectors without substantial additional training costs. Specifically, we design a novel three-stage Multimodal Question Answering (MQA) pipeline to guide the MLLMs to precisely localize complex textual and visual targets while effectively enhancing the focus of existing object detectors on relevant objects. To validate our approach, we present a new benchmark for evaluating our paradigm on four challenging open-vocabulary datasets, employing three state-of-the-art object detectors as baselines. Experimental results demonstrate that our proposed paradigm significantly improves the performance of existing detectors, particularly in unseen complex categories, across diverse and challenging scenarios. To facilitate future research, we will publicly release our code.",
        "tags": [
            "Detection",
            "Large Language Models"
        ]
    },
    {
        "id": "153",
        "title": "Intrinsic Model Weaknesses: How Priming Attacks Unveil Vulnerabilities in Large Language Models",
        "author": [
            "Yuyi Huang",
            "Runzhe Zhan",
            "Derek F. Wong",
            "Lidia S. Chao",
            "Ailin Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16491",
        "abstract": "Large language models (LLMs) have significantly influenced various industries but suffer from a critical flaw, the potential sensitivity of generating harmful content, which poses severe societal risks. We developed and tested novel attack strategies on popular LLMs to expose their vulnerabilities in generating inappropriate content. These strategies, inspired by psychological phenomena such as the \"Priming Effect\", \"Safe Attention Shift\", and \"Cognitive Dissonance\", effectively attack the models' guarding mechanisms. Our experiments achieved an attack success rate (ASR) of 100% on various open-source models, including Meta's Llama-3.2, Google's Gemma-2, Mistral's Mistral-NeMo, Falcon's Falcon-mamba, Apple's DCLM, Microsoft's Phi3, and Qwen's Qwen2.5, among others. Similarly, for closed-source models such as OpenAI's GPT-4o, Google's Gemini-1.5, and Claude-3.5, we observed an ASR of at least 95% on the AdvBench dataset, which represents the current state-of-the-art. This study underscores the urgent need to reassess the use of generative models in critical applications to mitigate potential adverse societal impacts.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models",
            "Mamba",
            "Qwen"
        ]
    },
    {
        "id": "154",
        "title": "Orchestrating Joint Offloading and Scheduling for Low-Latency Edge SLAM",
        "author": [
            "Yao Zhang",
            "Yuyi Mao",
            "Hui Wang",
            "Zhiwen Yu",
            "Song Guo",
            "Jun Zhang",
            "Liang Wang",
            "Bin Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16495",
        "abstract": "Visual Simultaneous Localization and Mapping (vSLAM) is a prevailing technology for many emerging robotic applications. Achieving real-time SLAM on mobile robotic systems with limited computational resources is challenging because the complexity of SLAM algorithms increases over time. This restriction can be lifted by offloading computations to edge servers, forming the emerging paradigm of edge-assisted SLAM. Nevertheless, the exogenous and stochastic input processes affect the dynamics of the edge-assisted SLAM system. Moreover, the requirements of clients on SLAM metrics change over time, exerting implicit and time-varying effects on the system. In this paper, we aim to push the limit beyond existing edge-assist SLAM by proposing a new architecture that can handle the input-driven processes and also satisfy clients' implicit and time-varying requirements. The key innovations of our work involve a regional feature prediction method for importance-aware local data processing, a configuration adaptation policy that integrates data compression/decompression and task offloading, and an input-dependent learning framework for task scheduling with constraint satisfaction. Extensive experiments prove that our architecture improves pose estimation accuracy and saves up to 47% of communication costs compared with a popular edge-assisted SLAM system, as well as effectively satisfies the clients' requirements.",
        "tags": [
            "Pose Estimation",
            "SLAM"
        ]
    },
    {
        "id": "155",
        "title": "PMAT: Optimizing Action Generation Order in Multi-Agent Reinforcement Learning",
        "author": [
            "Kun Hu",
            "Muning Wen",
            "Xihuai Wang",
            "Shao Zhang",
            "Yiwei Shi",
            "Minne Li",
            "Minglong Li",
            "Ying Wen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16496",
        "abstract": "Multi-agent reinforcement learning (MARL) faces challenges in coordinating agents due to complex interdependencies within multi-agent systems. Most MARL algorithms use the simultaneous decision-making paradigm but ignore the action-level dependencies among agents, which reduces coordination efficiency. In contrast, the sequential decision-making paradigm provides finer-grained supervision for agent decision order, presenting the potential for handling dependencies via better decision order management. However, determining the optimal decision order remains a challenge. In this paper, we introduce Action Generation with Plackett-Luce Sampling (AGPS), a novel mechanism for agent decision order optimization. We model the order determination task as a Plackett-Luce sampling process to address issues such as ranking instability and vanishing gradient during the network training process. AGPS realizes credit-based decision order determination by establishing a bridge between the significance of agents' local observations and their decision credits, thus facilitating order optimization and dependency management. Integrating AGPS with the Multi-Agent Transformer, we propose the Prioritized Multi-Agent Transformer (PMAT), a sequential decision-making MARL algorithm with decision order optimization. Experiments on benchmarks including StarCraft II Multi-Agent Challenge, Google Research Football, and Multi-Agent MuJoCo show that PMAT outperforms state-of-the-art algorithms, greatly enhancing coordination efficiency.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "156",
        "title": "FanChuan: A Multilingual and Graph-Structured Benchmark For Parody Detection and Analysis",
        "author": [
            "Yilun Zheng",
            "Sha Li",
            "Fangkun Wu",
            "Yang Ziyi",
            "Lin Hongchao",
            "Zhichao Hu",
            "Cai Xinjun",
            "Ziming Wang",
            "Jinxuan Chen",
            "Sitao Luan",
            "Jiahao Xu",
            "Lihui Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16503",
        "abstract": "Parody is an emerging phenomenon on social media, where individuals imitate a role or position opposite to their own, often for humor, provocation, or controversy. Detecting and analyzing parody can be challenging and is often reliant on context, yet it plays a crucial role in understanding cultural values, promoting subcultures, and enhancing self-expression. However, the study of parody is hindered by limited available data and deficient diversity in current datasets. To bridge this gap, we built seven parody datasets from both English and Chinese corpora, with 14,755 annotated users and 21,210 annotated comments in total. To provide sufficient context information, we also collect replies and construct user-interaction graphs to provide richer contextual information, which is lacking in existing datasets. With these datasets, we test traditional methods and Large Language Models (LLMs) on three key tasks: (1) parody detection, (2) comment sentiment analysis with parody, and (3) user sentiment analysis with parody. Our extensive experiments reveal that parody-related tasks still remain challenging for all models, and contextual information plays a critical role. Interestingly, we find that, in certain scenarios, traditional sentence embedding methods combined with simple classifiers can outperform advanced LLMs, i.e. DeepSeek-R1 and GPT-o3, highlighting parody as a significant challenge for LLMs.",
        "tags": [
            "DeepSeek",
            "Detection",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "157",
        "title": "Path Planning using Instruction-Guided Probabilistic Roadmaps",
        "author": [
            "Jiaqi Bao",
            "Ryo Yonetani"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16515",
        "abstract": "This work presents a novel data-driven path planning algorithm named Instruction-Guided Probabilistic Roadmap (IG-PRM). Despite the recent development and widespread use of mobile robot navigation, the safe and effective travels of mobile robots still require significant engineering effort to take into account the constraints of robots and their tasks. With IG-PRM, we aim to address this problem by allowing robot operators to specify such constraints through natural language instructions, such as ``aim for wider paths'' or ``mind small gaps''. The key idea is to convert such instructions into embedding vectors using large-language models (LLMs) and use the vectors as a condition to predict instruction-guided cost maps from occupancy maps. By constructing a roadmap based on the predicted costs, we can find instruction-guided paths via the standard shortest path search. Experimental results demonstrate the effectiveness of our approach on both synthetic and real-world indoor navigation environments.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Robot"
        ]
    },
    {
        "id": "158",
        "title": "Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension",
        "author": [
            "Yulong Wu",
            "Viktor Schlegel",
            "Riza Batista-Navarro"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16523",
        "abstract": "As neural language models achieve human-comparable performance on Machine Reading Comprehension (MRC) and see widespread adoption, ensuring their robustness in real-world scenarios has become increasingly important. Current robustness evaluation research, though, primarily develops synthetic perturbation methods, leaving unclear how well they reflect real life scenarios. Considering this, we present a framework to automatically examine MRC models on naturally occurring textual perturbations, by replacing paragraph in MRC benchmarks with their counterparts based on available Wikipedia edit history. Such perturbation type is natural as its design does not stem from an arteficial generative process, inherently distinct from the previously investigated synthetic approaches. In a large-scale study encompassing SQUAD datasets and various model architectures we observe that natural perturbations result in performance degradation in pre-trained encoder language models. More worryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs) inherit these errors. Further experiments demonstrate that our findings generalise to natural perturbations found in other more challenging MRC benchmarks. In an effort to mitigate these errors, we show that it is possible to improve the robustness to natural perturbations by training on naturally or synthetically perturbed examples, though a noticeable gap still remains compared to performance on unperturbed data.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "159",
        "title": "Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual Program Generation",
        "author": [
            "Deokhyung Kang",
            "Jeonghun Cho",
            "Yejin Jeon",
            "Sunbin Jang",
            "Minsub Lee",
            "Jawoon Cho",
            "Gary Geunbae Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16529",
        "abstract": "Visual programming languages (VPLs) allow users to create programs through graphical interfaces, which results in easier accessibility and their widespread usage in various domains. To further enhance this accessibility, recent research has focused on generating VPL code from user instructions using large language models (LLMs). Specifically, by employing prompting-based methods, these studies have shown promising results. Nevertheless, such approaches can be less effective for industrial VPLs such as Ladder Diagram (LD). LD is a pivotal language used in industrial automation processes and involves extensive domain-specific configurations, which are difficult to capture in a single prompt. In this work, we demonstrate that training-based methods outperform prompting-based methods for LD generation accuracy, even with smaller backbone models. Building on these findings, we propose a two-stage training strategy to further enhance VPL generation. First, we employ retrieval-augmented fine-tuning to leverage the repetitive use of subroutines commonly seen in industrial VPLs. Second, we apply direct preference optimization (DPO) to further guide the model toward accurate outputs, using systematically generated preference pairs through graph editing operations. Extensive experiments on real-world LD data demonstrate that our approach improves program-level accuracy by over 10% compared to supervised fine-tuning, which highlights its potential to advance industrial automation.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "160",
        "title": "Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs",
        "author": [
            "Jonathan RystrÃ¸m",
            "Hannah Rose Kirk",
            "Scott Hale"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16534",
        "abstract": "Large Language Models (LLMs) are becoming increasingly capable across global languages. However, the ability to communicate across languages does not necessarily translate to appropriate cultural representations. A key concern is US-centric bias, where LLMs reflect US rather than local cultural values. We propose a novel methodology that compares LLM-generated response distributions against population-level opinion data from the World Value Survey across four languages (Danish, Dutch, English, and Portuguese). Using a rigorous linear mixed-effects regression framework, we compare two families of models: Google's Gemma models (2B--27B parameters) and successive iterations of OpenAI's turbo-series. Across the families of models, we find no consistent relationships between language capabilities and cultural alignment. While the Gemma models have a positive correlation between language capability and cultural alignment across languages, the OpenAI models do not. Importantly, we find that self-consistency is a stronger predictor of multicultural alignment than multilingual capabilities. Our results demonstrate that achieving meaningful cultural alignment requires dedicated effort beyond improving general language capabilities.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "161",
        "title": "Rebalancing the Scales: A Systematic Mapping Study of Generative Adversarial Networks (GANs) in Addressing Data Imbalance",
        "author": [
            "Pankaj Yadav",
            "Gulshan Sihag",
            "Vivek Vijay"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16535",
        "abstract": "Machine learning algorithms are used in diverse domains, many of which face significant challenges due to data imbalance. Studies have explored various approaches to address the issue, like data preprocessing, cost-sensitive learning, and ensemble methods. Generative Adversarial Networks (GANs) showed immense potential as a data preprocessing technique that generates good quality synthetic data. This study employs a systematic mapping methodology to analyze 3041 papers on GAN-based sampling techniques for imbalanced data sourced from four digital libraries. A filtering process identified 100 key studies spanning domains such as healthcare, finance, and cybersecurity. Through comprehensive quantitative analysis, this research introduces three categorization mappings as application domains, GAN techniques, and GAN variants used to handle the imbalanced nature of the data. GAN-based over-sampling emerges as an effective preprocessing method. Advanced architectures and tailored frameworks helped GANs to improve further in the case of data imbalance. GAN variants like vanilla GAN, CTGAN, and CGAN show great adaptability in structured imbalanced data cases. Interest in GANs for imbalanced data has grown tremendously, touching a peak in recent years, with journals and conferences playing crucial roles in transmitting foundational theories and practical applications. While with these advances, none of the reviewed studies explicitly explore hybridized GAN frameworks with diffusion models or reinforcement learning techniques. This gap leads to a future research idea develop innovative approaches for effectively handling data imbalance.",
        "tags": [
            "Diffusion",
            "GAN"
        ]
    },
    {
        "id": "162",
        "title": "Advanced Chain-of-Thought Reasoning for Parameter Extraction from Documents Using Large Language Models",
        "author": [
            "Hong Cai Chen",
            "Yi Pin Xu",
            "Yang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16540",
        "abstract": "Extracting parameters from technical documentation is crucial for ensuring design precision and simulation reliability in electronic design. However, current methods struggle to handle high-dimensional design data and meet the demands of real-time processing. In electronic design automation (EDA), engineers often manually search through extensive documents to retrieve component parameters required for constructing PySpice models, a process that is both labor-intensive and time-consuming. To address this challenge, we propose an innovative framework that leverages large language models (LLMs) to automate the extraction of parameters and the generation of PySpice models directly from datasheets. Our framework introduces three Chain-of-Thought (CoT) based techniques: (1) Targeted Document Retrieval (TDR), which enables the rapid identification of relevant technical sections; (2) Iterative Retrieval Optimization (IRO), which refines the parameter search through iterative improvements; and (3) Preference Optimization (PO), which dynamically prioritizes key document sections based on relevance. Experimental results show that applying all three methods together improves retrieval precision by 47.69% and reduces processing latency by 37.84%. Furthermore, effect size analysis using Cohen's d reveals that PO significantly reduces latency, while IRO contributes most to precision enhancement. These findings underscore the potential of our framework to streamline EDA processes, enhance design accuracy, and shorten development timelines. Additionally, our algorithm has model-agnostic generalization, meaning it can improve parameter search performance across different LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "163",
        "title": "Reasoning About Persuasion: Can LLMs Enable Explainable Propaganda Detection?",
        "author": [
            "Maram Hasanain",
            "Md Arid Hasan",
            "Mohamed Bayan Kmainasi",
            "Elisa Sartori",
            "Ali Ezzat Shahroor",
            "Giovanni Da San Martino",
            "Firoj Alam"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16550",
        "abstract": "There has been significant research on propagandistic content detection across different modalities and languages. However, most studies have primarily focused on detection, with little attention given to explanations justifying the predicted label. This is largely due to the lack of resources that provide explanations alongside annotated labels. To address this issue, we propose a multilingual (i.e., Arabic and English) explanation-enhanced dataset, the first of its kind. Additionally, we introduce an explanation-enhanced LLM for both label detection and rationale-based explanation generation. Our findings indicate that the model performs comparably while also generating explanations. We will make the dataset and experimental resources publicly available for the research community.",
        "tags": [
            "Detection",
            "LLMs"
        ]
    },
    {
        "id": "164",
        "title": "Entropy-Lens: The Information Signature of Transformer Computations",
        "author": [
            "Riccardo Ali",
            "Francesco Caso",
            "Christopher Irwin",
            "Pietro LiÃ²"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16570",
        "abstract": "Transformer models have revolutionized fields from natural language processing to computer vision, yet their internal computational dynamics remain poorly understood raising concerns about predictability and robustness. In this work, we introduce Entropy-Lens, a scalable, model-agnostic framework that leverages information theory to interpret frozen, off-the-shelf large-scale transformers. By quantifying the evolution of Shannon entropy within intermediate residual streams, our approach extracts computational signatures that distinguish model families, categorize task-specific prompts, and correlate with output accuracy. We further demonstrate the generality of our method by extending the analysis to vision transformers. Our results suggest that entropy-based metrics can serve as a principled tool for unveiling the inner workings of modern transformer architectures.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "165",
        "title": "Efficient 4D Gaussian Stream with Low Rank Adaptation",
        "author": [
            "Zhenhuan Liu",
            "Shuai Liu",
            "Yidong Lu",
            "Yirui Chen",
            "Jie Yang",
            "Wei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16575",
        "abstract": "Recent methods have made significant progress in synthesizing novel views with long video sequences. This paper proposes a highly scalable method for dynamic novel view synthesis with continual learning. We leverage the 3D Gaussians to represent the scene and a low-rank adaptation-based deformation model to capture the dynamic scene changes. Our method continuously reconstructs the dynamics with chunks of video frames, reduces the streaming bandwidth by $90\\%$ while maintaining high rendering quality comparable to the off-line SOTA methods.",
        "tags": [
            "3D",
            "Low-Rank Adaptation"
        ]
    },
    {
        "id": "166",
        "title": "Can Indirect Prompt Injection Attacks Be Detected and Removed?",
        "author": [
            "Yulin Chen",
            "Haoran Li",
            "Yuan Sui",
            "Yufei He",
            "Yue Liu",
            "Yangqiu Song",
            "Bryan Hooi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16580",
        "abstract": "Prompt injection attacks manipulate large language models (LLMs) by misleading them to deviate from the original input instructions and execute maliciously injected instructions, because of their instruction-following capabilities and inability to distinguish between the original input instructions and maliciously injected instructions. To defend against such attacks, recent studies have developed various detection mechanisms. While significant efforts have focused on detecting direct prompt injection attacks, where injected instructions are directly from the attacker who is also the user, limited attention has been given to indirect prompt injection attacks, where injected instructions are indirectly from external tools, such as a search engine. Moreover, current works mainly investigate injection detection methods and pay less attention to the post-processing method that aims to mitigate the injection after detection. In this paper, we investigate the feasibility of detecting and removing indirect prompt injection attacks, and we construct a benchmark dataset for evaluation. For detection, we assess the performance of existing LLMs and open-source detection models, and we further train detection models using our crafted training datasets. For removal, we evaluate two intuitive methods: (1) the segmentation removal method, which segments the injected document and removes parts containing injected instructions, and (2) the extraction removal method, which trains an extraction model to identify and remove injected instructions.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models",
            "Segmentation"
        ]
    },
    {
        "id": "167",
        "title": "Audio-FLAN: A Preliminary Release",
        "author": [
            "Liumeng Xue",
            "Ziya Zhou",
            "Jiahao Pan",
            "Zixuan Li",
            "Shuai Fan",
            "Yinghao Ma",
            "Sitong Cheng",
            "Dongchao Yang",
            "Haohan Guo",
            "Yujia Xiao",
            "Xinsheng Wang",
            "Zixuan Shen",
            "Chuanbo Zhu",
            "Xinshen Zhang",
            "Tianchi Liu",
            "Ruibin Yuan",
            "Zeyue Tian",
            "Haohe Liu",
            "Emmanouil Benetos",
            "Ge Zhang",
            "Yike Guo",
            "Wei Xue"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16584",
        "abstract": "Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While instruction tuning has demonstrated remarkable success in improving generalization and zero-shot learning across text and vision, its application to audio remains largely unexplored. A major obstacle is the lack of comprehensive datasets that unify audio understanding and generation. To address this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset covering 80 diverse tasks across speech, music, and sound domains, with over 100 million instances. Audio-FLAN lays the foundation for unified audio-language models that can seamlessly handle both understanding (e.g., transcription, comprehension) and generation (e.g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner. The Audio-FLAN dataset is available on HuggingFace and GitHub and will be continuously updated.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "168",
        "title": "Multimodal Large Language Models for Text-rich Image Understanding: A Comprehensive Review",
        "author": [
            "Pei Fu",
            "Tongkun Guan",
            "Zining Wang",
            "Zhentao Guo",
            "Chen Duan",
            "Hao Sun",
            "Boming Chen",
            "Jiayao Ma",
            "Qianyi Jiang",
            "Kai Zhou",
            "Junfeng Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16586",
        "abstract": "The recent emergence of Multi-modal Large Language Models (MLLMs) has introduced a new dimension to the Text-rich Image Understanding (TIU) field, with models demonstrating impressive and inspiring performance. However, their rapid evolution and widespread adoption have made it increasingly challenging to keep up with the latest advancements. To address this, we present a systematic and comprehensive survey to facilitate further research on TIU MLLMs. Initially, we outline the timeline, architecture, and pipeline of nearly all TIU MLLMs. Then, we review the performance of selected models on mainstream benchmarks. Finally, we explore promising directions, challenges, and limitations within the field.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "169",
        "title": "Human2Robot: Learning Robot Actions from Paired Human-Robot Videos",
        "author": [
            "Sicheng Xie",
            "Haidong Cao",
            "Zejia Weng",
            "Zhen Xing",
            "Shiwei Shen",
            "Jiaqi Leng",
            "Xipeng Qiu",
            "Yanwei Fu",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16587",
        "abstract": "Distilling knowledge from human demonstrations is a promising way for robots to learn and act. Existing work often overlooks the differences between humans and robots, producing unsatisfactory results. In this paper, we study how perfectly aligned human-robot pairs benefit robot learning. Capitalizing on VR-based teleportation, we introduce H\\&R, a third-person dataset with 2,600 episodes, each of which captures the fine-grained correspondence between human hands and robot gripper. Inspired by the recent success of diffusion models, we introduce Human2Robot, an end-to-end diffusion framework that formulates learning from human demonstrates as a generative task. Human2Robot fully explores temporal dynamics in human videos to generate robot videos and predict actions at the same time. Through comprehensive evaluations of 8 seen, changed and unseen tasks in real-world settings, we demonstrate that Human2Robot can not only generate high-quality robot videos but also excel in seen tasks and generalize to unseen objects, backgrounds and even new tasks effortlessly.",
        "tags": [
            "Diffusion",
            "Robot"
        ]
    },
    {
        "id": "170",
        "title": "Co-MTP: A Cooperative Trajectory Prediction Framework with Multi-Temporal Fusion for Autonomous Driving",
        "author": [
            "Xinyu Zhang",
            "Zewei Zhou",
            "Zhaoyi Wang",
            "Yangjie Ji",
            "Yanjun Huang",
            "Hong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16589",
        "abstract": "Vehicle-to-everything technologies (V2X) have become an ideal paradigm to extend the perception range and see through the occlusion. Exiting efforts focus on single-frame cooperative perception, however, how to capture the temporal cue between frames with V2X to facilitate the prediction task even the planning task is still underexplored. In this paper, we introduce the Co-MTP, a general cooperative trajectory prediction framework with multi-temporal fusion for autonomous driving, which leverages the V2X system to fully capture the interaction among agents in both history and future domains to benefit the planning. In the history domain, V2X can complement the incomplete history trajectory in single-vehicle perception, and we design a heterogeneous graph transformer to learn the fusion of the history feature from multiple agents and capture the history interaction. Moreover, the goal of prediction is to support future planning. Thus, in the future domain, V2X can provide the prediction results of surrounding objects, and we further extend the graph transformer to capture the future interaction among the ego planning and the other vehicles' intentions and obtain the final future scenario state under a certain planning action. We evaluate the Co-MTP framework on the real-world dataset V2X-Seq, and the results show that Co-MTP achieves state-of-the-art performance and that both history and future fusion can greatly benefit prediction.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "171",
        "title": "Revealing the Pragmatic Dilemma for Moral Reasoning Acquisition in Language Models",
        "author": [
            "Guangliang Liu",
            "Lei Jiang",
            "Xitong Zhang",
            "Kristen Marie Johnson"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16600",
        "abstract": "Ensuring that Large Language Models (LLMs) return just responses which adhere to societal values is crucial for their broader application. Prior research has shown that LLMs often fail to perform satisfactorily on tasks requiring moral cognizance, such as ethics-based judgments. While current approaches have focused on fine-tuning LLMs with curated datasets to improve their capabilities on such tasks, choosing the optimal learning paradigm to enhance the ethical responses of LLMs remains an open research debate. In this work, we aim to address this fundamental question: can current learning paradigms enable LLMs to acquire sufficient moral reasoning capabilities? Drawing from distributional semantics theory and the pragmatic nature of moral discourse, our analysis indicates that performance improvements follow a mechanism similar to that of semantic-level tasks, and therefore remain affected by the pragmatic nature of morals latent in discourse, a phenomenon we name the pragmatic dilemma. We conclude that this pragmatic dilemma imposes significant limitations on the generalization ability of current learning paradigms, making it the primary bottleneck for moral reasoning acquisition in LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "172",
        "title": "Reasoning about Affordances: Causal and Compositional Reasoning in LLMs",
        "author": [
            "Magnus F. Gjerde",
            "Vanessa Cheung",
            "David Lagnado"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16606",
        "abstract": "With the rapid progress of Large Language Models (LLMs), it becomes increasingly important to understand their abilities and limitations. In two experiments, we investigate the causal and compositional reasoning abilities of LLMs and humans in the domain of object affordances, an area traditionally linked to embodied cognition. The tasks, designed from scratch to avoid data contamination, require decision-makers to select unconventional objects to replace a typical tool for a particular purpose, such as using a table tennis racket to dig a hole. In Experiment 1, we evaluated GPT-3.5 and GPT-4o, finding that GPT-4o, when given chain-of-thought prompting, performed on par with human participants, while GPT-3.5 lagged significantly. In Experiment 2, we introduced two new conditions, Distractor (more object choices, increasing difficulty) and Image (object options presented visually), and evaluated Claude 3 Sonnet and Claude 3.5 Sonnet in addition to the GPT models. The Distractor condition significantly impaired performance across humans and models, although GPT-4o and Claude 3.5 still performed well above chance. Surprisingly, the Image condition had little impact on humans or GPT-4o, but significantly lowered Claude 3.5's accuracy. Qualitative analysis showed that GPT-4o and Claude 3.5 have a stronger ability than their predecessors to identify and flexibly apply causally relevant object properties. The improvement from GPT-3.5 and Claude 3 to GPT-4o and Claude 3.5 suggests that models are increasingly capable of causal and compositional reasoning in some domains, although further mechanistic research is necessary to understand how LLMs reason.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "173",
        "title": "CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models",
        "author": [
            "Alexander Zhang",
            "Marcus Dong",
            "Jiaheng Liu",
            "Wei Zhang",
            "Yejie Wang",
            "Jian Yang",
            "Ge Zhang",
            "Tianyu Liu",
            "Zhongyuan Peng",
            "Yingshui Tan",
            "Yuanxing Zhang",
            "Zhexu Wang",
            "Weixun Wang",
            "Yancheng He",
            "Ken Deng",
            "Wangchunshu Zhou",
            "Wenhao Huang",
            "Zhaoxiang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16614",
        "abstract": "The critique capacity of Large Language Models (LLMs) is essential for reasoning abilities, which can provide necessary suggestions (e.g., detailed analysis and constructive feedback). Therefore, how to evaluate the critique capacity of LLMs has drawn great attention and several critique benchmarks have been proposed. However, existing critique benchmarks usually have the following limitations: (1). Focusing on diverse reasoning tasks in general domains and insufficient evaluation on code tasks (e.g., only covering code generation task), where the difficulty of queries is relatively easy (e.g., the code queries of CriticBench are from Humaneval and MBPP). (2). Lacking comprehensive evaluation from different dimensions. To address these limitations, we introduce a holistic code critique benchmark for LLMs called CodeCriticBench. Specifically, our CodeCriticBench includes two mainstream code tasks (i.e., code generation and code QA) with different difficulties. Besides, the evaluation protocols include basic critique evaluation and advanced critique evaluation for different characteristics, where fine-grained evaluation checklists are well-designed for advanced settings. Finally, we conduct extensive experimental results of existing LLMs, which show the effectiveness of CodeCriticBench.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "174",
        "title": "Energy-Efficient Transformer Inference: Optimization Strategies for Time Series Classification",
        "author": [
            "Arshia Kermani",
            "Ehsan Zeraatkar",
            "Habib Irani"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16627",
        "abstract": "The increasing computational demands of transformer models in time series classification necessitate effective optimization strategies for energy-efficient deployment. This paper presents a systematic investigation of optimization techniques, focusing on structured pruning and quantization methods for transformer architectures. Through extensive experimentation on three distinct datasets (RefrigerationDevices, ElectricDevices, and PLAID), we quantitatively evaluate model performance and energy efficiency across different transformer configurations. Our experimental results demonstrate that static quantization reduces energy consumption by 29.14% while maintaining classification performance, and L1 pruning achieves a 1.63% improvement in inference speed with minimal accuracy degradation. These findings provide valuable insights into the effectiveness of optimization strategies for transformer-based time series classification, establishing a foundation for efficient model deployment in resource-constrained environments.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "175",
        "title": "Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation for Visual Knowledge Intensive Queries",
        "author": [
            "Yin Wu",
            "Quanyu Long",
            "Jing Li",
            "Jianfei Yu",
            "Wenya Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16636",
        "abstract": "Retrieval-Augmented Generation (RAG) is a popular approach for enhancing Large Language Models (LLMs) by addressing their limitations in verifying facts and answering knowledge-intensive questions. As the research in LLM extends their capability to handle input modality other than text, e.g. image, several multimodal RAG benchmarks are proposed. Nonetheless, they mainly use textual knowledge bases as the primary source of evidences for augmentation. There still lack benchmarks designed to evaluate images as augmentation in RAG systems and how they leverage visual knowledge. We propose Visual-RAG, a novel Question Answering benchmark that emphasizes visual knowledge intensive questions. Unlike prior works relying on text-based evidence, Visual-RAG necessitates text-to-image retrieval and integration of relevant clue images to extract visual knowledge as evidence. With Visual-RAG, we evaluate 5 open-sourced and 3 proprietary Multimodal LLMs (MLLMs), revealing that images can serve as good evidence in RAG; however, even the SoTA models struggle with effectively extracting and utilizing visual knowledge",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG",
            "Text-to-Image"
        ]
    },
    {
        "id": "176",
        "title": "Automatic Joint Structured Pruning and Quantization for Efficient Neural Network Training and Compression",
        "author": [
            "Xiaoyi Qu",
            "David Aponte",
            "Colby Banbury",
            "Daniel P. Robinson",
            "Tianyu Ding",
            "Kazuhito Koishida",
            "Ilya Zharkov",
            "Tianyi Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16638",
        "abstract": "Structured pruning and quantization are fundamental techniques used to reduce the size of deep neural networks (DNNs) and typically are applied independently. Applying these techniques jointly via co-optimization has the potential to produce smaller, high-quality models. However, existing joint schemes are not widely used because of (1) engineering difficulties (complicated multi-stage processes), (2) black-box optimization (extensive hyperparameter tuning to control the overall compression), and (3) insufficient architecture generalization. To address these limitations, we present the framework GETA, which automatically and efficiently performs joint structured pruning and quantization-aware training on any DNNs. GETA introduces three key innovations: (i) a quantization-aware dependency graph (QADG) that constructs a pruning search space for generic quantization-aware DNN, (ii) a partially projected stochastic gradient method that guarantees layerwise bit constraints are satisfied, and (iii) a new joint learning strategy that incorporates interpretable relationships between pruning and quantization. We present numerical experiments on both convolutional neural networks and transformer architectures that show that our approach achieves competitive (often superior) performance compared to existing joint pruning and quantization methods.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "177",
        "title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale",
        "author": [
            "Chenlong Wang",
            "Zhaoyang Chu",
            "Zhengxiang Cheng",
            "Xuyi Yang",
            "Kaiyue Qiu",
            "Yao Wan",
            "Zhou Zhao",
            "Xuanhua Shi",
            "Dongping Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16645",
        "abstract": "Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: https://github.com/Lucky-voyage/Code-Sync.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "178",
        "title": "Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language Embedding Registration",
        "author": [
            "Kim Jun-Seong",
            "GeonU Kim",
            "Kim Yu-Ji",
            "Yu-Chiang Frank Wang",
            "Jaesung Choe",
            "Tae-Hyun Oh"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16652",
        "abstract": "We introduce Dr. Splat, a novel approach for open-vocabulary 3D scene understanding leveraging 3D Gaussian Splatting. Unlike existing language-embedded 3DGS methods, which rely on a rendering process, our method directly associates language-aligned CLIP embeddings with 3D Gaussians for holistic 3D scene understanding. The key of our method is a language feature registration technique where CLIP embeddings are assigned to the dominant Gaussians intersected by each pixel-ray. Moreover, we integrate Product Quantization (PQ) trained on general large-scale image data to compactly represent embeddings without per-scene optimization. Experiments demonstrate that our approach significantly outperforms existing approaches in 3D perception benchmarks, such as open-vocabulary 3D semantic segmentation, 3D object localization, and 3D object selection tasks. For video results, please visit : https://drsplat.github.io/",
        "tags": [
            "3D",
            "CLIP",
            "Gaussian Splatting",
            "Segmentation"
        ]
    },
    {
        "id": "179",
        "title": "VPNeXt -- Rethinking Dense Decoding for Plain Vision Transformer",
        "author": [
            "Xikai Tang",
            "Ye Huang",
            "Guangqiang Yin",
            "Lixin Duan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16654",
        "abstract": "We present VPNeXt, a new and simple model for the Plain Vision Transformer (ViT). Unlike the many related studies that share the same homogeneous paradigms, VPNeXt offers a fresh perspective on dense representation based on ViT. In more detail, the proposed VPNeXt addressed two concerns about the existing paradigm: (1) Is it necessary to use a complex Transformer Mask Decoder architecture to obtain good representations? (2) Does the Plain ViT really need to depend on the mock pyramid feature for upsampling? For (1), we investigated the potential underlying reasons that contributed to the effectiveness of the Transformer Decoder and introduced the Visual Context Replay (VCR) to achieve similar effects efficiently. For (2), we introduced the ViTUp module. This module fully utilizes the previously overlooked ViT real pyramid feature to achieve better upsampling results compared to the earlier mock pyramid feature. This represents the first instance of such functionality in the field of semantic segmentation for Plain ViT. We performed ablation studies on related modules to verify their effectiveness gradually. We conducted relevant comparative experiments and visualizations to show that VPNeXt achieved state-of-the-art performance with a simple and effective design. Moreover, the proposed VPNeXt significantly exceeded the long-established mIoU wall/barrier of the VOC2012 dataset, setting a new state-of-the-art by a large margin, which also stands as the largest improvement since 2015.",
        "tags": [
            "Segmentation",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "180",
        "title": "BioMaze: Benchmarking and Enhancing Large Language Models for Biological Pathway Reasoning",
        "author": [
            "Haiteng Zhao",
            "Chang Ma",
            "FangZhi Xu",
            "Lingpeng Kong",
            "Zhi-Hong Deng"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16660",
        "abstract": "The applications of large language models (LLMs) in various biological domains have been explored recently, but their reasoning ability in complex biological systems, such as pathways, remains underexplored, which is crucial for predicting biological phenomena, formulating hypotheses, and designing experiments. This work explores the potential of LLMs in pathway reasoning. We introduce BioMaze, a dataset with 5.1K complex pathway problems derived from real research, covering various biological contexts including natural dynamic changes, disturbances, additional intervention conditions, and multi-scale research targets. Our evaluation of methods such as CoT and graph-augmented reasoning, shows that LLMs struggle with pathway reasoning, especially in perturbed systems. To address this, we propose PathSeeker, an LLM agent that enhances reasoning through interactive subgraph-based navigation, enabling a more effective approach to handling the complexities of biological systems in a scientifically aligned manner. The dataset and code are available at https://github.com/zhao-ht/BioMaze.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "181",
        "title": "SBSC: Step-By-Step Coding for Improving Mathematical Olympiad Performance",
        "author": [
            "Kunal Singh",
            "Ankan Biswas",
            "Sayandeep Bhowmick",
            "Pradeep Moturi",
            "Siva Kishore Gollapalli"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16666",
        "abstract": "We propose Step-by-Step Coding (SBSC): a multi-turn math reasoning framework that enables Large Language Models (LLMs) to generate sequence of programs for solving Olympiad level math problems. At each step/turn, by leveraging the code execution outputs and programs of previous steps, the model generates the next sub-task and the corresponding program to solve it. This way, SBSC, sequentially navigates to reach the final answer. SBSC allows more granular, flexible and precise approach to problem-solving compared to existing methods. Extensive experiments highlight the effectiveness of SBSC in tackling competition and Olympiad-level math problems. For Claude-3.5-Sonnet, we observe SBSC (greedy decoding) surpasses existing state-of-the-art (SOTA) program generation based reasoning strategies by absolute 10.7% on AMC12, 8% on AIME and 12.6% on MathOdyssey. Given SBSC is multi-turn in nature, we also benchmark SBSC's greedy decoding against self-consistency decoding results of existing SOTA math reasoning strategies and observe performance gain by absolute 6.2% on AMC, 6.7% on AIME and 7.4% on MathOdyssey.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "182",
        "title": "MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models",
        "author": [
            "Hengzhi Li",
            "Megan Tjandrasuwita",
            "Yi R. Fung",
            "Armando Solar-Lezama",
            "Paul Pu Liang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16671",
        "abstract": "Socially intelligent AI that can understand and interact seamlessly with humans in daily lives is increasingly important as AI becomes more closely integrated with peoples' daily activities. However, current works in artificial social reasoning all rely on language-only, or language-dominant approaches to benchmark and training models, resulting in systems that are improving in verbal communication but struggle with nonverbal social understanding. To address this limitation, we tap into a novel source of data rich in nonverbal and social interactions -- mime videos. Mimes refer to the art of expression through gesture and movement without spoken words, which presents unique challenges and opportunities in interpreting non-verbal social communication. We contribute a new dataset called MimeQA, obtained by sourcing 221 videos from YouTube, through rigorous annotation and verification, resulting in a benchmark with 101 videos and 806 question-answer pairs. Using MimeQA, we evaluate state-of-the-art video large language models (vLLMs) and find that their overall accuracy ranges from 15-30%. Our analysis reveals that vLLMs often fail to ground imagined objects and over-rely on the text prompt while ignoring subtle nonverbal interactions. Our data resources are released at https://github.com/MIT-MI/MimeQA to inspire future work in foundation models that embody true social intelligence capable of interpreting non-verbal human interactions.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "183",
        "title": "AeroReformer: Aerial Referring Transformer for UAV-based Referring Image Segmentation",
        "author": [
            "Rui Li"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16680",
        "abstract": "As a novel and challenging task, referring segmentation combines computer vision and natural language processing to localize and segment objects based on textual descriptions. While referring image segmentation (RIS) has been extensively studied in natural images, little attention has been given to aerial imagery, particularly from unmanned aerial vehicles (UAVs). The unique challenges of UAV imagery, including complex spatial scales, occlusions, and varying object orientations, render existing RIS approaches ineffective. A key limitation has been the lack of UAV-specific datasets, as manually annotating pixel-level masks and generating textual descriptions is labour-intensive and time-consuming. To address this gap, we design an automatic labelling pipeline that leverages pre-existing UAV segmentation datasets and Multimodal Large Language Models (MLLM) for generating textual descriptions. Furthermore, we propose Aerial Referring Transformer (AeroReformer), a novel framework for UAV referring image segmentation (UAV-RIS), featuring a Vision-Language Cross-Attention Module (VLCAM) for effective cross-modal understanding and a Rotation-Aware Multi-Scale Fusion (RAMSF) decoder to enhance segmentation accuracy in aerial scenes. Extensive experiments on two newly developed datasets demonstrate the superiority of AeroReformer over existing methods, establishing a new benchmark for UAV-RIS. The datasets and code will be publicly available at: https://github.com/lironui/AeroReformer.",
        "tags": [
            "Large Language Models",
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "184",
        "title": "Automatic Input Rewriting Improves Translation with Large Language Models",
        "author": [
            "Dayeon Ki",
            "Marine Carpuat"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16682",
        "abstract": "Can we improve machine translation (MT) with LLMs by rewriting their inputs automatically? Users commonly rely on the intuition that well-written text is easier to translate when using off-the-shelf MT systems. LLMs can rewrite text in many ways but in the context of MT, these capabilities have been primarily exploited to rewrite outputs via post-editing. We present an empirical study of 21 input rewriting methods with 3 open-weight LLMs for translating from English into 6 target languages. We show that text simplification is the most effective MT-agnostic rewrite strategy and that it can be improved further when using quality estimation to assess translatability. Human evaluation further confirms that simplified rewrites and their MT outputs both largely preserve the original meaning of the source and MT. These results suggest LLM-assisted input rewriting as a promising direction for improving translations.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "185",
        "title": "WildLong: Synthesizing Realistic Long-Context Instruction Data at Scale",
        "author": [
            "Jiaxi Li",
            "Xingxing Zhang",
            "Xun Wang",
            "Xiaolong Huang",
            "Li Dong",
            "Liang Wang",
            "Si-Qing Chen",
            "Wei Lu",
            "Furu Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16684",
        "abstract": "Large language models (LLMs) with extended context windows enable tasks requiring extensive information integration but are limited by the scarcity of high-quality, diverse datasets for long-context instruction tuning. Existing data synthesis methods focus narrowly on objectives like fact retrieval and summarization, restricting their generalizability to complex, real-world tasks. WildLong extracts meta-information from real user queries, models co-occurrence relationships via graph-based methods, and employs adaptive generation to produce scalable data. It extends beyond single-document tasks to support multi-document reasoning, such as cross-document comparison and aggregation. Our models, finetuned on 150K instruction-response pairs synthesized using WildLong, surpasses existing open-source long-context-optimized models across benchmarks while maintaining strong performance on short-context tasks without incorporating supplementary short-context data. By generating a more diverse and realistic long-context instruction dataset, WildLong enhances LLMs' ability to generalize to complex, real-world reasoning over long contexts, establishing a new paradigm for long-context data synthesis.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "186",
        "title": "From Text to Space: Mapping Abstract Spatial Models in LLMs during a Grid-World Navigation Task",
        "author": [
            "Nicolas Martorell"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16690",
        "abstract": "Understanding how large language models (LLMs) represent and reason about spatial information is crucial for building robust agentic systems that can navigate real and simulated environments. In this work, we investigate the influence of different text-based spatial representations on LLM performance and internal activations in a grid-world navigation task. By evaluating models of various sizes on a task that requires navigating toward a goal, we examine how the format used to encode spatial information impacts decision-making. Our experiments reveal that cartesian representations of space consistently yield higher success rates and path efficiency, with performance scaling markedly with model size. Moreover, probing LLaMA-3.1-8B revealed subsets of internal units, primarily located in intermediate layers, that robustly correlate with spatial features, such as the position of the agent in the grid or action correctness, regardless of how that information is represented, and are also activated by unrelated spatial reasoning tasks. This work advances our understanding of how LLMs process spatial information and provides valuable insights for developing more interpretable and robust agentic AI systems.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "187",
        "title": "Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI",
        "author": [
            "Eunchung Noh",
            "Jeonghun Baek"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16691",
        "abstract": "Recent research has increasingly focused on training large language models (LLMs) using federated learning, known as FedLLM. However, responsible AI (RAI), which aims to ensure safe responses, remains underexplored in the context of FedLLM. In FedLLM, client data used for training may contain harmful content, leading to unsafe LLMs that generate harmful responses. Aggregating such unsafe LLMs into the global model and distributing them to clients may result in the widespread deployment of unsafe LLMs. To address this issue, we incorporate two well-known RAI methods into FedLLM: the safety filter and constitutional AI. Our experiments demonstrate that these methods significantly enhance the safety of the LLM, achieving over a 20% improvement on AdvBench, a benchmark for evaluating safety performance.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "188",
        "title": "Dynamic LLM Routing and Selection based on User Preferences: Balancing Performance, Cost, and Ethics",
        "author": [
            "Deepak Babu Piskala",
            "Vijay Raajaa",
            "Sachin Mishra",
            "Bruno Bozza"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16696",
        "abstract": "With the widespread deployment of large language models (LLMs) such as GPT4, BART, and LLaMA, the need for a system that can intelligently select the most suitable model for specific tasks while balancing cost, latency, accuracy, and ethical considerations has become increasingly important. Recognizing that not all tasks necessitate models with over 100 billion parameters, we introduce OptiRoute, an advanced model routing engine designed to dynamically select and route tasks to the optimal LLM based on detailed user-defined requirements. OptiRoute captures both functional (e.g., accuracy, speed, cost) and non-functional (e.g., helpfulness, harmlessness, honesty) criteria, leveraging lightweight task analysis and complexity estimation to efficiently match tasks with the best-fit models from a diverse array of LLMs. By employing a hybrid approach combining k-nearest neighbors (kNN) search and hierarchical filtering, OptiRoute optimizes for user priorities while minimizing computational overhead. This makes it ideal for real-time applications in cloud-based ML platforms, personalized AI services, and regulated industries.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "189",
        "title": "Code Summarization Beyond Function Level",
        "author": [
            "Vladimir Makharev",
            "Vladimir Ivanov"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16704",
        "abstract": "Code summarization is a critical task in natural language processing and software engineering, which aims to generate concise descriptions of source code. Recent advancements have improved the quality of these summaries, enhancing code readability and maintainability. However, the content of a repository or a class has not been considered in function code summarization. This study investigated the effectiveness of code summarization models beyond the function level, exploring the impact of class and repository contexts on the summary quality. The study involved revising benchmarks for evaluating models at class and repository levels, assessing baseline models, and evaluating LLMs with in-context learning to determine the enhancement of summary quality with additional context. The findings revealed that the fine-tuned state-of-the-art CodeT5+ base model excelled in code summarization, while incorporating few-shot learning and retrieved code chunks from RAG significantly enhanced the performance of LLMs in this task. Notably, the Deepseek Coder 1.3B and Starcoder2 15B models demonstrated substantial improvements in metrics such as BLEURT, METEOR, and BLEU-4 at both class and repository levels. Repository-level summarization exhibited promising potential but necessitates significant computational resources and gains from the inclusion of structured context. Lastly, we employed the recent SIDE code summarization metric in our evaluation. This study contributes to refining strategies for prompt engineering, few-shot learning, and RAG, addressing gaps in benchmarks for code summarization at various levels. Finally, we publish all study details, code, datasets, and results of evaluation in the GitHub repository available at https://github.com/kilimanj4r0/code-summarization-beyond-function-level.",
        "tags": [
            "DeepSeek",
            "LLMs",
            "RAG"
        ]
    },
    {
        "id": "190",
        "title": "Can ChatGPT Learn to Count Letters?",
        "author": [
            "Javier Conde",
            "Gonzalo MartÃ­nez",
            "Pedro Reviriego",
            "Zhen Gao",
            "Shanshan Liu",
            "Fabrizio Lombardi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16705",
        "abstract": "Large language models (LLMs) struggle on simple tasks such as counting the number of occurrences of a letter in a word. In this paper, we investigate if ChatGPT can learn to count letters and propose an efficient solution.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "191",
        "title": "Beyond Pattern Recognition: Probing Mental Representations of LMs",
        "author": [
            "Moritz Miller",
            "Kumar Shridhar"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16717",
        "abstract": "Language Models (LMs) have demonstrated impressive capabilities in solving complex reasoning tasks, particularly when prompted to generate intermediate explanations. However, it remains an open question whether these intermediate reasoning traces represent a dynamic, evolving thought process or merely reflect sophisticated pattern recognition acquired during large scale pre training. Drawing inspiration from human cognition, where reasoning unfolds incrementally as new information is assimilated and internal models are continuously updated, we propose to delve deeper into the mental model of various LMs. We propose a new way to assess the mental modeling of LMs, where they are provided with problem details gradually, allowing each new piece of data to build upon and refine the model's internal representation of the task. We systematically compare this step by step mental modeling strategy with traditional full prompt methods across both text only and vision and text modalities. Experiments on the MathWorld dataset across different model sizes and problem complexities confirm that both text-based LLMs and multimodal LMs struggle to create mental representations, questioning how their internal cognitive processes work.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "192",
        "title": "Speed and Conversational Large Language Models: Not All Is About Tokens per Second",
        "author": [
            "Javier Conde",
            "Miguel GonzÃ¡lez",
            "Pedro Reviriego",
            "Zhen Gao",
            "Shanshan Liu",
            "Fabrizio Lombardi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16721",
        "abstract": "The speed of open-weights large language models (LLMs) and its dependency on the task at hand, when run on GPUs, is studied to present a comparative analysis of the speed of the most popular open LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "193",
        "title": "Layer-Wise Evolution of Representations in Fine-Tuned Transformers: Insights from Sparse AutoEncoders",
        "author": [
            "Suneel Nadipalli"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16722",
        "abstract": "Fine-tuning pre-trained transformers is a powerful technique for enhancing the performance of base models on specific tasks. From early applications in models like BERT to fine-tuning Large Language Models (LLMs), this approach has been instrumental in adapting general-purpose architectures for specialized downstream tasks. Understanding the fine-tuning process is crucial for uncovering how transformers adapt to specific objectives, retain general representations, and acquire task-specific features. This paper explores the underlying mechanisms of fine-tuning, specifically in the BERT transformer, by analyzing activation similarity, training Sparse AutoEncoders (SAEs), and visualizing token-level activations across different layers. Based on experiments conducted across multiple datasets and BERT layers, we observe a steady progression in how features adapt to the task at hand: early layers primarily retain general representations, middle layers act as a transition between general and task-specific features, and later layers fully specialize in task adaptation. These findings provide key insights into the inner workings of fine-tuning and its impact on representation learning within transformer architectures.",
        "tags": [
            "BERT",
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "194",
        "title": "DOSE3 : Diffusion-based Out-of-distribution detection on SE(3) trajectories",
        "author": [
            "Hongzhe Cheng",
            "Tianyou Zheng",
            "Tianyi Zhang",
            "Matthew Johnson-Roberson",
            "Weiming Zhi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16725",
        "abstract": "Out-of-Distribution(OOD) detection, a fundamental machine learning task aimed at identifying abnormal samples, traditionally requires model retraining for different inlier distributions. While recent research demonstrates the applicability of diffusion models to OOD detection, existing approaches are limited to Euclidean or latent image spaces. Our work extends OOD detection to trajectories in the Special Euclidean Group in 3D ($\\mathbb{SE}(3)$), addressing a critical need in computer vision, robotics, and engineering applications that process object pose sequences in $\\mathbb{SE}(3)$. We present $\\textbf{D}$iffusion-based $\\textbf{O}$ut-of-distribution detection on $\\mathbb{SE}(3)$ ($\\mathbf{DOSE3}$), a novel OOD framework that extends diffusion to a unified sample space of $\\mathbb{SE}(3)$ pose sequences. Through extensive validation on multiple benchmark datasets, we demonstrate $\\mathbf{DOSE3}$'s superior performance compared to state-of-the-art OOD detection frameworks.",
        "tags": [
            "3D",
            "Detection",
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "195",
        "title": "RapidPen: Fully Automated IP-to-Shell Penetration Testing with LLM-based Agents",
        "author": [
            "Sho Nakatani"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16730",
        "abstract": "We present RapidPen, a fully automated penetration testing (pentesting) framework that addresses\nthe challenge of achieving an initial foothold (IP-to-Shell) without human intervention. Unlike prior\napproaches that focus primarily on post-exploitation or require a human-in-the-loop, RapidPen\nleverages large language models (LLMs) to autonomously discover and exploit vulnerabilities, starting from\na single IP address. By integrating advanced ReAct-style task planning (Re) with retrieval-augmented\nknowledge bases of successful exploits, along with a command-generation and direct execution feedback loop\n(Act), RapidPen systematically scans services, identifies viable attack vectors, and executes targeted\nexploits in a fully automated manner.\nIn our evaluation against a vulnerable target from the Hack The Box platform, RapidPen achieved shell\naccess within 200-400 seconds at a per-run cost of approximately \\$0.3-\\$0.6, demonstrating a\n60\\% success rate when reusing prior \"success-case\" data. These results underscore the potential\nof truly autonomous pentesting for both security novices and seasoned professionals. Organizations\nwithout dedicated security teams can leverage RapidPen to quickly identify critical vulnerabilities,\nwhile expert pentesters can offload repetitive tasks and focus on complex challenges.\nUltimately, our work aims to make penetration testing more accessible and cost-efficient,\nthereby enhancing the overall security posture of modern software ecosystems.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "196",
        "title": "Model-agnostic Coreset Selection via LLM-based Concept Bottlenecks",
        "author": [
            "Akshay Mehra",
            "Trisha Mittal",
            "Subhadra Gopalakrishnan",
            "Joshua Kimball"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16733",
        "abstract": "Coreset Selection (CS) identifies a subset of training data that achieves model performance comparable to using the entire dataset. Many state-of-the-art CS methods, select coresets using scores whose computation requires training the downstream model on the entire dataset and recording changes in its behavior on samples as it trains (training dynamics). These scores are inefficient to compute and hard to interpret as they do not indicate whether a sample is difficult to learn in general or only for a specific model. Our work addresses these challenges by proposing an interpretable score that gauges a sample's difficulty using human-understandable textual attributes (concepts) independent of any downstream model. Specifically, we measure the alignment between a sample's visual features and concept bottlenecks, derived via large language models, by training a linear concept bottleneck layer and compute the sample's difficulty score using it. We then use this score and a stratified sampling strategy to identify the coreset. Crucially, our score is efficiently computable without training the downstream model on the full dataset even once, leads to high-performing coresets for various downstream models, and is computable even for an unlabeled dataset. Through experiments on CIFAR-10, CIFAR-100, and ImageNet-1K, we show our coresets outperform random subsets, even at high pruning rates, and achieve model performance comparable to or better than coresets found by training dynamics-based methods.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "197",
        "title": "SQLong: Enhanced NL2SQL for Longer Contexts with LLMs",
        "author": [
            "Dai Quoc Nguyen",
            "Cong Duy Vu Hoang",
            "Duy Vu",
            "Gioacchino Tangari",
            "Thanh Tien Vu",
            "Don Dharmasiri",
            "Yuan-Fang Li",
            "Long Duong"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16747",
        "abstract": "Open-weight large language models (LLMs) have significantly advanced performance in the Natural Language to SQL (NL2SQL) task. However, their effectiveness diminishes when dealing with large database schemas, as the context length increases. To address this limitation, we present SQLong, a novel and efficient data augmentation framework designed to enhance LLM performance in long-context scenarios for the NL2SQL task. SQLong generates augmented datasets by extending existing database schemas with additional synthetic CREATE TABLE commands and corresponding data rows, sampled from diverse schemas in the training data. This approach effectively simulates long-context scenarios during finetuning and evaluation. Through experiments on the Spider and BIRD datasets, we demonstrate that LLMs finetuned with SQLong-augmented data significantly outperform those trained on standard datasets. These imply SQLong's practical implementation and its impact on improving NL2SQL capabilities in real-world settings with complex database schemas.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "198",
        "title": "Guardians of the Agentic System: Preventing Many Shots Jailbreak with Agentic System",
        "author": [
            "Saikat Barua",
            "Mostafizur Rahman",
            "Md Jafor Sadek",
            "Rafiul Islam",
            "Shehnaz Khaled",
            "Ahmedul Kabir"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16750",
        "abstract": "The autonomous AI agents using large language models can create undeniable values in all span of the society but they face security threats from adversaries that warrants immediate protective solutions because trust and safety issues arise. Considering the many-shot jailbreaking and deceptive alignment as some of the main advanced attacks, that cannot be mitigated by the static guardrails used during the supervised training, points out a crucial research priority for real world robustness. The combination of static guardrails in dynamic multi-agent system fails to defend against those attacks. We intend to enhance security for LLM-based agents through the development of new evaluation frameworks which identify and counter threats for safe operational deployment. Our work uses three examination methods to detect rogue agents through a Reverse Turing Test and analyze deceptive alignment through multi-agent simulations and develops an anti-jailbreaking system by testing it with GEMINI 1.5 pro and llama-3.3-70B, deepseek r1 models using tool-mediated adversarial scenarios. The detection capabilities are strong such as 94\\% accuracy for GEMINI 1.5 pro yet the system suffers persistent vulnerabilities when under long attacks as prompt length increases attack success rates (ASR) and diversity metrics become ineffective in prediction while revealing multiple complex system faults. The findings demonstrate the necessity of adopting flexible security systems based on active monitoring that can be performed by the agents themselves together with adaptable interventions by system admin as the current models can create vulnerabilities that can lead to the unreliable and vulnerable system. So, in our work, we try to address such situations and propose a comprehensive framework to counteract the security issues.",
        "tags": [
            "DeepSeek",
            "Detection",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "199",
        "title": "The Blessing of Reasoning: LLM-Based Contrastive Explanations in Black-Box Recommender Systems",
        "author": [
            "Yuyan Wang",
            "Pan Li",
            "Minmin Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16759",
        "abstract": "Modern recommender systems use ML models to predict consumer preferences from consumption history. Although these \"black-box\" models achieve impressive predictive performance, they often suffer from a lack of transparency and explainability. Contrary to the presumed tradeoff between explainability and accuracy, we show that integrating large language models (LLMs) with deep neural networks (DNNs) can improve both. We propose LR-Recsys, which augments DNN-based systems with LLM reasoning capabilities. LR-Recsys introduces a contrastive-explanation generator that produces human-readable positive explanations and negative explanations. These explanations are embedded via a fine-tuned autoencoder and combined with consumer and product features to improve predictions. Beyond offering explainability, we show that LR-Recsys also improves learning efficiency and predictive accuracy, as supported by high-dimensional, multi-environment statistical learning theory.\nLR-Recsys outperforms state-of-the-art recommender systems by 3-14% on three real-world datasets. Importantly, our analysis reveals that these gains primarily derive from LLMs' reasoning capabilities rather than their external domain knowledge. LR-RecSys presents an effective approach to combine LLMs with traditional DNNs, two of the most widely used ML models today. The explanations generated by LR-Recsys provide actionable insights for consumers, sellers, and platforms, helping to build trust, optimize product offerings, and inform targeting strategies.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "200",
        "title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
        "author": [
            "Joseph Suh",
            "Erfan Jahanparast",
            "Suhong Moon",
            "Minwoo Kang",
            "Serina Chang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16761",
        "abstract": "Large language models (LLMs) present novel opportunities in public opinion research by predicting survey responses in advance during the early stages of survey design. Prior methods steer LLMs via descriptions of subpopulations as LLMs' input prompt, yet such prompt engineering approaches have struggled to faithfully predict the distribution of survey responses from human subjects. In this work, we propose directly fine-tuning LLMs to predict response distributions by leveraging unique structural characteristics of survey data. To enable fine-tuning, we curate SubPOP, a significantly scaled dataset of 3,362 questions and 70K subpopulation-response pairs from well-established public opinion surveys. We show that fine-tuning on SubPOP greatly improves the match between LLM predictions and human responses across various subpopulations, reducing the LLM-human gap by up to 46% compared to baselines, and achieves strong generalization to unseen surveys and subpopulations. Our findings highlight the potential of survey-based fine-tuning to improve opinion prediction for diverse, real-world subpopulations and therefore enable more efficient survey designs. Our code is available at https://github.com/JosephJeesungSuh/subpop.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "201",
        "title": "A Transformer-in-Transformer Network Utilizing Knowledge Distillation for Image Recognition",
        "author": [
            "Dewan Tauhid Rahman",
            "Yeahia Sarker",
            "Antar Mazumder",
            "Md. Shamim Anower"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16762",
        "abstract": "This paper presents a novel knowledge distillation neural architecture leveraging efficient transformer networks for effective image classification. Natural images display intricate arrangements encompassing numerous extraneous elements. Vision transformers utilize localized patches to compute attention. However, exclusive dependence on patch segmentation proves inadequate in sufficiently encompassing the comprehensive nature of the image. To address this issue, we have proposed an inner-outer transformer-based architecture, which gives attention to the global and local aspects of the image. Moreover, The training of transformer models poses significant challenges due to their demanding resource, time, and data requirements. To tackle this, we integrate knowledge distillation into the architecture, enabling efficient learning. Leveraging insights from a larger teacher model, our approach enhances learning efficiency and effectiveness. Significantly, the transformer-in-transformer network acquires lightweight characteristics by means of distillation conducted within the feature extraction layer. Our featured network's robustness is established through substantial experimentation on the MNIST, CIFAR10, and CIFAR100 datasets, demonstrating commendable top-1 and top-5 accuracy. The conducted ablative analysis comprehensively validates the effectiveness of the chosen parameters and settings, showcasing their superiority against contemporary methodologies. Remarkably, the proposed Transformer-in-Transformer Network (TITN) model achieves impressive performance milestones across various datasets: securing the highest top-1 accuracy of 74.71% and a top-5 accuracy of 92.28% for the CIFAR100 dataset, attaining an unparalleled top-1 accuracy of 92.03% and top-5 accuracy of 99.80% for the CIFAR-10 dataset, and registering an exceptional top-1 accuracy of 99.56% for the MNIST dataset.",
        "tags": [
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "202",
        "title": "Exact Learning of Permutations for Nonzero Binary Inputs with Logarithmic Training Size and Quadratic Ensemble Complexity",
        "author": [
            "George Giapitzakis",
            "Artur Back de Luca",
            "Kimon Fountoulakis"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16763",
        "abstract": "The ability of an architecture to realize permutations is quite fundamental. For example, Large Language Models need to be able to correctly copy (and perhaps rearrange) parts of the input prompt into the output. Classical universal approximation theorems guarantee the existence of parameter configurations that solve this task but offer no insights into whether gradient-based algorithms can find them. In this paper, we address this gap by focusing on two-layer fully connected feed-forward neural networks and the task of learning permutations on nonzero binary inputs. We show that in the infinite width Neural Tangent Kernel (NTK) regime, an ensemble of such networks independently trained with gradient descent on only the $k$ standard basis vectors out of $2^k - 1$ possible inputs successfully learns any fixed permutation of length $k$ with arbitrarily high probability. By analyzing the exact training dynamics, we prove that the network's output converges to a Gaussian process whose mean captures the ground truth permutation via sign-based features. We then demonstrate how averaging these runs (an \"ensemble\" method) and applying a simple rounding step yields an arbitrarily accurate prediction on any possible input unseen during training. Notably, the number of models needed to achieve exact learning with high probability (which we refer to as ensemble complexity) exhibits a linearithmic dependence on the input size $k$ for a single test input and a quadratic dependence when considering all test inputs simultaneously.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "203",
        "title": "A Hybrid Approach to Information Retrieval and Answer Generation for Regulatory Texts",
        "author": [
            "Jhon Rayo",
            "Raul de la Rosa",
            "Mario Garrido"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16767",
        "abstract": "Regulatory texts are inherently long and complex, presenting significant challenges for information retrieval systems in supporting regulatory officers with compliance tasks. This paper introduces a hybrid information retrieval system that combines lexical and semantic search techniques to extract relevant information from large regulatory corpora. The system integrates a fine-tuned sentence transformer model with the traditional BM25 algorithm to achieve both semantic precision and lexical coverage. To generate accurate and comprehensive responses, retrieved passages are synthesized using Large Language Models (LLMs) within a Retrieval Augmented Generation (RAG) framework. Experimental results demonstrate that the hybrid system significantly outperforms standalone lexical and semantic approaches, with notable improvements in Recall@10 and MAP@10. By openly sharing our fine-tuned model and methodology, we aim to advance the development of robust natural language processing tools for compliance-driven applications in regulatory domains.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG",
            "Transformer"
        ]
    },
    {
        "id": "204",
        "title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint",
        "author": [
            "Qianli Ma",
            "Dongrui Liu",
            "Qian Chen",
            "Linfeng Zhang",
            "Jing Shao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16770",
        "abstract": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: \\textbf{neuron misidentification} due to simplistic parameter magnitude-based selection, and \\textbf{cross-task neuron interference} during merging. To address these challenges, we propose \\textbf{LED-Merging}, a three-stage framework that \\textbf{L}ocates task-specific neurons via gradient-based attribution, dynamically \\textbf{E}lects critical neurons through multi-model importance fusion, and \\textbf{D}isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging reduces harmful response rates(\\emph{e.g.}, a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench) while preserving 95\\% of utility performance(\\emph{e.g.}, 52.39\\% accuracy on GSM8K). LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "205",
        "title": "AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement",
        "author": [
            "Zhexin Zhang",
            "Leqi Lei",
            "Junxiao Yang",
            "Xijie Huang",
            "Yida Lu",
            "Shiyao Cui",
            "Renmiao Chen",
            "Qinglin Zhang",
            "Xinyuan Wang",
            "Hao Wang",
            "Hao Li",
            "Xianqi Lei",
            "Chengwei Pan",
            "Lei Sha",
            "Hongning Wang",
            "Minlie Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16776",
        "abstract": "As AI models are increasingly deployed across diverse real-world scenarios, ensuring their safety remains a critical yet underexplored challenge. While substantial efforts have been made to evaluate and enhance AI safety, the lack of a standardized framework and comprehensive toolkit poses significant obstacles to systematic research and practical adoption. To bridge this gap, we introduce AISafetyLab, a unified framework and toolkit that integrates representative attack, defense, and evaluation methodologies for AI safety. AISafetyLab features an intuitive interface that enables developers to seamlessly apply various techniques while maintaining a well-structured and extensible codebase for future advancements. Additionally, we conduct empirical studies on Vicuna, analyzing different attack and defense strategies to provide valuable insights into their comparative effectiveness. To facilitate ongoing research and development in AI safety, AISafetyLab is publicly available at https://github.com/thu-coai/AISafetyLab, and we are committed to its continuous maintenance and improvement.",
        "tags": [
            "Vicuna"
        ]
    },
    {
        "id": "206",
        "title": "MultiOCR-QA: Dataset for Evaluating Robustness of LLMs in Question Answering on Multilingual OCR Texts",
        "author": [
            "Bhawna Piryani",
            "Jamshid Mozafari",
            "Abdelrahman Abdallah",
            "Antoine Doucet",
            "Adam Jatowt"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16781",
        "abstract": "Optical Character Recognition (OCR) plays a crucial role in digitizing historical and multilingual documents, yet OCR errors -- imperfect extraction of the text, including character insertion, deletion and permutation -- can significantly impact downstream tasks like question-answering (QA). In this work, we introduce a multilingual QA dataset MultiOCR-QA, designed to analyze the effects of OCR noise on QA systems' performance. The MultiOCR-QA dataset comprises 60K question-answer pairs covering three languages, English, French, and German. The dataset is curated from OCR-ed old documents, allowing for the evaluation of OCR-induced challenges on question answering. We evaluate MultiOCR-QA on various levels and types of OCR errors to access the robustness of LLMs in handling real-world digitization errors. Our findings show that QA systems are highly prone to OCR induced errors and exhibit performance degradation on noisy OCR text.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "207",
        "title": "CipherPrune: Efficient and Scalable Private Transformer Inference",
        "author": [
            "Yancheng Zhang",
            "Jiaqi Xue",
            "Mengxin Zheng",
            "Mimi Xie",
            "Mingzhe Zhang",
            "Lei Jiang",
            "Qian Lou"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16782",
        "abstract": "Private Transformer inference using cryptographic protocols offers promising solutions for privacy-preserving machine learning; however, it still faces significant runtime overhead (efficiency issues) and challenges in handling long-token inputs (scalability issues). We observe that the Transformer's operational complexity scales quadratically with the number of input tokens, making it essential to reduce the input token length. Notably, each token varies in importance, and many inputs contain redundant tokens. Additionally, prior private inference methods that rely on high-degree polynomial approximations for non-linear activations are computationally expensive. Therefore, reducing the polynomial degree for less important tokens can significantly accelerate private inference. Building on these observations, we propose \\textit{CipherPrune}, an efficient and scalable private inference framework that includes a secure encrypted token pruning protocol, a polynomial reduction protocol, and corresponding Transformer network optimizations. At the protocol level, encrypted token pruning adaptively removes unimportant tokens from encrypted inputs in a progressive, layer-wise manner. Additionally, encrypted polynomial reduction assigns lower-degree polynomials to less important tokens after pruning, enhancing efficiency without decryption. At the network level, we introduce protocol-aware network optimization via a gradient-based search to maximize pruning thresholds and polynomial reduction conditions while maintaining the desired accuracy. Our experiments demonstrate that CipherPrune reduces the execution overhead of private Transformer inference by approximately $6.1\\times$ for 128-token inputs and $10.6\\times$ for 512-token inputs, compared to previous methods, with only a marginal drop in accuracy. The code is publicly available at https://github.com/UCF-Lou-Lab-PET/cipher-prune-inference.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "208",
        "title": "SwimVG: Step-wise Multimodal Fusion and Adaption for Visual Grounding",
        "author": [
            "Liangtao Shi",
            "Ting Liu",
            "Xiantao Hu",
            "Yue Hu",
            "Quanjun Yin",
            "Richang Hong"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16786",
        "abstract": "Visual grounding aims to ground an image region through natural language, which heavily relies on cross-modal alignment. Most existing methods transfer visual/linguistic knowledge separately by fully fine-tuning uni-modal pre-trained models, followed by a simple stack of visual-language transformers for multimodal fusion. However, these approaches not only limit adequate interaction between visual and linguistic contexts, but also incur significant computational costs. Therefore, to address these issues, we explore a step-wise multimodal fusion and adaption framework, namely SwimVG. Specifically, SwimVG proposes step-wise multimodal prompts (Swip) and cross-modal interactive adapters (CIA) for visual grounding, replacing the cumbersome transformer stacks for multimodal fusion. Swip can improve {the} alignment between the vision and language representations step by step, in a token-level fusion manner. In addition, weight-level CIA further promotes multimodal fusion by cross-modal interaction. Swip and CIA are both parameter-efficient paradigms, and they fuse the cross-modal features from shallow to deep layers gradually. Experimental results on four widely-used benchmarks demonstrate that SwimVG achieves remarkable abilities and considerable benefits in terms of efficiency. Our code is available at https://github.com/liuting20/SwimVG.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "209",
        "title": "AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay",
        "author": [
            "Ziyi Tang",
            "Zechuan Chen",
            "Jiarui Yang",
            "Jiayao Mai",
            "Yongsen Zheng",
            "Keze Wang",
            "Jinrui Chen",
            "Liang Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16789",
        "abstract": "Alpha mining, a critical component in quantitative investment, focuses on discovering predictive signals for future asset returns in increasingly complex financial markets. However, the pervasive issue of alpha decay, where factors lose their predictive power over time, poses a significant challenge for alpha mining. Traditional methods like genetic programming face rapid alpha decay from overfitting and complexity, while approaches driven by Large Language Models (LLMs), despite their promise, often rely too heavily on existing knowledge, creating homogeneous factors that worsen crowding and accelerate decay. To address this challenge, we propose AlphaAgent, an autonomous framework that effectively integrates LLM agents with ad hoc regularizations for mining decay-resistant alpha factors. AlphaAgent employs three key mechanisms: (i) originality enforcement through a similarity measure based on abstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor alignment via LLM-evaluated semantic consistency between market hypotheses and generated factors, and (iii) complexity control via AST-based structural constraints, preventing over-engineered constructions that are prone to overfitting. These mechanisms collectively guide the alpha generation process to balance originality, financial rationale, and adaptability to evolving market conditions, mitigating the risk of alpha decay. Extensive evaluations show that AlphaAgent outperforms traditional and LLM-based methods in mitigating alpha decay across bull and bear markets, consistently delivering significant alpha in Chinese CSI 500 and US S&P 500 markets over the past four years. Notably, AlphaAgent showcases remarkable resistance to alpha decay, elevating the potential for yielding powerful factors.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "210",
        "title": "Are Large Language Models Good Data Preprocessors?",
        "author": [
            "Elyas Meguellati",
            "Nardiena Pratama",
            "Shazia Sadiq",
            "Gianluca Demartini"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16790",
        "abstract": "High-quality textual training data is essential for the success of multimodal data processing tasks, yet outputs from image captioning models like BLIP and GIT often contain errors and anomalies that are difficult to rectify using rule-based methods. While recent work addressing this issue has predominantly focused on using GPT models for data preprocessing on relatively simple public datasets, there is a need to explore a broader range of Large Language Models (LLMs) and tackle more challenging and diverse datasets.\nIn this study, we investigate the use of multiple LLMs, including LLaMA 3.1 70B, GPT-4 Turbo, and Sonnet 3.5 v2, to refine and clean the textual outputs of BLIP and GIT. We assess the impact of LLM-assisted data cleaning by comparing downstream-task (SemEval 2024 Subtask \"Multilabel Persuasion Detection in Memes\") models trained on cleaned versus non-cleaned data. While our experimental results show improvements when using LLM-cleaned captions, statistical tests reveal that most of these improvements are not significant. This suggests that while LLMs have the potential to enhance data cleaning and repairing, their effectiveness may be limited depending on the context they are applied to, the complexity of the task, and the level of noise in the text.\nOur findings highlight the need for further research into the capabilities and limitations of LLMs in data preprocessing pipelines, especially when dealing with challenging datasets, contributing empirical evidence to the ongoing discussion about integrating LLMs into data preprocessing pipelines.",
        "tags": [
            "Detection",
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "211",
        "title": "The Role of Sparsity for Length Generalization in Transformers",
        "author": [
            "Noah Golowich",
            "Samy Jelassi",
            "David Brandfonbrener",
            "Sham M. Kakade",
            "Eran Malach"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16792",
        "abstract": "Training large language models to predict beyond their training context lengths has drawn much attention in recent years, yet the principles driving such behavior of length generalization remain underexplored. We propose a new theoretical framework to study length generalization for the next-token prediction task, as performed by decoder-only transformers. Conceptually, we show that length generalization occurs as long as each predicted token depends on a small (fixed) number of previous tokens. We formalize such tasks via a notion we call $k$-sparse planted correlation distributions, and show that an idealized model of transformers which generalize attention heads successfully length-generalize on such tasks. As a bonus, our theoretical model justifies certain techniques to modify positional embeddings which have been introduced to improve length generalization, such as position coupling.\nWe support our theoretical results with experiments on synthetic tasks and natural language, which confirm that a key factor driving length generalization is a ``sparse'' dependency structure of each token on the previous ones. Inspired by our theory, we introduce Predictive Position Coupling, which trains the transformer to predict the position IDs used in a positional coupling approach. Predictive Position Coupling thereby allows us to broaden the array of tasks to which position coupling can successfully be applied to achieve length generalization.",
        "tags": [
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "212",
        "title": "AAD-LLM: Neural Attention-Driven Auditory Scene Understanding",
        "author": [
            "Xilin Jiang",
            "Sukru Samet Dindar",
            "Vishal Choudhari",
            "Stephan Bickel",
            "Ashesh Mehta",
            "Guy M McKhann",
            "Adeen Flinker",
            "Daniel Friedman",
            "Nima Mesgarani"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16794",
        "abstract": "Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existing models do not incorporate this selectivity, limiting their ability to generate perception-aligned responses. To address this, we introduce Intention-Informed Auditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM (AAD-LLM), a prototype system that integrates brain signals to infer listener attention. AAD-LLM extends an auditory LLM by incorporating intracranial electroencephalography (iEEG) recordings to decode which speaker a listener is attending to and refine responses accordingly. The model first predicts the attended speaker from neural activity, then conditions response generation on this inferred attentional state. We evaluate AAD-LLM on speaker description, speech transcription and extraction, and question answering in multitalker scenarios, with both objective and subjective ratings showing improved alignment with listener intention. By taking a first step toward intention-aware auditory AI, this work explores a new paradigm where listener perception informs machine listening, paving the way for future listener-centered auditory systems. Demo and code available: https://aad-llm.github.io.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "213",
        "title": "Unsupervised Topic Models are Data Mixers for Pre-training Language Models",
        "author": [
            "Jiahui Peng",
            "Xinlin Zhuang",
            "Qiu Jiantao",
            "Ren Ma",
            "Jing Yu",
            "Tianyi Bai",
            "Conghui He"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16802",
        "abstract": "The performance of large language models (LLMs) is significantly affected by the quality and composition of their pre-training data, which is inherently diverse, spanning various domains, sources, and topics. Effectively integrating these heterogeneous data sources is crucial for optimizing LLM performance. Previous research has predominantly concentrated on domain-based data mixing, often neglecting the nuanced topic-level characteristics of the data. To address this gap, we propose a simple yet effective topic-based data mixing strategy that utilizes fine-grained topics generated through our topic modeling method, DataWeave. DataWeave employs a multi-stage clustering process to group semantically similar documents and utilizes LLMs to generate detailed topics, thereby facilitating a more nuanced understanding of dataset composition. Our strategy employs heuristic methods to upsample or downsample specific topics, which significantly enhances LLM performance on downstream tasks, achieving superior results compared to previous, more complex data mixing approaches. Furthermore, we confirm that the topics Science and Relationships are particularly effective, yielding the most substantial performance improvements. We will make our code and datasets publicly available.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "214",
        "title": "Multi-Agent Autonomous Driving Systems with Large Language Models: A Survey of Recent Advances",
        "author": [
            "Yaozu Wu",
            "Dongyuan Li",
            "Yankai Chen",
            "Renhe Jiang",
            "Henry Peng Zou",
            "Liancheng Fang",
            "Zhen Wang",
            "Philip S. Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16804",
        "abstract": "Autonomous Driving Systems (ADSs) are revolutionizing transportation by reducing human intervention, improving operational efficiency, and enhancing safety. Large Language Models (LLMs), known for their exceptional planning and reasoning capabilities, have been integrated into ADSs to assist with driving decision-making. However, LLM-based single-agent ADSs face three major challenges: limited perception, insufficient collaboration, and high computational demands. To address these issues, recent advancements in LLM-based multi-agent ADSs have focused on improving inter-agent communication and cooperation. This paper provides a frontier survey of LLM-based multi-agent ADSs. We begin with a background introduction to related concepts, followed by a categorization of existing LLM-based approaches based on different agent interaction modes. We then discuss agent-human interactions in scenarios where LLM-based agents engage with humans. Finally, we summarize key applications, datasets, and challenges in this field to support future research (https://anonymous.4open.science/r/LLM-based_Multi-agent_ADS-3A5C/README.md).",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "215",
        "title": "CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport Alignment for Language Models with Different Tokenizers",
        "author": [
            "Anh Duc Le",
            "Tu Vu",
            "Nam Le Hai",
            "Nguyen Thi Ngoc Diep",
            "Linh Ngo Van",
            "Trung Le",
            "Thien Huu Nguyen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16806",
        "abstract": "Large Language Models (LLMs) achieve state-of-the-art performance across various NLP tasks but face deployment challenges due to high computational costs and memory constraints. Knowledge distillation (KD) is a promising solution, transferring knowledge from large teacher models to smaller student models. However, existing KD methods often assume shared vocabularies and tokenizers, limiting their flexibility. While approaches like Universal Logit Distillation (ULD) and Dual-Space Knowledge Distillation (DSKD) address vocabulary mismatches, they overlook the critical \\textbf{reasoning-aware distillation} aspect. To bridge this gap, we propose CoT2Align a universal KD framework that integrates Chain-of-Thought (CoT) augmentation and introduces Cross-CoT Alignment to enhance reasoning transfer. Additionally, we extend Optimal Transport beyond token-wise alignment to a sequence-level and layer-wise alignment approach that adapts to varying sequence lengths while preserving contextual integrity. Comprehensive experiments demonstrate that CoT2Align outperforms existing KD methods across different vocabulary settings, improving reasoning capabilities and robustness in domain-specific tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "216",
        "title": "Grounded Persuasive Language Generation for Automated Marketing",
        "author": [
            "Jibang Wu",
            "Chenghao Yang",
            "Simon Mahns",
            "Chaoqi Wang",
            "Hao Zhu",
            "Fei Fang",
            "Haifeng Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16810",
        "abstract": "This paper develops an agentic framework that employs large language models (LLMs) to automate the generation of persuasive and grounded marketing content, using real estate listing descriptions as our focal application domain. Our method is designed to align the generated content with user preferences while highlighting useful factual attributes. This agent consists of three key modules: (1) Grounding Module, mimicking expert human behavior to predict marketable features; (2) Personalization Module, aligning content with user preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion of localized features. We conduct systematic human-subject experiments in the domain of real estate marketing, with a focus group of potential house buyers. The results demonstrate that marketing descriptions generated by our approach are preferred over those written by human experts by a clear margin. Our findings suggest a promising LLM-based agentic framework to automate large-scale targeted marketing while ensuring responsible generation using only facts.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "217",
        "title": "CLIP-SENet: CLIP-based Semantic Enhancement Network for Vehicle Re-identification",
        "author": [
            "Liping Lu",
            "Zihao Fu",
            "Duanfeng Chu",
            "Wei Wang",
            "Bingrong Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16815",
        "abstract": "Vehicle re-identification (Re-ID) is a crucial task in intelligent transportation systems (ITS), aimed at retrieving and matching the same vehicle across different surveillance cameras. Numerous studies have explored methods to enhance vehicle Re-ID by focusing on semantic enhancement. However, these methods often rely on additional annotated information to enable models to extract effective semantic features, which brings many limitations. In this work, we propose a CLIP-based Semantic Enhancement Network (CLIP-SENet), an end-to-end framework designed to autonomously extract and refine vehicle semantic attributes, facilitating the generation of more robust semantic feature representations. Inspired by zero-shot solutions for downstream tasks presented by large-scale vision-language models, we leverage the powerful cross-modal descriptive capabilities of the CLIP image encoder to initially extract general semantic information. Instead of using a text encoder for semantic alignment, we design an adaptive fine-grained enhancement module (AFEM) to adaptively enhance this general semantic information at a fine-grained level to obtain robust semantic feature representations. These features are then fused with common Re-ID appearance features to further refine the distinctions between vehicles. Our comprehensive evaluation on three benchmark datasets demonstrates the effectiveness of CLIP-SENet. Our approach achieves new state-of-the-art performance, with 92.9% mAP and 98.7% Rank-1 on VeRi-776 dataset, 90.4% Rank-1 and 98.7% Rank-5 on VehicleID dataset, and 89.1% mAP and 97.9% Rank-1 on the more challenging VeRi-Wild dataset.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "218",
        "title": "Fast, Accurate Manifold Denoising by Tunneling Riemannian Optimization",
        "author": [
            "Shiyu Wang",
            "Mariam Avagyan",
            "Yihan Shen",
            "Arnaud Lamy",
            "Tingran Wang",
            "Szabolcs MÃ¡rka",
            "Zsuzsa MÃ¡rka",
            "John Wright"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16819",
        "abstract": "Learned denoisers play a fundamental role in various signal generation (e.g., diffusion models) and reconstruction (e.g., compressed sensing) architectures, whose success derives from their ability to leverage low-dimensional structure in data. Existing denoising methods, however, either rely on local approximations that require a linear scan of the entire dataset or treat denoising as generic function approximation problems, often sacrificing efficiency and interpretability. We consider the problem of efficiently denoising a new noisy data point sampled from an unknown $d$-dimensional manifold $M \\in \\mathbb{R}^D$, using only noisy samples. This work proposes a framework for test-time efficient manifold denoising, by framing the concept of \"learning-to-denoise\" as \"learning-to-optimize\". We have two technical innovations: (i) online learning methods which learn to optimize over the manifold of clean signals using only noisy data, effectively \"growing\" an optimizer one sample at a time. (ii) mixed-order methods which guarantee that the learned optimizers achieve global optimality, ensuring both efficiency and near-optimal denoising performance. We corroborate these claims with theoretical analyses of both the complexity and denoising performance of mixed-order traversal. Our experiments on scientific manifolds demonstrate significantly improved complexity-performance tradeoffs compared to nearest neighbor search, which underpins existing provable denoising approaches based on exhaustive search.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "219",
        "title": "Uncertainty Quantification of Large Language Models through Multi-Dimensional Responses",
        "author": [
            "Tiejin Chen",
            "Xiaoou Liu",
            "Longchao Da",
            "Xiaoou Liu",
            "Vagelis Papalexakis",
            "Hua Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16820",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks due to large training datasets and powerful transformer architecture. However, the reliability of responses from LLMs remains a question. Uncertainty quantification (UQ) of LLMs is crucial for ensuring their reliability, especially in areas such as healthcare, finance, and decision-making. Existing UQ methods primarily focus on semantic similarity, overlooking the deeper knowledge dimensions embedded in responses. We introduce a multi-dimensional UQ framework that integrates semantic and knowledge-aware similarity analysis. By generating multiple responses and leveraging auxiliary LLMs to extract implicit knowledge, we construct separate similarity matrices and apply tensor decomposition to derive a comprehensive uncertainty representation. This approach disentangles overlapping information from both semantic and knowledge dimensions, capturing both semantic variations and factual consistency, leading to more accurate UQ. Our empirical evaluations demonstrate that our method outperforms existing techniques in identifying uncertain responses, offering a more robust framework for enhancing LLM reliability in high-stakes applications.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "220",
        "title": "Posterior Inference with Diffusion Models for High-dimensional Black-box Optimization",
        "author": [
            "Taeyoung Yun",
            "Kiyoung Om",
            "Jaewoo Lee",
            "Sujin Yun",
            "Jinkyoo Park"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16824",
        "abstract": "Optimizing high-dimensional and complex black-box functions is crucial in numerous scientific applications. While Bayesian optimization (BO) is a powerful method for sample-efficient optimization, it struggles with the curse of dimensionality and scaling to thousands of evaluations. Recently, leveraging generative models to solve black-box optimization problems has emerged as a promising framework. However, those methods often underperform compared to BO methods due to limited expressivity and difficulty of uncertainty estimation in high-dimensional spaces. To overcome these issues, we introduce \\textbf{DiBO}, a novel framework for solving high-dimensional black-box optimization problems. Our method iterates two stages. First, we train a diffusion model to capture the data distribution and an ensemble of proxies to predict function values with uncertainty quantification. Second, we cast the candidate selection as a posterior inference problem to balance exploration and exploitation in high-dimensional spaces. Concretely, we fine-tune diffusion models to amortize posterior inference. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines across various synthetic and real-world black-box optimization tasks. Our code is publicly available \\href{https://github.com/umkiyoung/DiBO}{here}",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "221",
        "title": "Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization",
        "author": [
            "Yao Xiao",
            "Hai Ye",
            "Linyao Chen",
            "Hwee Tou Ng",
            "Lidong Bing",
            "Xiaoli Li",
            "Roy Ka-wei Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16825",
        "abstract": "Iterative data generation and model retraining are widely used to align large language models (LLMs). It typically involves a policy model to generate on-policy responses and a reward model to guide training data selection. Direct Preference Optimization (DPO) further enhances this process by constructing preference pairs of chosen and rejected responses. In this work, we aim to \\emph{scale up} the number of on-policy samples via repeated random sampling to improve alignment performance. Conventional practice selects the sample with the highest reward as chosen and the lowest as rejected for DPO. However, our experiments reveal that this strategy leads to a \\emph{decline} in performance as the sample size increases. To address this, we investigate preference data construction through the lens of underlying normal distribution of sample rewards. We categorize the reward space into seven representative points and systematically explore all 21 ($C_7^2$) pairwise combinations. Through evaluations on four models using AlpacaEval 2, we find that selecting the rejected response at reward position $\\mu - 2\\sigma$ rather than the minimum reward, is crucial for optimal performance. We finally introduce a scalable preference data construction strategy that consistently enhances model performance as the sample scale increases.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "222",
        "title": "Predicting the Energy Landscape of Stochastic Dynamical System via Physics-informed Self-supervised Learning",
        "author": [
            "Ruikun Li",
            "Huandong Wang",
            "Qingmin Liao",
            "Yong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16828",
        "abstract": "Energy landscapes play a crucial role in shaping dynamics of many real-world complex systems. System evolution is often modeled as particles moving on a landscape under the combined effect of energy-driven drift and noise-induced diffusion, where the energy governs the long-term motion of the particles. Estimating the energy landscape of a system has been a longstanding interdisciplinary challenge, hindered by the high operational costs or the difficulty of obtaining supervisory signals. Therefore, the question of how to infer the energy landscape in the absence of true energy values is critical. In this paper, we propose a physics-informed self-supervised learning method to learn the energy landscape from the evolution trajectories of the system. It first maps the system state from the observation space to a discrete landscape space by an adaptive codebook, and then explicitly integrates energy into the graph neural Fokker-Planck equation, enabling the joint learning of energy estimation and evolution prediction. Experimental results across interdisciplinary systems demonstrate that our estimated energy has a correlation coefficient above 0.9 with the ground truth, and evolution prediction accuracy exceeds the baseline by an average of 17.65\\%. The code is available at http://github.com/tsinghua-fib-lab/PESLA.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "223",
        "title": "REGen: A Reliable Evaluation Framework for Generative Event Argument Extraction",
        "author": [
            "Omar Sharif",
            "Joseph Gatto",
            "Madhusudan Basak",
            "Sarah M. Preum"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16838",
        "abstract": "Event argument extraction identifies arguments for predefined event roles in text. Traditional evaluations rely on exact match (EM), requiring predicted arguments to match annotated spans exactly. However, this approach fails for generative models like large language models (LLMs), which produce diverse yet semantically accurate responses. EM underestimates performance by disregarding valid variations, implicit arguments (unstated but inferable), and scattered arguments (distributed across a document). To bridge this gap, we introduce Reliable Evaluation framework for Generative event argument extraction (REGen), a framework that better aligns with human judgment. Across six datasets, REGen improves performance by an average of 23.93 F1 points over EM. Human validation further confirms REGen's effectiveness, achieving 87.67% alignment with human assessments of argument correctness.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "224",
        "title": "\"Actionable Help\" in Crises: A Novel Dataset and Resource-Efficient Models for Identifying Request and Offer Social Media Posts",
        "author": [
            "Rabindra Lamsal",
            "Maria Rodriguez Read",
            "Shanika Karunasekera",
            "Muhammad Imran"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16839",
        "abstract": "During crises, social media serves as a crucial coordination tool, but the vast influx of posts--from \"actionable\" requests and offers to generic content like emotional support, behavioural guidance, or outdated information--complicates effective classification. Although generative LLMs (Large Language Models) can address this issue with few-shot classification, their high computational demands limit real-time crisis response. While fine-tuning encoder-only models (e.g., BERT) is a popular choice, these models still exhibit higher inference times in resource-constrained environments. Moreover, although distilled variants (e.g., DistilBERT) exist, they are not tailored for the crisis domain. To address these challenges, we make two key contributions. First, we present CrisisHelpOffer, a novel dataset of 101k tweets collaboratively labelled by generative LLMs and validated by humans, specifically designed to distinguish actionable content from noise. Second, we introduce the first crisis-specific mini models optimized for deployment in resource-constrained settings. Across 13 crisis classification tasks, our mini models surpass BERT (also outperform or match the performance of RoBERTa, MPNet, and BERTweet), offering higher accuracy with significantly smaller sizes and faster speeds. The Medium model is 47% smaller with 3.8% higher accuracy at 3.5x speed, the Small model is 68% smaller with a 1.8% accuracy gain at 7.7x speed, and the Tiny model, 83% smaller, matches BERT's accuracy at 18.6x speed. All models outperform existing distilled variants, setting new benchmarks. Finally, as a case study, we analyze social media posts from a global crisis to explore help-seeking and assistance-offering behaviours in selected developing and developed countries.",
        "tags": [
            "BERT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "225",
        "title": "Exploring Causes and Mitigation of Hallucinations in Large Vision Language Models",
        "author": [
            "Yaqi Sun",
            "Kyohei Atarashi",
            "Koh Takeuchi",
            "Hisashi Kashima"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16842",
        "abstract": "Large Vision-Language Models (LVLMs) integrate image encoders with Large Language Models (LLMs) to process multi-modal inputs and perform complex visual tasks. However, they often generate hallucinations by describing non-existent objects or attributes, compromising their reliability. This study analyzes hallucination patterns in image captioning, showing that not all tokens in the generation process are influenced by image input and that image dependency can serve as a useful signal for hallucination detection. To address this, we develop an automated pipeline to identify hallucinated objects and train a token-level classifier using hidden representations from parallel inference passes-with and without image input. Leveraging this classifier, we introduce a decoding strategy that effectively controls hallucination rates in image captioning at inference time.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "226",
        "title": "Improving LLM General Preference Alignment via Optimistic Online Mirror Descent",
        "author": [
            "Yuheng Zhang",
            "Dian Yu",
            "Tao Ge",
            "Linfeng Song",
            "Zhichen Zeng",
            "Haitao Mi",
            "Nan Jiang",
            "Dong Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16852",
        "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated remarkable effectiveness in aligning large language models (LLMs) with human preferences. Many existing alignment approaches rely on the Bradley-Terry (BT) model assumption, which assumes the existence of a ground-truth reward for each prompt-response pair. However, this assumption can be overly restrictive when modeling complex human preferences. In this paper, we drop the BT model assumption and study LLM alignment under general preferences, formulated as a two-player game. Drawing on theoretical insights from learning in games, we integrate optimistic online mirror descent into our alignment framework to approximate the Nash policy. Theoretically, we demonstrate that our approach achieves an $O(T^{-1})$ bound on the duality gap, improving upon the previous $O(T^{-1/2})$ result. More importantly, we implement our method and show through experiments that it outperforms state-of-the-art RLHF algorithms across multiple representative benchmarks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "227",
        "title": "SLABIM: A SLAM-BIM Coupled Dataset in HKUST Main Building",
        "author": [
            "Haoming Huang",
            "Zhijian Qiao",
            "Zehuan Yu",
            "Chuhao Liu",
            "Shaojie Shen",
            "Fumin Zhang",
            "Huan Yin"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16856",
        "abstract": "Existing indoor SLAM datasets primarily focus on robot sensing, often lacking building architectures. To address this gap, we design and construct the first dataset to couple the SLAM and BIM, named SLABIM. This dataset provides BIM and SLAM-oriented sensor data, both modeling a university building at HKUST. The as-designed BIM is decomposed and converted for ease of use. We employ a multi-sensor suite for multi-session data collection and mapping to obtain the as-built model. All the related data are timestamped and organized, enabling users to deploy and test effectively. Furthermore, we deploy advanced methods and report the experimental results on three tasks: registration, localization and semantic mapping, demonstrating the effectiveness and practicality of SLABIM. We make our dataset open-source at https://github.com/HKUST-Aerial-Robotics/SLABIM.",
        "tags": [
            "Robot",
            "Robotics",
            "SLAM"
        ]
    },
    {
        "id": "228",
        "title": "LongAttn: Selecting Long-context Training Data via Token-level Attention",
        "author": [
            "Longyun Wu",
            "Dawei Zhu",
            "Guangxiang Zhao",
            "Zhuocheng Yu",
            "Junfeng Ran",
            "Xiangyu Wong",
            "Lin Sun",
            "Sujian Li"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16860",
        "abstract": "With the development of large language models (LLMs), there has been an increasing need for significant advancements in handling long contexts. To enhance long-context capabilities, constructing high-quality training data with long-range dependencies is crucial. Existing methods to select long-context data often rely on sentence-level analysis, which can be greatly optimized in both performance and efficiency. In this paper, we propose a novel token-level framework, LongAttn, which leverages the self-attention mechanism of LLMs to measure the long-range dependencies for the data. By calculating token-level dependency strength and distribution uniformity of token scores, LongAttn effectively quantifies long-range dependencies, enabling more accurate and efficient data selection. We filter LongABC-32K from open-source long-context datasets (ArXiv, Book, and Code). Through our comprehensive experiments, LongAttn has demonstrated its excellent effectiveness, scalability, and efficiency. To facilitate future research in long-context data, we released our code and the high-quality long-context training data LongABC-32K.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "229",
        "title": "Leveraging Large Language Models for Effective and Explainable Multi-Agent Credit Assignment",
        "author": [
            "Kartik Nagpal",
            "Dayi Dong",
            "Jean-Baptiste Bouvier",
            "Negar Mehr"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16863",
        "abstract": "Recent work, spanning from autonomous vehicle coordination to in-space assembly, has shown the importance of learning collaborative behavior for enabling robots to achieve shared goals. A common approach for learning this cooperative behavior is to utilize the centralized-training decentralized-execution paradigm. However, this approach also introduces a new challenge: how do we evaluate the contributions of each agent's actions to the overall success or failure of the team. This credit assignment problem has remained open, and has been extensively studied in the Multi-Agent Reinforcement Learning literature. In fact, humans manually inspecting agent behavior often generate better credit evaluations than existing methods. We combine this observation with recent works which show Large Language Models demonstrate human-level performance at many pattern recognition tasks. Our key idea is to reformulate credit assignment to the two pattern recognition problems of sequence improvement and attribution, which motivates our novel LLM-MCA method. Our approach utilizes a centralized LLM reward-critic which numerically decomposes the environment reward based on the individualized contribution of each agent in the scenario. We then update the agents' policy networks based on this feedback. We also propose an extension LLM-TACA where our LLM critic performs explicit task assignment by passing an intermediary goal directly to each agent policy in the scenario. Both our methods far outperform the state-of-the-art on a variety of benchmarks, including Level-Based Foraging, Robotic Warehouse, and our new Spaceworld benchmark which incorporates collision-related safety constraints. As an artifact of our methods, we generate large trajectory datasets with each timestep annotated with per-agent reward information, as sampled from our LLM critics.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "230",
        "title": "Graphy'our Data: Towards End-to-End Modeling, Exploring and Generating Report from Raw Data",
        "author": [
            "Longbin Lai",
            "Changwei Luo",
            "Yunkai Lou",
            "Mingchen Ju",
            "Zhengyi Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16868",
        "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable performance in tasks such as Retrieval-Augmented Generation (RAG) and autonomous AI agent workflows. Yet, when faced with large sets of unstructured documents requiring progressive exploration, analysis, and synthesis, such as conducting literature survey, existing approaches often fall short. We address this challenge -- termed Progressive Document Investigation -- by introducing Graphy, an end-to-end platform that automates data modeling, exploration and high-quality report generation in a user-friendly manner. Graphy comprises an offline Scrapper that transforms raw documents into a structured graph of Fact and Dimension nodes, and an online Surveyor that enables iterative exploration and LLM-driven report generation. We showcase a pre-scrapped graph of over 50,000 papers -- complete with their references -- demonstrating how Graphy facilitates the literature-survey scenario. The demonstration video can be found at https://youtu.be/uM4nzkAdGlM.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "231",
        "title": "Mitigating Hallucinations in Diffusion Models through Adaptive Attention Modulation",
        "author": [
            "Trevine Oorloff",
            "Yaser Yacoob",
            "Abhinav Shrivastava"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16872",
        "abstract": "Diffusion models, while increasingly adept at generating realistic images, are notably hindered by hallucinations -- unrealistic or incorrect features inconsistent with the trained data distribution. In this work, we propose Adaptive Attention Modulation (AAM), a novel approach to mitigate hallucinations by analyzing and modulating the self-attention mechanism in diffusion models. We hypothesize that self-attention during early denoising steps may inadvertently amplify or suppress features, contributing to hallucinations. To counter this, AAM introduces a temperature scaling mechanism within the softmax operation of the self-attention layers, dynamically modulating the attention distribution during inference. Additionally, AAM employs a masked perturbation technique to disrupt early-stage noise that may otherwise propagate into later stages as hallucinations. Extensive experiments demonstrate that AAM effectively reduces hallucinatory artifacts, enhancing both the fidelity and reliability of generated images. For instance, the proposed approach improves the FID score by 20.8% and reduces the percentage of hallucinated images by 12.9% (in absolute terms) on the Hands dataset.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "232",
        "title": "A Multi-LLM-Agent-Based Framework for Economic and Public Policy Analysis",
        "author": [
            "Yuzhi Hao",
            "Danyang Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16879",
        "abstract": "This paper pioneers a novel approach to economic and public policy analysis by leveraging multiple Large Language Models (LLMs) as heterogeneous artificial economic agents. We first evaluate five LLMs' economic decision-making capabilities in solving two-period consumption allocation problems under two distinct scenarios: with explicit utility functions and based on intuitive reasoning. While previous research has often simulated heterogeneity by solely varying prompts, our approach harnesses the inherent variations in analytical capabilities across different LLMs to model agents with diverse cognitive traits. Building on these findings, we construct a Multi-LLM-Agent-Based (MLAB) framework by mapping these LLMs to specific educational groups and corresponding income brackets. Using interest-income taxation as a case study, we demonstrate how the MLAB framework can simulate policy impacts across heterogeneous agents, offering a promising new direction for economic and public policy analysis by leveraging LLMs' human-like reasoning capabilities and computational power.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "233",
        "title": "CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter",
        "author": [
            "Yepeng Weng",
            "Dianwen Mei",
            "Huishi Qiu",
            "Xujie Chen",
            "Li Liu",
            "Jiang Tian",
            "Zhongchao Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16880",
        "abstract": "Speculative decoding is a powerful technique that accelerates Large Language Model (LLM) inference by leveraging a lightweight speculative draft model. However, existing designs suffers in performance due to misalignment between training and inference. Recent methods have tried to solve this issue by adopting a multi-step training strategy, but the complex inputs of different training steps make it harder for the draft model to converge. To address this, we propose CORAL, a novel framework that improves both accuracy and efficiency in speculative drafting. CORAL introduces Cross-Step Representation Alignment, a method that enhances consistency across multiple training steps, significantly improving speculative drafting performance. Additionally, we identify the LM head as a major bottleneck in the inference speed of the draft model. We introduce a weight-grouping mechanism that selectively activates a subset of LM head parameters during inference, substantially reducing the latency of the draft model. We evaluate CORAL on three LLM families and three benchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming state-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that CORAL effectively mitigates training-inference misalignment and delivers significant speedup for modern LLMs with large vocabularies.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "234",
        "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance",
        "author": [
            "Xuanfan Ni",
            "Liyan Xu",
            "Chenyang Lyu",
            "Longyue Wang",
            "Mo Yu",
            "Lemao Liu",
            "Fandong Meng",
            "Jie Zhou",
            "Piji Li"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16886",
        "abstract": "To alleviate memory burden during inference of large language models (LLMs), numerous studies have focused on compressing the KV cache by exploring aspects such as attention sparsity. However, these techniques often require a pre-defined cache budget; as the optimal budget varies with different input lengths and task types, it limits their practical deployment accepting open-domain instructions. To address this limitation, we propose a new KV cache compression objective: to always ensure the full-cache performance regardless of specific inputs, while maximizing KV cache pruning as much as possible. To achieve this goal, we introduce a novel KV cache compression method dubbed DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance, then halting the pruning process. Empirical evaluation spanning diverse context lengths, task types, and model sizes suggests that our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average. Furthermore, our method is easy to integrate within LLM inference, not only optimizing memory space, but also showing reduced inference time compared to existing methods.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "235",
        "title": "Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text Classification without Manually Labeled Data",
        "author": [
            "Yejian Zhang",
            "Shingo Takada"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16892",
        "abstract": "Machine learning-based classifiers have been used for text classification, such as sentiment analysis, news classification, and toxic comment classification. However, supervised machine learning models often require large amounts of labeled data for training, and manual annotation is both labor-intensive and requires domain-specific knowledge, leading to relatively high annotation costs. To address this issue, we propose an approach that integrates large language models (LLMs) into an active learning framework. Our approach combines the Robustly Optimized BERT Pretraining Approach (RoBERTa), Generative Pre-trained Transformer (GPT), and active learning, achieving high cross-task text classification performance without the need for any manually labeled data. Furthermore, compared to directly applying GPT for classification tasks, our approach retains over 93% of its classification performance while requiring only approximately 6% of the computational time and monetary cost, effectively balancing performance and resource efficiency. These findings provide new insights into the efficient utilization of LLMs and active learning algorithms in text classification tasks, paving the way for their broader application.",
        "tags": [
            "BERT",
            "GPT",
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "236",
        "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment",
        "author": [
            "Chenghao Fan",
            "Zhenyi Lu",
            "Sichen Liu",
            "Xiaoye Qu",
            "Wei Wei",
            "Chengfeng Gu",
            "Yu Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16894",
        "abstract": "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. Another path for improving LoRA is incorporating a Mixture-of-Experts (MoE) architecture. However, weight misalignment and complex gradient dynamics make it challenging to adopt SVD prior to the LoRA MoE architecture. To mitigate these issues, we propose \\underline{G}reat L\\underline{o}R\\underline{A} Mixture-of-Exper\\underline{t} (GOAT), a framework that (1) adaptively integrates relevant priors using an SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by deriving a theoretical scaling factor. We demonstrate that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoE's efficiency and performance. Experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate GOAT's state-of-the-art performance, closing the gap with Full FT.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "LoRA",
            "Low-Rank Adaptation"
        ]
    },
    {
        "id": "237",
        "title": "Unlocking Scientific Concepts: How Effective Are LLM-Generated Analogies for Student Understanding and Classroom Practice?",
        "author": [
            "Zekai Shao",
            "Siyu Yuan",
            "Lin Gao",
            "Yixuan He",
            "Deqing Yang",
            "Siming Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16895",
        "abstract": "Teaching scientific concepts is essential but challenging, and analogies help students connect new concepts to familiar ideas. Advancements in large language models (LLMs) enable generating analogies, yet their effectiveness in education remains underexplored. In this paper, we first conducted a two-stage study involving high school students and teachers to assess the effectiveness of LLM-generated analogies in biology and physics through a controlled in-class test and a classroom field study. Test results suggested that LLM-generated analogies could enhance student understanding particularly in biology, but require teachers' guidance to prevent over-reliance and overconfidence. Classroom experiments suggested that teachers could refine LLM-generated analogies to their satisfaction and inspire new analogies from generated ones, encouraged by positive classroom feedback and homework performance boosts. Based on findings, we developed and evaluated a practical system to help teachers generate and refine teaching analogies. We discussed future directions for developing and evaluating LLM-supported teaching and learning by analogy.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "238",
        "title": "Zero-shot Load Forecasting for Integrated Energy Systems: A Large Language Model-based Framework with Multi-task Learning",
        "author": [
            "Jiaheng Li",
            "Donghe Li",
            "Ye Yang",
            "Huan Xi",
            "Yu Xiao",
            "Li Sun",
            "Dou An",
            "Qingyu Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16896",
        "abstract": "The growing penetration of renewable energy sources in power systems has increased the complexity and uncertainty of load forecasting, especially for integrated energy systems with multiple energy carriers. Traditional forecasting methods heavily rely on historical data and exhibit limited transferability across different scenarios, posing significant challenges for emerging applications in smart grids and energy internet. This paper proposes the TSLLM-Load Forecasting Mechanism, a novel zero-shot load forecasting framework based on large language models (LLMs) to address these challenges. The framework consists of three key components: a data preprocessing module that handles multi-source energy load data, a time series prompt generation module that bridges the semantic gap between energy data and LLMs through multi-task learning and similarity alignment, and a prediction module that leverages pre-trained LLMs for accurate forecasting. The framework's effectiveness was validated on a real-world dataset comprising load profiles from 20 Australian solar-powered households, demonstrating superior performance in both conventional and zero-shot scenarios. In conventional testing, our method achieved a Mean Squared Error (MSE) of 0.4163 and a Mean Absolute Error (MAE) of 0.3760, outperforming existing approaches by at least 8\\%. In zero-shot prediction experiments across 19 households, the framework maintained consistent accuracy with a total MSE of 11.2712 and MAE of 7.6709, showing at least 12\\% improvement over current methods. The results validate the framework's potential for accurate and transferable load forecasting in integrated energy systems, particularly beneficial for renewable energy integration and smart grid applications.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "239",
        "title": "Culture-TRIP: Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinment",
        "author": [
            "Suchae Jeong",
            "Inseong Choi",
            "Youngsik Yun",
            "Jihie Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16902",
        "abstract": "Text-to-Image models, including Stable Diffusion, have significantly improved in generating images that are highly semantically aligned with the given prompts. However, existing models may fail to produce appropriate images for the cultural concepts or objects that are not well known or underrepresented in western cultures, such as `hangari' (Korean utensil). In this paper, we propose a novel approach, Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinement (Culture-TRIP), which refines the prompt in order to improve the alignment of the image with such culture nouns in text-to-image models. Our approach (1) retrieves cultural contexts and visual details related to the culture nouns in the prompt and (2) iteratively refines and evaluates the prompt based on a set of cultural criteria and large language models. The refinement process utilizes the information retrieved from Wikipedia and the Web. Our user survey, conducted with 66 participants from eight different countries demonstrates that our proposed approach enhances the alignment between the images and the prompts. In particular, C-TRIP demonstrates improved alignment between the generated images and underrepresented culture nouns. Resource can be found at https://shane3606.github.io/Culture-TRIP.",
        "tags": [
            "Diffusion",
            "Large Language Models",
            "Text-to-Image"
        ]
    },
    {
        "id": "240",
        "title": "GuidedBench: Equipping Jailbreak Evaluation with Guidelines",
        "author": [
            "Ruixuan Huang",
            "Xunguang Wang",
            "Zongjie Li",
            "Daoyuan Wu",
            "Shuai Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16903",
        "abstract": "Jailbreaking methods for large language models (LLMs) have gained increasing attention for building safe and responsible AI systems. After analyzing 35 jailbreak methods across six categories, we find that existing benchmarks, relying on universal LLM-based or keyword-matching scores, lack case-specific criteria, leading to conflicting results. In this paper, we introduce a more robust evaluation framework for jailbreak methods, with a curated harmful question dataset, detailed case-by-case evaluation guidelines, and a scoring system equipped with these guidelines. Our experiments show that existing jailbreak methods exhibit better discrimination when evaluated using our benchmark. Some jailbreak methods that claim to achieve over 90% attack success rate (ASR) on other benchmarks only reach a maximum of 30.2% on our benchmark, providing a higher ceiling for more advanced jailbreak research; furthermore, using our scoring system reduces the variance of disagreements between different evaluator LLMs by up to 76.33%. This demonstrates its ability to provide more fair and stable evaluation.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "241",
        "title": "AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language Models",
        "author": [
            "Qin Zhu",
            "Fei Huang",
            "Runyu Peng",
            "Keming Lu",
            "Bowen Yu",
            "Qinyuan Cheng",
            "Xipeng Qiu",
            "Xuanjing Huang",
            "Junyang Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16906",
        "abstract": "While logical reasoning evaluation of Large Language Models (LLMs) has attracted significant attention, existing benchmarks predominantly rely on multiple-choice formats that are vulnerable to random guessing, leading to overestimated performance and substantial performance fluctuations. To obtain more accurate assessments of models' reasoning capabilities, we propose an automated method for synthesizing open-ended logic puzzles, and use it to develop a bilingual benchmark, AutoLogi. Our approach features program-based verification and controllable difficulty levels, enabling more reliable evaluation that better distinguishes models' reasoning abilities. Extensive evaluation of eight modern LLMs shows that AutoLogi can better reflect true model capabilities, with performance scores spanning from 35% to 73% compared to the narrower range of 21% to 37% on the source multiple-choice dataset. Beyond benchmark creation, this synthesis method can generate high-quality training data by incorporating program verifiers into the rejection sampling process, enabling systematic enhancement of LLMs' reasoning capabilities across diverse datasets.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "242",
        "title": "MambaFlow: A Novel and Flow-guided State Space Model for Scene Flow Estimation",
        "author": [
            "Jiehao Luo",
            "Jintao Cheng",
            "Xiaoyu Tang",
            "Qingwen Zhang",
            "Bohuan Xue",
            "Rui Fan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16907",
        "abstract": "Scene flow estimation aims to predict 3D motion from consecutive point cloud frames, which is of great interest in autonomous driving field. Existing methods face challenges such as insufficient spatio-temporal modeling and inherent loss of fine-grained feature during voxelization. However, the success of Mamba, a representative state space model (SSM) that enables global modeling with linear complexity, provides a promising solution. In this paper, we propose MambaFlow, a novel scene flow estimation network with a mamba-based decoder. It enables deep interaction and coupling of spatio-temporal features using a well-designed backbone. Innovatively, we steer the global attention modeling of voxel-based features with point offset information using an efficient Mamba-based decoder, learning voxel-to-point patterns that are used to devoxelize shared voxel representations into point-wise features. To further enhance the model's generalization capabilities across diverse scenarios, we propose a novel scene-adaptive loss function that automatically adapts to different motion http://patterns.Extensive experiments on the Argoverse 2 benchmark demonstrate that MambaFlow achieves state-of-the-art performance with real-time inference speed among existing works, enabling accurate flow estimation in real-world urban scenarios. The code is available at https://github.com/SCNU-RISLAB/MambaFlow.",
        "tags": [
            "3D",
            "Mamba"
        ]
    },
    {
        "id": "243",
        "title": "ENACT-Heart -- ENsemble-based Assessment Using CNN and Transformer on Heart Sounds",
        "author": [
            "Jiho Han",
            "Adnan Shaout"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16914",
        "abstract": "This study explores the application of Vision Transformer (ViT) principles in audio analysis, specifically focusing on heart sounds. This paper introduces ENACT-Heart - a novel ensemble approach that leverages the complementary strengths of Convolutional Neural Networks (CNN) and ViT through a Mixture of Experts (MoE) framework, achieving a remarkable classification accuracy of 97.52%. This outperforms the individual contributions of ViT (93.88%) and CNN (95.45%), demonstrating the potential for enhanced diagnostic accuracy in cardiovascular health monitoring. These results demonstrate the potential of ensemble methods in enhancing classification performance for cardiovascular health monitoring and diagnosis.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "244",
        "title": "Multi-Dimensional Quality Assessment for Text-to-3D Assets: Dataset and Model",
        "author": [
            "Kang Fu",
            "Huiyu Duan",
            "Zicheng Zhang",
            "Xiaohong Liu",
            "Xiongkuo Min",
            "Jia Wang",
            "Guangtao Zhai"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16915",
        "abstract": "Recent advancements in text-to-image (T2I) generation have spurred the development of text-to-3D asset (T23DA) generation, leveraging pretrained 2D text-to-image diffusion models for text-to-3D asset synthesis. Despite the growing popularity of text-to-3D asset generation, its evaluation has not been well considered and studied. However, given the significant quality discrepancies among various text-to-3D assets, there is a pressing need for quality assessment models aligned with human subjective judgments. To tackle this challenge, we conduct a comprehensive study to explore the T23DA quality assessment (T23DAQA) problem in this work from both subjective and objective perspectives. Given the absence of corresponding databases, we first establish the largest text-to-3D asset quality assessment database to date, termed the AIGC-T23DAQA database. This database encompasses 969 validated 3D assets generated from 170 prompts via 6 popular text-to-3D asset generation models, and corresponding subjective quality ratings for these assets from the perspectives of quality, authenticity, and text-asset correspondence, respectively. Subsequently, we establish a comprehensive benchmark based on the AIGC-T23DAQA database, and devise an effective T23DAQA model to evaluate the generated 3D assets from the aforementioned three perspectives, respectively.",
        "tags": [
            "3D",
            "Diffusion",
            "Text-to-3D",
            "Text-to-Image"
        ]
    },
    {
        "id": "245",
        "title": "Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties",
        "author": [
            "Zhenglin Wang",
            "Jialong Wu",
            "Pengfei LI",
            "Yong Jiang",
            "Deyu Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16922",
        "abstract": "Temporal reasoning is fundamental to human cognition and is crucial for various real-world applications. While recent advances in Large Language Models have demonstrated promising capabilities in temporal reasoning, existing benchmarks primarily rely on rule-based construction, lack contextual depth, and involve a limited range of temporal entities. To address these limitations, we introduce Chinese Time Reasoning (CTM), a benchmark designed to evaluate LLMs on temporal reasoning within the extensive scope of Chinese dynastic chronology. CTM emphasizes cross-entity relationships, pairwise temporal alignment, and contextualized and culturally-grounded reasoning, providing a comprehensive evaluation. Extensive experimental results reveal the challenges posed by CTM and highlight potential avenues for improvement.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "246",
        "title": "A Systematic Survey of Automatic Prompt Optimization Techniques",
        "author": [
            "Kiran Ramnath",
            "Kang Zhou",
            "Sheng Guan",
            "Soumya Smruti Mishra",
            "Xuan Qi",
            "Zhengyuan Shen",
            "Shuai Wang",
            "Sangmin Woo",
            "Sullam Jeoung",
            "Yawei Wang",
            "Haozhu Wang",
            "Han Ding",
            "Yuzhe Lu",
            "Zhichao Xu",
            "Yun Zhou",
            "Balasubramaniam Srinivasan",
            "Qiaojing Yan",
            "Yueyan Chen",
            "Haibo Ding",
            "Panpan Xu",
            "Lin Lee Cheong"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16923",
        "abstract": "Since the advent of large language models (LLMs), prompt engineering has been a crucial step for eliciting desired responses for various Natural Language Processing (NLP) tasks. However, prompt engineering remains an impediment for end users due to rapid advances in models, tasks, and associated best practices. To mitigate this, Automatic Prompt Optimization (APO) techniques have recently emerged that use various automated techniques to help improve the performance of LLMs on various tasks. In this paper, we present a comprehensive survey summarizing the current progress and remaining challenges in this field. We provide a formal definition of APO, a 5-part unifying framework, and then proceed to rigorously categorize all relevant works based on their salient features therein. We hope to spur further research guided by our framework.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "247",
        "title": "BigMac: A Communication-Efficient Mixture-of-Experts Model Structure for Fast Training and Inference",
        "author": [
            "Zewen Jin",
            "Shengnan Wang",
            "Jiaan Zhu",
            "Hongrui Zhan",
            "Youhui Bai",
            "Lin Zhang",
            "Zhenyu Ming",
            "Cheng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16927",
        "abstract": "The Mixture-of-Experts (MoE) structure scales the Transformer-based large language models (LLMs) and improves their performance with only the sub-linear increase in computation resources. Recently, a fine-grained DeepSeekMoE structure is proposed, which can further improve the computing efficiency of MoE without performance degradation. However, the All-to-All communication introduced by MoE has become a bottleneck, especially for the fine-grained structure, which typically involves and activates more experts, hence contributing to heavier communication overhead.\nIn this paper, we propose a novel MoE structure named BigMac, which is also fine-grained but with high communication efficiency. The innovation of BigMac is mainly due to that we abandon the \\textbf{c}ommunicate-\\textbf{d}escend-\\textbf{a}scend-\\textbf{c}ommunicate (CDAC) manner used by fine-grained MoE, which leads to the All-to-All communication always taking place at the highest dimension. Instead, BigMac designs an efficient \\textbf{d}escend-\\textbf{c}ommunicate-\\textbf{c}ommunicate-\\textbf{a}scend (DCCA) manner. Specifically, we add a descending and ascending projection at the entrance and exit of the expert, respectively, which enables the communication to perform at a very low dimension. Furthermore, to adapt to DCCA, we re-design the structure of small experts, ensuring that the expert in BigMac has enough complexity to address tokens. Experimental results show that BigMac achieves comparable or even better model quality than fine-grained MoEs with the same number of experts and a similar number of total parameters. Equally importantly, BigMac reduces the end-to-end latency by up to 3.09$\\times$ for training and increases the throughput by up to 3.11$\\times$ for inference on state-of-the-art AI computing frameworks including Megatron, Tutel, and DeepSpeed-Inference.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "248",
        "title": "Reasoning Does Not Necessarily Improve Role-Playing Ability",
        "author": [
            "Xiachong Feng",
            "Longxu Dou",
            "Lingpeng Kong"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16940",
        "abstract": "The application of role-playing large language models (LLMs) is rapidly expanding in both academic and commercial domains, driving an increasing demand for high-precision role-playing models. Simultaneously, the rapid advancement of reasoning techniques has continuously pushed the performance boundaries of LLMs. This intersection of practical role-playing demands and evolving reasoning capabilities raises an important research question: \"Can reasoning techniques enhance the role-playing capabilities of LLMs?\" To address this, we conduct a comprehensive study using 6 role-playing benchmarks, 24 LLMs, and 3 distinct role-playing strategies, comparing the effectiveness of direct zero-shot role-playing, role-playing with Chain-of-Thought (CoT), and role-playing using reasoning-optimized LLMs. Our findings reveal that CoT may reduce role-playing performance, reasoning-optimized LLMs are unsuitable for role-playing, reasoning ability disrupts the role-playing scaling law, large models still lack proficiency in advanced role-playing, and Chinese role-playing performance surpasses English role-playing performance. Furthermore, based on extensive experimental results, we propose two promising future research directions: Role-aware CoT for improving role-playing LLMs and Reinforcement Learning for role-playing LLMs, aiming to enhance the adaptability, consistency, and effectiveness of role-playing LLMs for both research and real-world applications.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "249",
        "title": "Gaussian Difference: Find Any Change Instance in 3D Scenes",
        "author": [
            "Binbin Jiang",
            "Rui Huang",
            "Qingyi Zhao",
            "Yuxiang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16941",
        "abstract": "Instance-level change detection in 3D scenes presents significant challenges, particularly in uncontrolled environments lacking labeled image pairs, consistent camera poses, or uniform lighting conditions. This paper addresses these challenges by introducing a novel approach for detecting changes in real-world scenarios. Our method leverages 4D Gaussians to embed multiple images into Gaussian distributions, enabling the rendering of two coherent image sequences. We segment each image and assign unique identifiers to instances, facilitating efficient change detection through ID comparison. Additionally, we utilize change maps and classification encodings to categorize 4D Gaussians as changed or unchanged, allowing for the rendering of comprehensive change maps from any viewpoint. Extensive experiments across various instance-level change detection datasets demonstrate that our method significantly outperforms state-of-the-art approaches like C-NERF and CYWS-3D, especially in scenarios with substantial lighting variations. Our approach offers improved detection accuracy, robustness to lighting changes, and efficient processing times, advancing the field of 3D change detection.",
        "tags": [
            "3D",
            "Detection",
            "NeRF"
        ]
    },
    {
        "id": "250",
        "title": "Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance",
        "author": [
            "Chenghua Huang",
            "Lu Wang",
            "Fangkai Yang",
            "Pu Zhao",
            "Zhixu Li",
            "Qingwei Lin",
            "Dongmei Zhang",
            "Saravan Rajmohan",
            "Qi Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16944",
        "abstract": "Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with a pretrained, fixed reward model for guidance. This approach increases computational complexity and instability due to actor-critic interdependence. Additionally, PPO lacks access to true environment rewards in LLM tasks, limiting its adaptability. Under such conditions, pretraining a value model or a reward model becomes equivalent, as both provide fixed supervisory signals without new ground-truth feedback. To address these issues, we propose \\textbf{Decoupled Value Policy Optimization (DVPO)}, a lean framework that replaces traditional reward modeling with a pretrained \\emph{global value model (GVM)}. The GVM is conditioned on policy trajectories and predicts token-level return-to-go estimates. By decoupling value model from policy training (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40\\% and training time by 35\\% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in performance.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RL"
        ]
    },
    {
        "id": "251",
        "title": "Atten-Transformer: A Deep Learning Framework for User App Usage Prediction",
        "author": [
            "Longlong Li",
            "Cunquan Qu",
            "Guanghui Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16957",
        "abstract": "Accurately predicting smartphone app usage patterns is crucial for user experience optimization and targeted marketing. However, existing methods struggle to capture intricate dependencies in user behavior, particularly in sparse or complex usage scenarios. To address these challenges, we introduce Atten-Transformer, a novel model that integrates temporal attention with a Transformer network to dynamically identify and leverage key app usage patterns. Unlike conventional methods that primarily consider app order and duration, our approach employs a multi-dimensional feature representation, incorporating both feature encoding and temporal encoding to enhance predictive accuracy. The proposed attention mechanism effectively assigns importance to critical app usage moments, improving both model interpretability and generalization. Extensive experiments on multiple smartphone usage datasets, including LSapp and Tsinghua App Usage datasets, demonstrate that Atten-Transformer consistently outperforms state-of-the-art models across different data splits. Specifically, our model achieves a 45.24\\% improvement in HR@1 on the Tsinghua dataset (Time-based Split) and a 18.25\\% improvement in HR@1 on the LSapp dataset (Cold Start Split), showcasing its robustness across diverse app usage scenarios. These findings highlight the potential of integrating adaptive attention mechanisms in mobile usage forecasting, paving the way for enhanced user engagement and resource allocation.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "252",
        "title": "UrduLLaMA 1.0: Dataset Curation, Preprocessing, and Evaluation in Low-Resource Settings",
        "author": [
            "Layba Fiaz",
            "Munief Hassan Tahir",
            "Sana Shams",
            "Sarmad Hussain"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16961",
        "abstract": "Multilingual Large Language Models (LLMs) often provide suboptimal performance on low-resource languages like Urdu. This paper introduces UrduLLaMA 1.0, a model derived from the open-source Llama-3.1-8B-Instruct architecture and continually pre-trained on 128 million Urdu tokens, capturing the rich diversity of the language. To enhance instruction-following and translation capabilities, we leverage Low-Rank Adaptation (LoRA) to fine tune the model on 41,000 Urdu instructions and approximately 50,000 English-Urdu translation pairs. Evaluation across three machine translation datasets demonstrates significant performance improvements compared to state-of-the-art (SOTA) models, establishing a new benchmark for Urdu LLMs. These findings underscore the potential of targeted adaptation strategies with limited data and computational resources to address the unique challenges of low-resource languages.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models",
            "LoRA",
            "Low-Rank Adaptation"
        ]
    },
    {
        "id": "253",
        "title": "Make LLM Inference Affordable to Everyone: Augmenting GPU Memory with NDP-DIMM",
        "author": [
            "Lian Liu",
            "Shixin Zhao",
            "Bing Li",
            "Haimeng Ren",
            "Zhaohui Xu",
            "Mengdi Wang",
            "Xiaowei Li",
            "Yinhe Han",
            "Ying Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16963",
        "abstract": "The billion-scale Large Language Models (LLMs) need deployment on expensive server-grade GPUs with large-storage HBMs and abundant computation capability. As LLM-assisted services become popular, achieving cost-effective LLM inference on budget-friendly hardware becomes the trend. Extensive researches relocate LLM parameters from expensive GPUs to host memory. However, the restricted bandwidth between the host and GPU memory limits the inference performance.\nThis work introduces Hermes, a budget-friendly system that leverages the near-data processing (NDP) within commodity DRAM DIMMs to enhance the performance of a single consumer-grade GPU, achieving efficient LLM inference. The inherent activation sparsity in LLMs naturally divides weight parameters into two categories, termed ``hot\" and ``cold\" neurons, respectively. Hot neurons, which consist of only approximately 20\\% of all weight parameters, account for 80\\% of the total computational load, while cold neurons make up the other 80\\% of parameters but are responsible for just 20\\% of the computational load. Therefore, we propose a heterogeneous computing strategy: mapping hot neurons to a single computation-efficient GPU, while offloading cold neurons to NDP-DIMMs, which offer large memory size but limited computation capabilities. Meanwhile, the dynamic nature of activation sparsity needs a real-time partition of hot/cold neurons and adaptive remapping of cold neurons across multiple NDP-DIMM modules. Therefore, we introduce a lightweight predictor optimizing real-time neuron partition and adjustment between GPU and NDP-DIMMs. We also utilize a window-based online scheduling mechanism to maintain load balance among NDP-DIMM modules. Hermes facilitates the deployment of LLaMA2-70B on consumer-grade hardware at 13.75 tokens/s and realizes an average 75.24$\\times$ speedup over the state-of-the-art offloading-based inference system.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "254",
        "title": "Autoregressive Image Generation Guided by Chains of Thought",
        "author": [
            "Miaomiao Cai",
            "Guanjie Wang",
            "Wei Li",
            "Zhijun Tu",
            "Hanting Chen",
            "Shaohui Lin",
            "Jie Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16965",
        "abstract": "In the field of autoregressive (AR) image generation, models based on the 'next-token prediction' paradigm of LLMs have shown comparable performance to diffusion models by reducing inductive biases. However, directly applying LLMs to complex image generation can struggle with reconstructing the structure and details of the image, impacting the accuracy and stability of generation. Additionally, the 'next-token prediction' paradigm in the AR model does not align with the contextual scanning and logical reasoning processes involved in human visual perception, limiting effective image generation. Chain-of-Thought (CoT), as a key reasoning capability of LLMs, utilizes reasoning prompts to guide the model, improving reasoning performance on complex natural language process (NLP) tasks, enhancing accuracy and stability of generation, and helping the model maintain contextual coherence and logical consistency, similar to human reasoning. Inspired by CoT from the field of NLP, we propose autoregressive Image Generation with Thoughtful Reasoning (IGTR) to enhance autoregressive image generation. IGTR adds reasoning prompts without modifying the model structure or raster generation order. Specifically, we design specialized image-related reasoning prompts for AR image generation to simulate the human reasoning process, which enhances contextual reasoning by allowing the model to first perceive overall distribution information before generating the image, and improve generation stability by increasing the inference steps. Compared to the AR method without prompts, our method shows outstanding performance and achieves an approximate improvement of 20%.",
        "tags": [
            "Diffusion",
            "LLMs"
        ]
    },
    {
        "id": "255",
        "title": "LongSafety: Evaluating Long-Context Safety of Large Language Models",
        "author": [
            "Yida Lu",
            "Jiale Cheng",
            "Zhexin Zhang",
            "Shiyao Cui",
            "Cunxiang Wang",
            "Xiaotao Gu",
            "Yuxiao Dong",
            "Jie Tang",
            "Hongning Wang",
            "Minlie Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16971",
        "abstract": "As Large Language Models (LLMs) continue to advance in understanding and generating long sequences, new safety concerns have been introduced through the long context. However, the safety of LLMs in long-context tasks remains under-explored, leaving a significant gap in both evaluation and improvement of their safety. To address this, we introduce LongSafety, the first comprehensive benchmark specifically designed to evaluate LLM safety in open-ended long-context tasks. LongSafety encompasses 7 categories of safety issues and 6 user-oriented long-context tasks, with a total of 1,543 test cases, averaging 5,424 words per context. Our evaluation towards 16 representative LLMs reveals significant safety vulnerabilities, with most models achieving safety rates below 55%. Our findings also indicate that strong safety performance in short-context scenarios does not necessarily correlate with safety in long-context tasks, emphasizing the unique challenges and urgency of improving long-context safety. Moreover, through extensive analysis, we identify challenging safety issues and task types for long-context models. Furthermore, we find that relevant context and extended input sequences can exacerbate safety risks in long-context scenarios, highlighting the critical need for ongoing attention to long-context safety challenges. Our code and data are available at https://github.com/thu-coai/LongSafety.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "256",
        "title": "TraFlow: Trajectory Distillation on Pre-Trained Rectified Flow",
        "author": [
            "Zhangkai Wu",
            "Xuhui Fan",
            "Hongyu Wu",
            "Longbing Cao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16972",
        "abstract": "Majorities of distillation methods on pre-trained diffusion models or on pre-trained rectified flow, focus on either the distillation outputs or the trajectories between random noises and clean images to speed up sample generations from pre-trained models. In those trajectory-based distillation methods, consistency distillation requires the self-consistent trajectory projection to regulate the trajectory, which might avoid the common ODE approximation error {while still be concerning about sampling efficiencies}. At the same time, rectified flow distillations enforce straight trajectory for fast sampling, although an ODE solver is still required. In this work, we propose a trajectory distillation method, \\modelname, that enjoys the benefits of both and enables few-step generations. TraFlow adopts the settings of consistency trajectory models, and further enforces the properties of self-consistency and straightness throughout the entire trajectory. These two properties are pursued by reaching a balance with following three targets: (1) reconstruct the output from pre-trained models; (2) learn the amount of changes by pre-trained models; (3) satisfy the self-consistency over its trajectory. Extensive experimental results have shown the effectiveness of our proposed method.",
        "tags": [
            "Diffusion",
            "ODE",
            "Rectified Flow"
        ]
    },
    {
        "id": "257",
        "title": "Hotter and Colder: A New Approach to Annotating Sentiment, Emotions, and Bias in Icelandic Blog Comments",
        "author": [
            "Steinunn Rut FriÃ°riksdÃ³ttir",
            "Dan Saattrup Nielsen",
            "Hafsteinn Einarsson"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16987",
        "abstract": "This paper presents Hotter and Colder, a dataset designed to analyze various types of online behavior in Icelandic blog comments. Building on previous work, we used GPT-4o mini to annotate approximately 800,000 comments for 25 tasks, including sentiment analysis, emotion detection, hate speech, and group generalizations. Each comment was automatically labeled on a 5-point Likert scale. In a second annotation stage, comments with high or low probabilities of containing each examined behavior were subjected to manual revision. By leveraging crowdworkers to refine these automatically labeled comments, we ensure the quality and accuracy of our dataset resulting in 12,232 uniquely annotated comments and 19,301 annotations. Hotter and Colder provides an essential resource for advancing research in content moderation and automatically detectiong harmful online behaviors in Icelandic.",
        "tags": [
            "Detection",
            "GPT"
        ]
    },
    {
        "id": "258",
        "title": "Semantic Neural Radiance Fields for Multi-Date Satellite Data",
        "author": [
            "Valentin Wagner",
            "Sebastian Bullinger",
            "Christoph Bodensteiner",
            "Michael Arens"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16992",
        "abstract": "In this work we propose a satellite specific Neural Radiance Fields (NeRF) model capable to obtain a three-dimensional semantic representation (neural semantic field) of the scene. The model derives the output from a set of multi-date satellite images with corresponding pixel-wise semantic labels. We demonstrate the robustness of our approach and its capability to improve noisy input labels. We enhance the color prediction by utilizing the semantic information to address temporal image inconsistencies caused by non-stationary categories such as vehicles. To facilitate further research in this domain, we present a dataset comprising manually generated labels for popular multi-view satellite images. Our code and dataset are available at https://github.com/wagnva/semantic-nerf-for-satellite-data.",
        "tags": [
            "NeRF"
        ]
    },
    {
        "id": "259",
        "title": "FADE: Why Bad Descriptions Happen to Good Features",
        "author": [
            "Bruno Puri",
            "Aakriti Jain",
            "Elena Golimblevskaia",
            "Patrick Kahardipraja",
            "Thomas Wiegand",
            "Wojciech Samek",
            "Sebastian Lapuschkin"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16994",
        "abstract": "Recent advances in mechanistic interpretability have highlighted the potential of automating interpretability pipelines in analyzing the latent representations within LLMs. While they may enhance our understanding of internal mechanisms, the field lacks standardized evaluation methods for assessing the validity of discovered features. We attempt to bridge this gap by introducing FADE: Feature Alignment to Description Evaluation, a scalable model-agnostic framework for evaluating feature-description alignment. FADE evaluates alignment across four key metrics - Clarity, Responsiveness, Purity, and Faithfulness - and systematically quantifies the causes for the misalignment of feature and their description. We apply FADE to analyze existing open-source feature descriptions, and assess key components of automated interpretability pipelines, aiming to enhance the quality of descriptions. Our findings highlight fundamental challenges in generating feature descriptions, particularly for SAEs as compared to MLP neurons, providing insights into the limitations and future directions of automated interpretability. We release FADE as an open-source package at: https://github.com/brunibrun/FADE.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "260",
        "title": "PQDAST: Depth-Aware Arbitrary Style Transfer for Games via Perceptual Quality-Guided Distillation",
        "author": [
            "Eleftherios Ioannou",
            "Steve Maddock"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16996",
        "abstract": "Artistic style transfer is concerned with the generation of imagery that combines the content of an image with the style of an artwork. In the realm of computer games, most work has focused on post-processing video frames. Some recent work has integrated style transfer into the game pipeline, but it is limited to single styles. Integrating an arbitrary style transfer method into the game pipeline is challenging due to the memory and speed requirements of games. We present PQDAST, the first solution to address this. We use a perceptual quality-guided knowledge distillation framework and train a compressed model using the FLIP evaluator, which substantially reduces both memory usage and processing time with limited impact on stylisation quality. For better preservation of depth and fine details, we utilise a synthetic dataset with depth and temporal considerations during training. The developed model is injected into the rendering pipeline to further enforce temporal stability and avoid diminishing post-process effects. Quantitative and qualitative experiments demonstrate that our approach achieves superior performance in temporal consistency, with comparable style transfer quality, to state-of-the-art image, video and in-game methods.",
        "tags": [
            "Style Transfer"
        ]
    },
    {
        "id": "261",
        "title": "An Enhanced Large Language Model For Cross Modal Query Understanding System Using DL-KeyBERT Based CAZSSCL-MPGPT",
        "author": [
            "Shreya Singh"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17000",
        "abstract": "Large Language Models (LLMs) are advanced deep-learning models designed to understand and generate human language. They work together with models that process data like images, enabling cross-modal understanding. However, existing approaches often suffer from the echo chamber effect, where redundant visual patterns reduce model generalization and accuracy. Thus, the proposed system considered this limitation and developed an enhanced LLM-based framework for cross-modal query understanding using DL-KeyBERT-based CAZSSCL-MPGPT. The collected dataset consists of pre-processed images and texts. The preprocessed images then undergo object segmentation using Easom-You Only Look Once (E-YOLO). The object skeleton is generated, along with the knowledge graph using a Conditional Random Knowledge Graph (CRKG) technique. Further, features are extracted from the knowledge graph, generated skeletons, and segmented objects. The optimal features are then selected using the Fossa Optimization Algorithm (FOA). Meanwhile, the text undergoes word embedding using DL-KeyBERT. Finally, the cross-modal query understanding system utilizes CAZSSCL-MPGPT to generate accurate and contextually relevant image descriptions as text. The proposed CAZSSCL-MPGPT achieved an accuracy of 99.14187362% in the COCO dataset 2017 and 98.43224393% in the vqav2-val dataset.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Segmentation"
        ]
    },
    {
        "id": "262",
        "title": "Be CIM or Be Memory: A Dual-mode-aware DNN Compiler for CIM Accelerators",
        "author": [
            "Shixin Zhao",
            "Yuming Li",
            "Bing Li",
            "Yintao He",
            "Mengdi Wang",
            "Yinhe Han",
            "Ying Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17006",
        "abstract": "Computing-in-memory (CIM) architectures demonstrate superior performance over traditional architectures. To unleash the potential of CIM accelerators, many compilation methods have been proposed, focusing on application scheduling optimization specific to CIM. However, existing compilation methods often overlook CIM's capability to switch dynamically between compute and memory modes, which is crucial for accommodating the diverse memory and computational needs of real-world deep neural network architectures, especially the emerging large language models. To fill this gap, we introduce CMSwitch, a novel compiler to optimize resource allocation for CIM accelerators with adaptive mode-switching capabilities, thereby enhancing the performance of DNN applications. Specifically, our approach integrates the compute-memory mode switch into the CIM compilation optimization space by introducing a new hardware abstraction attribute. Then, we propose a novel compilation optimization pass that identifies the optimal network segment and the corresponding mode resource allocations using dynamic programming and mixed-integer programming. CMSwitch uses the tailored meta-operator to express the compilation result in a generalized manner. Evaluation results demonstrate that CMSwitch achieves an average speedup of 1.31$\\times$ compared to existing SOTA CIM compilation works, highlighting CMSwitch's effectiveness in fully exploiting the potential of CIM processors for a wide range of real-world DNN applications.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "263",
        "title": "Quantifying Logical Consistency in Transformers via Query-Key Alignment",
        "author": [
            "Eduard Tulchinskii",
            "Anastasia Voznyuk",
            "Laida Kushnareva",
            "Andrei Andriiainen",
            "Irina Piontkovskaya",
            "Evgeny Burnaev",
            "Serguei Barannikov"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17017",
        "abstract": "Large language models (LLMs) have demonstrated impressive performance in various natural language processing tasks, yet their ability to perform multi-step logical reasoning remains an open challenge. Although Chain-of-Thought prompting has improved logical reasoning by enabling models to generate intermediate steps, it lacks mechanisms to assess the coherence of these logical transitions. In this paper, we propose a novel, lightweight evaluation strategy for logical reasoning that uses query-key alignments inside transformer attention heads. By computing a single forward pass and extracting a \"QK-score\" from carefully chosen heads, our method reveals latent representations that reliably separate valid from invalid inferences, offering a scalable alternative to traditional ablation-based techniques. We also provide an empirical validation on multiple logical reasoning benchmarks, demonstrating improved robustness of our evaluation method against distractors and increased reasoning depth. The experiments were conducted on a diverse set of models, ranging from 1.5B to 70B parameters.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "264",
        "title": "Towards Auto-Regressive Next-Token Prediction: In-Context Learning Emerges from Generalization",
        "author": [
            "Zixuan Gong",
            "Xiaolin Hu",
            "Huayi Tang",
            "Yong Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17024",
        "abstract": "Large language models (LLMs) have demonstrated remarkable in-context learning (ICL) abilities. However, existing theoretical analysis of ICL primarily exhibits two limitations: (a) Limited i.i.d. Setting. Most studies focus on supervised function learning tasks where prompts are constructed with i.i.d. input-label pairs. This i.i.d. assumption diverges significantly from real language learning scenarios where prompt tokens are interdependent. (b) Lack of Emergence Explanation. Most literature answers what ICL does from an implicit optimization perspective but falls short in elucidating how ICL emerges and the impact of pre-training phase on ICL. In our paper, to extend (a), we adopt a more practical paradigm, auto-regressive next-token prediction (AR-NTP), which closely aligns with the actual training of language models. Specifically, within AR-NTP, we emphasize prompt token-dependency, which involves predicting each subsequent token based on the preceding sequence. To address (b), we formalize a systematic pre-training and ICL framework, highlighting the layer-wise structure of sequences and topics, alongside a two-level expectation. In conclusion, we present data-dependent, topic-dependent and optimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs, investigating that ICL emerges from the generalization of sequences and topics. Our theory is supported by experiments on numerical linear dynamic systems, synthetic GINC and real-world language datasets.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "265",
        "title": "Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology",
        "author": [
            "Longchao Da",
            "Xiaoou Liu",
            "Jiaxin Dai",
            "Lu Cheng",
            "Yaqing Wang",
            "Hua Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17026",
        "abstract": "Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, and thus provides insights into the reliability of LLM's output regarding a question. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a reasoning topology perspective. By designing a structural elicitation strategy, we guide the LLMs to frame the explanations of an answer into a graph topology. This process decomposes the explanations into the knowledge related sub-questions and topology-based reasoning structures, which allows us to quantify uncertainty not only at the semantic level but also from the reasoning path. It further brings convenience to assess knowledge redundancy and provide interpretable insights into the reasoning process. Our method offers a systematic way to interpret the LLM reasoning, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations and demonstrates the potential of topology-based quantification.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "266",
        "title": "Distributional Vision-Language Alignment by Cauchy-Schwarz Divergence",
        "author": [
            "Wenzhe Yin",
            "Zehao Xiao",
            "Pan Zhou",
            "Shujian Yu",
            "Jiayi Shen",
            "Jan-Jakob Sonke",
            "Efstratios Gavves"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17028",
        "abstract": "Multimodal alignment is crucial for various downstream tasks such as cross-modal generation and retrieval. Previous multimodal approaches like CLIP maximize the mutual information mainly by aligning pairwise samples across modalities while overlooking the distributional differences, leading to suboptimal alignment with modality gaps. In this paper, to overcome the limitation, we propose CS-Aligner, a novel and straightforward framework that performs distributional vision-language alignment by integrating Cauchy-Schwarz (CS) divergence with mutual information. In the proposed framework, we find that the CS divergence and mutual information serve complementary roles in multimodal alignment, capturing both the global distribution information of each modality and the pairwise semantic relationships, yielding tighter and more precise alignment. Moreover, CS-Aligher enables incorporating additional information from unpaired data and token-level representations, enhancing flexible and fine-grained alignment in practice. Experiments on text-to-image generation and cross-modality retrieval tasks demonstrate the effectiveness of our method on vision-language alignment.",
        "tags": [
            "CLIP",
            "Text-to-Image"
        ]
    },
    {
        "id": "267",
        "title": "Evolution 6.0: Evolving Robotic Capabilities Through Generative Design",
        "author": [
            "Muhammad Haris Khan",
            "Artyom Myshlyaev",
            "Artyom Lykov",
            "Miguel Altamirano Cabrera",
            "Dzmitry Tsetserukou"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17034",
        "abstract": "We propose a new concept, Evolution 6.0, which represents the evolution of robotics driven by Generative AI. When a robot lacks the necessary tools to accomplish a task requested by a human, it autonomously designs the required instruments and learns how to use them to achieve the goal. Evolution 6.0 is an autonomous robotic system powered by Vision-Language Models (VLMs), Vision-Language Action (VLA) models, and Text-to-3D generative models for tool design and task execution. The system comprises two key modules: the Tool Generation Module, which fabricates task-specific tools from visual and textual data, and the Action Generation Module, which converts natural language instructions into robotic actions. It integrates QwenVLM for environmental understanding, OpenVLA for task execution, and Llama-Mesh for 3D tool generation. Evaluation results demonstrate a 90% success rate for tool generation with a 10-second inference time, and action generation achieving 83.5% in physical and visual generalization, 70% in motion generalization, and 37% in semantic generalization. Future improvements will focus on bimanual manipulation, expanded task capabilities, and enhanced environmental interpretation to improve real-world adaptability.",
        "tags": [
            "3D",
            "LLaMA",
            "Robot",
            "Robotics",
            "Text-to-3D"
        ]
    },
    {
        "id": "268",
        "title": "PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance",
        "author": [
            "Haoran Li",
            "Wenbin Hu",
            "Huihao Jing",
            "Yulin Chen",
            "Qi Hu",
            "Sirui Han",
            "Tianshu Chu",
            "Peizhao Hu",
            "Yangqiu Song"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17041",
        "abstract": "Recent advancements in generative large language models (LLMs) have enabled wider applicability, accessibility, and flexibility. However, their reliability and trustworthiness are still in doubt, especially for concerns regarding individuals' data privacy. Great efforts have been made on privacy by building various evaluation benchmarks to study LLMs' privacy awareness and robustness from their generated outputs to their hidden representations. Unfortunately, most of these works adopt a narrow formulation of privacy and only investigate personally identifiable information (PII). In this paper, we follow the merit of the Contextual Integrity (CI) theory, which posits that privacy evaluation should not only cover the transmitted attributes but also encompass the whole relevant social context through private information flows. We present PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted at legal compliance to cover well-annotated privacy and safety regulations, real court cases, privacy policies, and synthetic data built from the official toolkit to study LLMs' privacy and safety compliance. We evaluate the latest LLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our experimental results suggest that though LLMs can effectively capture key CI parameters inside a given context, they still require further advancements for privacy compliance.",
        "tags": [
            "DeepSeek",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "269",
        "title": "Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam",
        "author": [
            "Tianjin Huang",
            "Haotian Hu",
            "Zhenyu Zhang",
            "Gaojie Jin",
            "Xiang Li",
            "Li Shen",
            "Tianlong Chen",
            "Lu Liu",
            "Qingsong Wen",
            "Zhangyang Wang",
            "Shiwei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17055",
        "abstract": "This paper comprehensively evaluates several recently proposed optimizers for 4-bit training, revealing that low-bit precision amplifies sensitivity to learning rates and often causes unstable gradient norms, leading to divergence at higher learning rates. Among these, SPAM, a recent optimizer featuring momentum reset and spike-aware gradient clipping, achieves the best performance across various bit levels, but struggles to stabilize gradient norms, requiring careful learning rate tuning. To address these limitations, we propose Stable-SPAM, which incorporates enhanced gradient normalization and clipping techniques. In particular, Stable-SPAM (1) adaptively updates the clipping threshold for spiked gradients by tracking their historical maxima; (2) normalizes the entire gradient matrix based on its historical $l_2$-norm statistics; and $(3)$ inherits momentum reset from SPAM to periodically reset the first and second moments of Adam, mitigating the accumulation of spiked gradients. Extensive experiments show that Stable-SPAM effectively stabilizes gradient norms in 4-bit LLM training, delivering superior performance compared to Adam and SPAM. Notably, our 4-bit LLaMA-1B model trained with Stable-SPAM outperforms the BF16 LLaMA-1B trained with Adam by up to $2$ perplexity. Furthermore, when both models are trained in 4-bit, Stable-SPAM achieves the same loss as Adam while requiring only about half the training steps. Code is available at https://github.com/TianjinYellow/StableSPAM.git.",
        "tags": [
            "LLaMA"
        ]
    },
    {
        "id": "270",
        "title": "LLM-QE: Improving Query Expansion by Aligning Large Language Models with Ranking Preferences",
        "author": [
            "Sijia Yao",
            "Pengcheng Huang",
            "Zhenghao Liu",
            "Yu Gu",
            "Yukun Yan",
            "Shi Yu",
            "Ge Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17057",
        "abstract": "Query expansion plays a crucial role in information retrieval, which aims to bridge the semantic gap between queries and documents to improve matching performance. This paper introduces LLM-QE, a novel approach that leverages Large Language Models (LLMs) to generate document-based query expansions, thereby enhancing dense retrieval models. Unlike traditional methods, LLM-QE designs both rank-based and answer-based rewards and uses these reward models to optimize LLMs to align with the ranking preferences of both retrievers and LLMs, thus mitigating the hallucination of LLMs during query expansion. Our experiments on the zero-shot dense retrieval model, Contriever, demonstrate the effectiveness of LLM-QE, achieving an improvement of over 8%. Furthermore, by incorporating answer-based reward modeling, LLM-QE generates more relevant and precise information related to the documents, rather than simply producing redundant tokens to maximize rank-based rewards. Notably, LLM-QE also improves the training process of dense retrievers, achieving a more than 5% improvement after fine-tuning. All codes are available at https://github.com/NEUIR/LLM-QE.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "271",
        "title": "Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability",
        "author": [
            "Ashhadul Islam",
            "Samir Brahim Belhaouari",
            "Amine Bermak"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17071",
        "abstract": "The exponential growth of large language models (LLMs) like ChatGPT has revolutionized artificial intelligence, offering unprecedented capabilities in natural language processing. However, the extensive computational resources required for training these models have significant environmental implications, including high carbon emissions, energy consumption, and water usage. This research presents a novel approach to LLM pruning, focusing on the systematic evaluation of individual weight importance throughout the training process. By monitoring parameter evolution over time, we propose a method that effectively reduces model size without compromising performance. Extensive experiments with both a scaled-down LLM and a large multimodal model reveal that moderate pruning enhances efficiency and reduces loss, while excessive pruning drastically deteriorates model performance. These findings highlight the critical need for optimized AI models to ensure sustainable development, balancing technological advancement with environmental responsibility.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "272",
        "title": "VR-Pipe: Streamlining Hardware Graphics Pipeline for Volume Rendering",
        "author": [
            "Junseo Lee",
            "Jaisung Kim",
            "Junyong Park",
            "Jaewoong Sim"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17078",
        "abstract": "Graphics rendering that builds on machine learning and radiance fields is gaining significant attention due to its outstanding quality and speed in generating photorealistic images from novel viewpoints. However, prior work has primarily focused on evaluating its performance through software-based rendering on programmable shader cores, leaving its performance when exploiting fixed-function graphics units largely unexplored.\nIn this paper, we investigate the performance implications of performing radiance field rendering on the hardware graphics pipeline. In doing so, we implement the state-of-the-art radiance field method, 3D Gaussian splatting, using graphics APIs and evaluate it across synthetic and real-world scenes on today's graphics hardware. Based on our analysis, we present VR-Pipe, which seamlessly integrates two innovations into graphics hardware to streamline the hardware pipeline for volume rendering, such as radiance field methods. First, we introduce native hardware support for early termination by repurposing existing special-purpose hardware in modern GPUs. Second, we propose multi-granular tile binning with quad merging, which opportunistically blends fragments in shader cores before passing them to fixed-function blending units. Our evaluation shows that VR-Pipe greatly improves rendering performance, achieving up to a 2.78x speedup over the conventional graphics pipeline with negligible hardware overhead.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "273",
        "title": "Automatically Evaluating the Paper Reviewing Capability of Large Language Models",
        "author": [
            "Hyungyu Shin",
            "Jingyu Tang",
            "Yoonjoo Lee",
            "Nayoung Kim",
            "Hyunseung Lim",
            "Ji Yong Cho",
            "Hwajung Hong",
            "Moontae Lee",
            "Juho Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17086",
        "abstract": "Peer review is essential for scientific progress, but it faces challenges such as reviewer shortages and growing workloads. Although Large Language Models (LLMs) show potential for providing assistance, research has reported significant limitations in the reviews they generate. While the insights are valuable, conducting the analysis is challenging due to the considerable time and effort required, especially given the rapid pace of LLM developments. To address the challenge, we developed an automatic evaluation pipeline to assess the LLMs' paper review capability by comparing them with expert-generated reviews. By constructing a dataset consisting of 676 OpenReview papers, we examined the agreement between LLMs and experts in their strength and weakness identifications. The results showed that LLMs lack balanced perspectives, significantly overlook novelty assessment when criticizing, and produce poor acceptance decisions. Our automated pipeline enables a scalable evaluation of LLMs' paper review capability over time.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "274",
        "title": "WildFrame: Comparing Framing in Humans and LLMs on Naturally Occurring Texts",
        "author": [
            "Gili Lior",
            "Liron Nacchace",
            "Gabriel Stanovsky"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17091",
        "abstract": "Humans are influenced by how information is presented, a phenomenon known as the framing effect. Previous work has shown that LLMs may also be susceptible to framing but has done so on synthetic data and did not compare to human behavior. We introduce WildFrame, a dataset for evaluating LLM responses to positive and negative framing, in naturally-occurring sentences, and compare humans on the same data. WildFrame consists of 1,000 texts, first selecting real-world statements with clear sentiment, then reframing them in either positive or negative light, and lastly, collecting human sentiment annotations. By evaluating eight state-of-the-art LLMs on WildFrame, we find that all models exhibit framing effects similar to humans ($r\\geq0.57$), with both humans and models being more influenced by positive rather than negative reframing. Our findings benefit model developers, who can either harness framing or mitigate its effects, depending on the downstream application.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "275",
        "title": "Enhancing Image Matting in Real-World Scenes with Mask-Guided Iterative Refinement",
        "author": [
            "Rui Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17093",
        "abstract": "Real-world image matting is essential for applications in content creation and augmented reality. However, it remains challenging due to the complex nature of scenes and the scarcity of high-quality datasets. To address these limitations, we introduce Mask2Alpha, an iterative refinement framework designed to enhance semantic comprehension, instance awareness, and fine-detail recovery in image matting. Our framework leverages self-supervised Vision Transformer features as semantic priors, strengthening contextual understanding in complex scenarios. To further improve instance differentiation, we implement a mask-guided feature selection module, enabling precise targeting of objects in multi-instance settings. Additionally, a sparse convolution-based optimization scheme allows Mask2Alpha to recover high-resolution details through progressive refinement,from low-resolution semantic passes to high-resolution sparse reconstructions. Benchmarking across various real-world datasets, Mask2Alpha consistently achieves state-of-the-art results, showcasing its effectiveness in accurate and efficient image matting.",
        "tags": [
            "Matting",
            "Transformer"
        ]
    },
    {
        "id": "276",
        "title": "Improved Diffusion-based Generative Model with Better Adversarial Robustness",
        "author": [
            "Zekun Wang",
            "Mingyang Yi",
            "Shuchen Xue",
            "Zhenguo Li",
            "Ming Liu",
            "Bing Qin",
            "Zhi-Ming Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17099",
        "abstract": "Diffusion Probabilistic Models (DPMs) have achieved significant success in generative tasks. However, their training and sampling processes suffer from the issue of distribution mismatch. During the denoising process, the input data distributions differ between the training and inference stages, potentially leading to inaccurate data generation. To obviate this, we analyze the training objective of DPMs and theoretically demonstrate that this mismatch can be alleviated through Distributionally Robust Optimization (DRO), which is equivalent to performing robustness-driven Adversarial Training (AT) on DPMs. Furthermore, for the recently proposed Consistency Model (CM), which distills the inference process of the DPM, we prove that its training objective also encounters the mismatch issue. Fortunately, this issue can be mitigated by AT as well. Based on these insights, we propose to conduct efficient AT on both DPM and CM. Finally, extensive empirical studies validate the effectiveness of AT in diffusion-based models. The code is available at https://github.com/kugwzk/AT_Diff.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "277",
        "title": "Generative Models in Decision Making: A Survey",
        "author": [
            "Yinchuan Li",
            "Xinyu Shao",
            "Jianping Zhang",
            "Haozhi Wang",
            "Leo Maxime Brunswic",
            "Kaiwen Zhou",
            "Jiqian Dong",
            "Kaiyang Guo",
            "Xiu Li",
            "Zhitang Chen",
            "Jun Wang",
            "Jianye Hao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17100",
        "abstract": "In recent years, the exceptional performance of generative models in generative tasks has sparked significant interest in their integration into decision-making processes. Due to their ability to handle complex data distributions and their strong model capacity, generative models can be effectively incorporated into decision-making systems by generating trajectories that guide agents toward high-reward state-action regions or intermediate sub-goals. This paper presents a comprehensive review of the application of generative models in decision-making tasks. We classify seven fundamental types of generative models: energy-based models, generative adversarial networks, variational autoencoders, normalizing flows, diffusion models, generative flow networks, and autoregressive models. Regarding their applications, we categorize their functions into three main roles: controllers, modelers and optimizers, and discuss how each role contributes to decision-making. Furthermore, we examine the deployment of these models across five critical real-world decision-making scenarios. Finally, we summarize the strengths and limitations of current approaches and propose three key directions for advancing next-generation generative directive models: high-performance algorithms, large-scale generalized decision-making models, and self-evolving and adaptive models.",
        "tags": [
            "Diffusion",
            "Energy-Based Models",
            "Normalizing Flows"
        ]
    },
    {
        "id": "278",
        "title": "SFLD: Reducing the content bias for AI-generated Image Detection",
        "author": [
            "Seoyeon Gye",
            "Junwon Ko",
            "Hyounguk Shon",
            "Minchan Kwon",
            "Junmo Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17105",
        "abstract": "Identifying AI-generated content is critical for the safe and ethical use of generative AI. Recent research has focused on developing detectors that generalize to unknown generators, with popular methods relying either on high-level features or low-level fingerprints. However, these methods have clear limitations: biased towards unseen content, or vulnerable to common image degradations, such as JPEG compression. To address these issues, we propose a novel approach, SFLD, which incorporates PatchShuffle to integrate high-level semantic and low-level textural information. SFLD applies PatchShuffle at multiple levels, improving robustness and generalization across various generative models. Additionally, current benchmarks face challenges such as low image quality, insufficient content preservation, and limited class diversity. In response, we introduce TwinSynths, a new benchmark generation methodology that constructs visually near-identical pairs of real and synthetic images to ensure high quality and content preservation. Our extensive experiments and analysis show that SFLD outperforms existing methods on detecting a wide variety of fake images sourced from GANs, diffusion models, and TwinSynths, demonstrating the state-of-the-art performance and generalization capabilities to novel generative models.",
        "tags": [
            "Detection",
            "Diffusion"
        ]
    },
    {
        "id": "279",
        "title": "Adversarial Training for Defense Against Label Poisoning Attacks",
        "author": [
            "Melis Ilayda Bal",
            "Volkan Cevher",
            "Michael Muehlebach"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17121",
        "abstract": "As machine learning models grow in complexity and increasingly rely on publicly sourced data, such as the human-annotated labels used in training large language models, they become more vulnerable to label poisoning attacks. These attacks, in which adversaries subtly alter the labels within a training dataset, can severely degrade model performance, posing significant risks in critical applications. In this paper, we propose FLORAL, a novel adversarial training defense strategy based on support vector machines (SVMs) to counter these threats. Utilizing a bilevel optimization framework, we cast the training process as a non-zero-sum Stackelberg game between an attacker, who strategically poisons critical training labels, and the model, which seeks to recover from such attacks. Our approach accommodates various model architectures and employs a projected gradient descent algorithm with kernel SVMs for adversarial training. We provide a theoretical analysis of our algorithm's convergence properties and empirically evaluate FLORAL's effectiveness across diverse classification tasks. Compared to robust baselines and foundation models such as RoBERTa, FLORAL consistently achieves higher robust accuracy under increasing attacker budgets. These results underscore the potential of FLORAL to enhance the resilience of machine learning models against label poisoning threats, thereby ensuring robust classification in adversarial settings.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "280",
        "title": "Strong convergence of the adaptive Milstein method for nonlinear stochastic differential equations with piecewise continuous arguments",
        "author": [
            "Yuhang Zhang",
            "Minghui Song",
            "Jiaqi Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17126",
        "abstract": "In this work, an adaptive time-stepping Milstein method is constructed for stochastic differential equations with piecewise continuous arguments (SDEPCAs), where the drift is one-sided Lipschitz continuous and the diffusion does not impose the commutativity condition. It is widely recognized that explicit Euler or Milstein methods may blow up when the system exhibits superlinear growth, and modifications are needed. Hence we propose an adaptive variant to deal with the case of superlinear growth drift coefficient. To the best of our knowledge, this is the first work to develop a numerical method with variable step sizes for nonlinear SDEPCAs. It is proven that the adaptive Milstein method is strongly convergent in the sense of $L_p, p\\ge 2$, and the convergence rate is optimal, which is consistent with the order of the explicit Milstein scheme with globally Lipschitz coefficients. Finally, several numerical experiments are presented to support the theoretical analysis.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "281",
        "title": "Thus Spake Long-Context Large Language Model",
        "author": [
            "Xiaoran Liu",
            "Ruixiao Li",
            "Mianqiu Huang",
            "Zhigeng Liu",
            "Yuerong Song",
            "Qipeng Guo",
            "Siyang He",
            "Qiqi Wang",
            "Linlin Li",
            "Qun Liu",
            "Yaqian Zhou",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17129",
        "abstract": "Long context is an important topic in Natural Language Processing (NLP), running through the development of NLP architectures, and offers immense opportunities for Large Language Models (LLMs) giving LLMs the lifelong learning potential akin to humans. Unfortunately, the pursuit of a long context is accompanied by numerous obstacles. Nevertheless, long context remains a core competitive advantage for LLMs. In the past two years, the context length of LLMs has achieved a breakthrough extension to millions of tokens. Moreover, the research on long-context LLMs has expanded from length extrapolation to a comprehensive focus on architecture, infrastructure, training, and evaluation technologies.\nInspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy between the journey of extending the context of LLM and the attempts of humans to transcend its mortality. In this survey, We will illustrate how LLM struggles between the tremendous need for a longer context and its equal need to accept the fact that it is ultimately finite. To achieve this, we give a global picture of the lifecycle of long-context LLMs from four perspectives: architecture, infrastructure, training, and evaluation, showcasing the full spectrum of long-context technologies. At the end of this survey, we will present 10 unanswered questions currently faced by long-context LLMs. We hope this survey can serve as a systematic introduction to the research on long-context LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "282",
        "title": "Evaluating the Effectiveness of Large Language Models in Automated News Article Summarization",
        "author": [
            "Lionel Richy Panlap Houamegni",
            "Fatih Gedikli"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17136",
        "abstract": "The automation of news analysis and summarization presents a promising solution to the challenge of processing and analyzing vast amounts of information prevalent in today's information society. Large Language Models (LLMs) have demonstrated the capability to transform vast amounts of textual data into concise and easily comprehensible summaries, offering an effective solution to the problem of information overload and providing users with a quick overview of relevant information. A particularly significant application of this technology lies in supply chain risk analysis. Companies must monitor the news about their suppliers and respond to incidents for several critical reasons, including compliance with laws and regulations, risk management, and maintaining supply chain resilience. This paper develops an automated news summarization system for supply chain risk analysis using LLMs. The proposed solution aggregates news from various sources, summarizes them using LLMs, and presents the condensed information to users in a clear and concise format. This approach enables companies to optimize their information processing and make informed decisions. Our study addresses two main research questions: (1) Are LLMs effective in automating news summarization, particularly in the context of supply chain risk analysis? (2) How effective are various LLMs in terms of readability, duplicate detection, and risk identification in their summarization quality? In this paper, we conducted an offline study using a range of publicly available LLMs at the time and complemented it with a user study focused on the top performing systems of the offline experiments to evaluate their effectiveness further. Our results demonstrate that LLMs, particularly Few-Shot GPT-4o mini, offer significant improvements in summary quality and risk identification.",
        "tags": [
            "Detection",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "283",
        "title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation",
        "author": [
            "Qianhui Zhao",
            "Li Zhang",
            "Fang Liu",
            "Xiaoli Lian",
            "Qiaoyuanhe Meng",
            "Ziqian Jiao",
            "Zetong Zhou",
            "Borui Zhang",
            "Runlin Guo",
            "Jia Li"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17139",
        "abstract": "Code generation is a latency-sensitive task that demands high timeliness, but the autoregressive decoding mechanism of Large Language Models (LLMs) leads to poor inference efficiency. Existing LLM inference acceleration methods mainly focus on standalone functions using only built-in components. Moreover, they treat code like natural language sequences, ignoring its unique syntax and semantic characteristics. As a result, the effectiveness of these approaches in code generation tasks remains limited and fails to align with real-world programming scenarios. To alleviate this issue, we propose CodeSwift, a simple yet highly efficient inference acceleration approach specifically designed for code generation, without comprising the quality of the output. CodeSwift constructs a multi-source datastore, providing access to both general and project-specific knowledge, facilitating the retrieval of high-quality draft sequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval timing, and enhances efficiency through parallel retrieval and a context- and LLM preference-aware cache. Experimental results show that CodeSwift can reach up to 2.53x and 2.54x speedup compared to autoregressive decoding in repository-level and standalone code generation tasks, respectively, outperforming state-of-the-art inference acceleration approaches by up to 88%.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "284",
        "title": "Sentiment analysis of texts from social networks based on machine learning methods for monitoring public sentiment",
        "author": [
            "Arsen Tolebay Nurlanuly"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17143",
        "abstract": "A sentiment analysis system powered by machine learning was created in this study to improve real-time social network public opinion monitoring. For sophisticated sentiment identification, the suggested approach combines cutting-edge transformer-based architectures (DistilBERT, RoBERTa) with traditional machine learning models (Logistic Regression, SVM, Naive Bayes). The system achieved an accuracy of up to 80-85% using transformer models in real-world scenarios after being tested using both deep learning techniques and standard machine learning processes on annotated social media datasets. According to experimental results, deep learning models perform noticeably better than lexicon-based and conventional rule-based classifiers, lowering misclassification rates and enhancing the ability to recognize nuances like sarcasm. According to feature importance analysis, context tokens, sentiment-bearing keywords, and part-of-speech structure are essential for precise categorization. The findings confirm that AI-driven sentiment frameworks can provide a more adaptive and efficient approach to modern sentiment challenges. Despite the system's impressive performance, issues with computing overhead, data quality, and domain-specific terminology still exist. In order to monitor opinions on a broad scale, future research will investigate improving computing performance, extending coverage to various languages, and integrating real-time streaming APIs. The results demonstrate that governments, corporations, and social researchers looking for more in-depth understanding of public mood on digital platforms can find a reliable and adaptable answer in AI-powered sentiment analysis.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "285",
        "title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
        "author": [
            "Canyu Zhao",
            "Mingyu Liu",
            "Huanyi Zheng",
            "Muzhi Zhu",
            "Zhiyue Zhao",
            "Hao Chen",
            "Tong He",
            "Chunhua Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17157",
        "abstract": "Our primary goal here is to create a good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data. To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images. Our exhaustive evaluation metrics demonstrate that DICEPTION effectively tackles multiple perception tasks, achieving performance on par with state-of-the-art models. We achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B pixel-level annotated images). Inspired by Wang et al., DICEPTION formulates the outputs of various perception tasks using color encoding; and we show that the strategy of assigning random colors to different instances is highly effective in both entity segmentation and semantic segmentation. Unifying various perception tasks as conditional image generation enables us to fully leverage pre-trained text-to-image models. Thus, DICEPTION can be efficiently trained at a cost of orders of magnitude lower, compared to conventional models that were trained from scratch. When adapting our model to other tasks, it only requires fine-tuning on as few as 50 images and 1% of its parameters. DICEPTION provides valuable insights and a more promising solution for visual generalist models.",
        "tags": [
            "Diffusion",
            "SAM",
            "Segmentation",
            "Text-to-Image",
            "ViT"
        ]
    },
    {
        "id": "286",
        "title": "Parameter Efficient Merging for Multimodal Large Language Models with Complementary Parameter Adaptation",
        "author": [
            "Fanhu Zeng",
            "Haiyang Guo",
            "Fei Zhu",
            "Li Shen",
            "Hao Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17159",
        "abstract": "Fine-tuning pre-trained models with custom data leads to numerous expert models on specific tasks. Merging models into one universal model to empower multi-task ability refraining from data leakage has gained popularity. With the expansion in data and model size, parameter efficient tuning becomes the common practice for obtaining task-specific models efficiently. However, we observe that existing methods designed for full fine-tuning merging fail under efficient tuning. To address the issues, we analyze from low-rank decomposition and reveal that maintaining direction and compensating for gap between singular values are crucial for efficient model merging. Consequently, we propose CoPA-Merging, a training-free parameter efficient merging method with complementary parameter adaptation. Specifically, we (1) prune parameters and construct scaling coefficients from inter-parameter relation to compensate for performance drop from task interference and (2) perform cross-task normalization to enhance unseen task generalization. We establish a benchmark consisting of diverse multimodal tasks, on which we conduct experiments to certificate the outstanding performance and generalizability of our method. Additional study and extensive analyses further showcase the effectiveness.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "287",
        "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation",
        "author": [
            "MarÃ­a Andrea Cruz BlandÃ³n",
            "Jayasimha Talur",
            "Bruno Charron",
            "Dong Liu",
            "Saab Mansour",
            "Marcello Federico"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17163",
        "abstract": "Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience.\nIn this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs. We release our benchmark to support the community developing accurate evaluation methods for multilingual RAG systems.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "288",
        "title": "JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for Legal Reasoning",
        "author": [
            "Huanghai Liu",
            "Quzhe Huang",
            "Qingjing Chen",
            "Yiran Hu",
            "Jiayu Ma",
            "Yun Liu",
            "Weixing Shen",
            "Yansong Feng"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17166",
        "abstract": "The Four-Element Theory is a fundamental framework in criminal law, defining the constitution of crime through four dimensions: Subject, Object, Subjective aspect, and Objective aspect. This theory is widely referenced in legal reasoning, and many Large Language Models (LLMs) attempt to incorporate it when handling legal tasks. However, current approaches rely on LLMs' internal knowledge to incorporate this theory, often lacking completeness and representativeness. To address this limitation, we introduce JUREX-4E, an expert-annotated knowledge base covering 155 criminal charges. It is structured through a progressive hierarchical annotation framework that prioritizes legal source validity and employs diverse legal interpretation methods to ensure comprehensiveness and authority. We evaluate JUREX-4E on the Similar Charge Distinction task and apply it to Legal Case Retrieval, demonstrating its effectiveness in improving LLM performance. Experimental results validate the high quality of JUREX-4E and its substantial impact on downstream legal tasks, underscoring its potential for advancing legal AI applications. Code: https://github.com/THUlawtech/JUREX",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "289",
        "title": "Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without Easily Identifiable Unrelated Padding)",
        "author": [
            "Damien Sileo"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17169",
        "abstract": "Large language models demonstrate promising long context processing capabilities, with recent models touting context windows close to one million tokens. However, the evaluations supporting these claims often involve simple retrieval tasks or synthetic tasks padded with irrelevant text, which the models may easily detect and discard. In this work, we generate lengthy simplified English text with first-order logic representations spanning up to 2048 clauses (around 25k GPT-4 tokens). We formulate an evaluation task with evidence retrieval for contradiction detection. The long, homogeneous text is filled with distractors that are both hard to distinguish from relevant evidences and provably not interfering with them. Our evaluation of evidence retrieval shows that the effective context window is much smaller with realistic distractors, already crumbling at 128 clauses.",
        "tags": [
            "Detection",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "290",
        "title": "Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch",
        "author": [
            "Xueru Wen",
            "Jie Lou",
            "Zichao Li",
            "Yaojie Lu",
            "Xing Yu",
            "Yuqiu Ji",
            "Guohai Xu",
            "Hongyu Lin",
            "Ben He",
            "Xianpei Han",
            "Le Sun",
            "Debing Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17173",
        "abstract": "Reward models (RMs) are crucial for aligning large language models (LLMs) with human preferences. However, most RM research is centered on English and relies heavily on synthetic resources, which leads to limited and less reliable datasets and benchmarks for Chinese. To address this gap, we introduce CheemsBench, a fully human-annotated RM evaluation benchmark within Chinese contexts, and CheemsPreference, a large-scale and diverse preference dataset annotated through human-machine collaboration to support Chinese RM training. We systematically evaluate open-source discriminative and generative RMs on CheemsBench and observe significant limitations in their ability to capture human preferences in Chinese scenarios. Additionally, based on CheemsPreference, we construct an RM that achieves state-of-the-art performance on CheemsBench, demonstrating the necessity of human supervision in RM training. Our findings reveal that scaled AI-generated data struggles to fully capture human preferences, emphasizing the importance of high-quality human supervision in RM development.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "291",
        "title": "Measuring Data Diversity for Instruction Tuning: A Systematic Analysis and A Reliable Metric",
        "author": [
            "Yuming Yang",
            "Yang Nan",
            "Junjie Ye",
            "Shihan Dou",
            "Xiao Wang",
            "Shuo Li",
            "Huijie Lv",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17184",
        "abstract": "Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by assessing their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information distribution in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level \"novelty.\" Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "292",
        "title": "Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks",
        "author": [
            "Andrei Chernov"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17187",
        "abstract": "Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers have gained significant attention. Currently, state-of-the-art LLMs utilize this architecture. There is a substantial amount of research on how to train such models and how to select hyperparameters for this architecture. However, there is a lack of studies focusing on post-evaluation analysis of MoE layer properties. In this paper, we take a first step toward closing this gap by evaluating expert contributions on the quiz-based MMLU benchmark. We show that most experts were never activated during inference on this benchmark. Additionally, the output distribution of gating networks is much closer to uniform than sparse. Finally, we demonstrate that the average performance of some experts within the same layer varies significantly.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "293",
        "title": "Disentangling Visual Transformers: Patch-level Interpretability for Image Classification",
        "author": [
            "Guillaume Jeanneret",
            "LoÃ¯c Simon",
            "FrÃ©dÃ©ric Jurie"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17196",
        "abstract": "Visual transformers have achieved remarkable performance in image classification tasks, but this performance gain has come at the cost of interpretability. One of the main obstacles to the interpretation of transformers is the self-attention mechanism, which mixes visual information across the whole image in a complex way. In this paper, we propose Hindered Transformer (HiT), a novel interpretable by design architecture inspired by visual transformers. Our proposed architecture rethinks the design of transformers to better disentangle patch influences at the classification stage. Ultimately, HiT can be interpreted as a linear combination of patch-level information. We show that the advantages of our approach in terms of explicability come with a reasonable trade-off in performance, making it an attractive alternative for applications where interpretability is paramount.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "294",
        "title": "Dimitra: Audio-driven Diffusion model for Expressive Talking Head Generation",
        "author": [
            "Baptiste Chopin",
            "Tashvik Dhamija",
            "Pranav Balaji",
            "Yaohui Wang",
            "Antitza Dantcheva"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17198",
        "abstract": "We propose Dimitra, a novel framework for audio-driven talking head generation, streamlined to learn lip motion, facial expression, as well as head pose motion. Specifically, we train a conditional Motion Diffusion Transformer (cMDT) by modeling facial motion sequences with 3D representation. We condition the cMDT with only two input signals, an audio-sequence, as well as a reference facial image. By extracting additional features directly from audio, Dimitra is able to increase quality and realism of generated videos. In particular, phoneme sequences contribute to the realism of lip motion, whereas text transcript to facial expression and head pose realism. Quantitative and qualitative experiments on two widely employed datasets, VoxCeleb2 and HDTF, showcase that Dimitra is able to outperform existing approaches for generating realistic talking heads imparting lip motion, facial expression, and head pose.",
        "tags": [
            "3D",
            "Diffusion",
            "Diffusion Transformer",
            "Talking Head",
            "Transformer"
        ]
    },
    {
        "id": "295",
        "title": "Order Matters: Investigate the Position Bias in Multi-constraint Instruction Following",
        "author": [
            "Jie Zeng",
            "Qianyu He",
            "Qingyu Ren",
            "Jiaqing Liang",
            "Yanghua Xiao",
            "Weikang Zhou",
            "Zeye Sun",
            "Fei Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17204",
        "abstract": "Real-world instructions with multiple constraints pose a significant challenge to existing large language models (LLMs). An observation is that the LLMs exhibit dramatic performance fluctuation when disturbing the order of the incorporated constraints. Yet, none of the existing works has systematically investigated this position bias problem in the field of multi-constraint instruction following. To bridge this gap, we design a probing task where we quantitatively measure the difficulty distribution of the constraints by a novel Difficulty Distribution Index (CDDI). Through the experimental results, we find that LLMs are more performant when presented with the constraints in a ``hard-to-easy'' order. This preference can be generalized to LLMs with different architecture or different sizes of parameters. Additionally, we conduct an explanation study, providing an intuitive insight into the correlation between the LLM's attention and constraint orders. Our code and dataset are publicly available at https://github.com/meowpass/PBIF.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "296",
        "title": "Neural Attention: A Novel Mechanism for Enhanced Expressive Power in Transformer Models",
        "author": [
            "Andrew DiGiugno",
            "Ausif Mahmood"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17206",
        "abstract": "Transformer models typically calculate attention matrices using dot products, which have limitations when capturing nonlinear relationships between embedding vectors. We propose Neural Attention, a technique that replaces dot products with feed-forward networks, enabling a more expressive representation of relationships between tokens. This approach modifies only the attention matrix calculation while preserving the matrix dimensions, making it easily adaptable to existing transformer-based architectures. We provide a detailed mathematical justification for why Neural Attention increases representational capacity and conduct controlled experiments to validate this claim. When comparing Neural Attention and Dot-Product Attention, NLP experiments on WikiText-103 show a reduction in perplexity of over 5 percent. Similarly, experiments on CIFAR-10 and CIFAR-100 show comparable improvements for image classification tasks. While Neural Attention introduces higher computational demands, we develop techniques to mitigate these challenges, ensuring practical usability without sacrificing the increased expressivity it provides. This work establishes Neural Attention as an effective means of enhancing the predictive capabilities of transformer models across a variety of applications.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "297",
        "title": "CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought",
        "author": [
            "Boxuan Zhang",
            "Ruqi Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17214",
        "abstract": "Large language models (LLMs) excel in many tasks but struggle to accurately quantify uncertainty in their generated responses. This limitation makes it challenging to detect misinformation and ensure reliable decision-making. Existing uncertainty quantification (UQ) methods for LLMs are primarily prompt-wise rather than response-wise, often requiring multiple response samples, which incurs high computational costs. Moreover, LLMs have been shown to be overconfident, particularly when using reasoning steps to derive their answers. In this work, we propose CoT-UQ, a response-wise UQ framework that integrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT) into the UQ process. CoT-UQ captures critical information during inference by extracting keywords from each reasoning step and assessing their importance to the final answer. This key reasoning information is then aggregated to produce a final uncertainty estimate. We conduct extensive experiments based on LLaMA Family with model sizes varying from 8B to 13B across logical and mathematical reasoning tasks. Experimental results demonstrate that CoT-UQ significantly outperforms existing UQ methods, achieving an average improvement of 5.9% AUROC compared to current UQ methods. The code is available at: https://github.com/ZBox1005/CoT-UQ.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "298",
        "title": "Making LLMs Reason? The Intermediate Language Problem in Neurosymbolic Approaches",
        "author": [
            "Alexander Beiser",
            "David Penz"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17216",
        "abstract": "Logical reasoning tasks manifest themselves as a challenge to Large Language Models (LLMs). Neurosymbolic approaches use LLMs to translate logical reasoning problems formulated in natural language into a formal intermediate language. Subsequently, the usage of symbolic reasoners yields reliable solving thereof. However, LLMs often fail in translation due to poorly chosen intermediate languages.\nWe introduce the intermediate language problem, which is the problem of choosing a suitable formal language representation for neurosymbolic approaches. Theoretically, we argue that its origins lie in the inability of LLMs to distinguish syntax from semantics and the relative independence of the problem from its representation. We showcase its existence experimentally by contrasting two intermediate languages, Answer Set Programming and the Python Knowledge Engine. In addition, we demonstrate the effects of varying degrees of supplementary context information. Our results show a maximum difference in overall-accuracy of 53.20% and 49.26% in execution-accuracy. When using the GPT4o-mini LLM we beat the state-of-the-art in overall-accuracy on the ProntoQA dataset by 21.20% and by 50.50% on the ProofWriter dataset.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "299",
        "title": "MegaLoc: One Retrieval to Place Them All",
        "author": [
            "Gabriele Berton",
            "Carlo Masone"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17237",
        "abstract": "Retrieving images from the same location as a given query is an important component of multiple computer vision tasks, like Visual Place Recognition, Landmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However, existing solutions are built to specifically work for one of these tasks, and are known to fail when the requirements slightly change or when they meet out-of-distribution data. In this paper we combine a variety of existing methods, training techniques, and datasets to train a retrieval model, called MegaLoc, that is performant on multiple tasks. We find that MegaLoc (1) achieves state of the art on a large number of Visual Place Recognition datasets, (2) impressive results on common Landmark Retrieval datasets, and (3) sets a new state of the art for Visual Localization on the LaMAR datasets, where we only changed the retrieval method to the existing localization pipeline. The code for MegaLoc is available at https://github.com/gmberton/MegaLoc",
        "tags": [
            "3D",
            "SLAM"
        ]
    },
    {
        "id": "300",
        "title": "VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing",
        "author": [
            "Xiangpeng Yang",
            "Linchao Zhu",
            "Hehe Fan",
            "Yi Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17258",
        "abstract": "Recent advancements in diffusion models have significantly improved video generation and editing capabilities. However, multi-grained video editing, which encompasses class-level, instance-level, and part-level modifications, remains a formidable challenge. The major difficulties in multi-grained editing include semantic misalignment of text-to-region control and feature coupling within the diffusion model. To address these difficulties, we present VideoGrain, a zero-shot approach that modulates space-time (cross- and self-) attention mechanisms to achieve fine-grained control over video content. We enhance text-to-region control by amplifying each local prompt's attention to its corresponding spatial-disentangled region while minimizing interactions with irrelevant areas in cross-attention. Additionally, we improve feature separation by increasing intra-region awareness and reducing inter-region interference in self-attention. Extensive experiments demonstrate our method achieves state-of-the-art performance in real-world scenarios. Our code, data, and demos are available at https://knightyxp.github.io/VideoGrain_project_page/",
        "tags": [
            "Diffusion",
            "Video Editing",
            "Video Generation"
        ]
    },
    {
        "id": "301",
        "title": "Detecting Benchmark Contamination Through Watermarking",
        "author": [
            "Tom Sander",
            "Pierre Fernandez",
            "Saeed Mahloujifar",
            "Alain Durmus",
            "Chuan Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17259",
        "abstract": "Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect ``radioactivity'', \\ie traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g. $p$-val $=10^{-3}$ for +5$\\%$ on ARC-Easy.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "302",
        "title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective",
        "author": [
            "Chengyin Xu",
            "Kaiyuan Chen",
            "Xiao Li",
            "Ke Shen",
            "Chenggang Li"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17262",
        "abstract": "The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) the \"emergence phenomenon\", wherein downstream performance metrics become meaningful only after extensive training, which limits the ability to use smaller models for prediction; (2) Uneven task difficulty distributions and the absence of consistent scaling laws, resulting in substantial metric variability. Existing performance prediction methods suffer from limited accuracy and reliability, thereby impeding the assessment of potential LLM capabilities. To address these challenges, we propose a Clustering-On-Difficulty (COD) downstream performance prediction framework. COD first constructs a predictable support subset by clustering tasks based on difficulty features, strategically excluding non-emergent and non-scalable clusters. The scores on the selected subset serve as effective intermediate predictors of downstream performance on the full evaluation set. With theoretical support, we derive a mapping function that transforms performance metrics from the predictable subset to the full evaluation set, thereby ensuring accurate extrapolation of LLM downstream performance. The proposed method has been applied to predict performance scaling for a 70B LLM, providing actionable insights for training resource allocation and assisting in monitoring the training process. Notably, COD achieves remarkable predictive accuracy on the 70B LLM by leveraging an ensemble of small models, demonstrating an absolute mean deviation of 1.36% across eight important LLM evaluation benchmarks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "303",
        "title": "MonoTODia: Translating Monologue Requests to Task-Oriented Dialogues",
        "author": [
            "Sebastian Steindl",
            "Ulrich SchÃ¤fer",
            "Bernd Ludwig"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17268",
        "abstract": "Data scarcity is one of the main problems when it comes to real-world applications of transformer-based models. This is especially evident for task-oriented dialogue (TOD) systems, which require specialized datasets, that are usually not readily available. This can hinder companies from adding TOD systems to their services. This study therefore investigates a novel approach to sourcing annotated dialogues from existing German monologue material. Focusing on a real-world example, we investigate whether these monologues can be transformed into dialogue formats suitable for training TOD systems. We show the approach with the concrete example of a company specializing in travel bookings via e-mail. We fine-tune state-of-the-art Large Language Models for the task of rewriting e-mails as dialogues and annotating them. To ensure the quality and validity of the generated data, we employ crowd workers to evaluate the dialogues across multiple criteria and to provide gold-standard annotations for the test dataset. We further evaluate the usefulness of the dialogues for training TOD systems. Our evaluation shows that the dialogues and annotations are of high quality and can serve as a valuable starting point for training TOD systems. Finally, we make the annotated dataset publicly available to foster future research.",
        "tags": [
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "304",
        "title": "Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing",
        "author": [
            "Yi-Kai Zhang",
            "De-Chuan Zhan",
            "Han-Jia Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17282",
        "abstract": "Large Language Models (LLMs) have demonstrated human-like instruction-following abilities, particularly those exceeding 100 billion parameters. The combined capability of some smaller, resource-friendly LLMs can address most of the instructions that larger LLMs excel at. In this work, we explore how to route the best-performing LLM for each instruction to achieve better overall performance. We develop a new paradigm, constructing capability instructions with model capability representation, user instruction, and performance inquiry prompts to assess the performance. To learn from capability instructions, we introduce a new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with. Model-SAT uses a model capability encoder that extends its model representation to a lightweight LLM. Our experiments show that Model-SAT understands the performance dimensions of candidate models and provides the probabilities of their capability to handle various instructions. Additionally, during deployment, a new model can quickly infer its aptitude test results across 50 tasks, each with 20 shots. Model-SAT performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios. The code is available at https://github.com/Now-Join-Us/CIT-LLM-Routing",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "305",
        "title": "GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow",
        "author": [
            "Simon Boeder",
            "Fabian Gigengack",
            "Benjamin Risse"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17288",
        "abstract": "Occupancy estimation has become a prominent task in 3D computer vision, particularly within the autonomous driving community. In this paper, we present a novel approach to occupancy estimation, termed GaussianFlowOcc, which is inspired by Gaussian Splatting and replaces traditional dense voxel grids with a sparse 3D Gaussian representation. Our efficient model architecture based on a Gaussian Transformer significantly reduces computational and memory requirements by eliminating the need for expensive 3D convolutions used with inefficient voxel-based representations that predominantly represent empty 3D spaces. GaussianFlowOcc effectively captures scene dynamics by estimating temporal flow for each Gaussian during the overall network training process, offering a straightforward solution to a complex problem that is often neglected by existing methods. Moreover, GaussianFlowOcc is designed for scalability, as it employs weak supervision and does not require costly dense 3D voxel annotations based on additional data (e.g., LiDAR). Through extensive experimentation, we demonstrate that GaussianFlowOcc significantly outperforms all previous methods for weakly supervised occupancy estimation on the nuScenes dataset while featuring an inference speed that is 50 times faster than current SOTA.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Transformer"
        ]
    },
    {
        "id": "306",
        "title": "Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts",
        "author": [
            "Zhenghao Liu",
            "Xingsheng Zhu",
            "Tianshuo Zhou",
            "Xinyi Zhang",
            "Xiaoyuan Yi",
            "Yukun Yan",
            "Yu Gu",
            "Ge Yu",
            "Maosong Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17297",
        "abstract": "This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a benchmark designed to evaluate the effectiveness of Multi-modal Large Language Models (MLLMs) in leveraging knowledge from multi-modal retrieval documents. The benchmark comprises four tasks: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking. All tasks are set in an open-domain setting, requiring RAG models to retrieve query-relevant information from a multi-modal document collection and use it as input context for RAG modeling. To enhance the context utilization capabilities of MLLMs, we also introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an instruction tuning method that optimizes MLLMs within multi-modal contexts. Our experiments show that MM-RAIT improves the performance of RAG systems by enabling them to effectively learn from multi-modal contexts. All data and code are available at https://github.com/NEUIR/M2RAG.",
        "tags": [
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "307",
        "title": "Delta Decompression for MoE-based LLMs Compression",
        "author": [
            "Hao Gu",
            "Wei Li",
            "Lujun Li",
            "Qiyuan Zhu",
            "Mark Lee",
            "Shengjie Sun",
            "Wei Xue",
            "Yike Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17298",
        "abstract": "Mixture-of-Experts (MoE) architectures in large language models (LLMs) achieve exceptional performance, but face prohibitive storage and memory requirements. To address these challenges, we present $D^2$-MoE, a new delta decompression compressor for reducing the parameters of MoE LLMs. Based on observations of expert diversity, we decompose their weights into a shared base weight and unique delta weights. Specifically, our method first merges each expert's weight into the base weight using the Fisher information matrix to capture shared components. Then, we compress delta weights through Singular Value Decomposition (SVD) by exploiting their low-rank properties. Finally, we introduce a semi-dynamical structured pruning strategy for the base weights, combining static and dynamic redundancy analysis to achieve further parameter reduction while maintaining input adaptivity. In this way, our $D^2$-MoE successfully compact MoE LLMs to high compression ratios without additional training. Extensive experiments highlight the superiority of our approach, with over 13% performance gains than other compressors on Mixtral|Phi-3.5|DeepSeek|Qwen2 MoE LLMs at 40$\\sim$60% compression rates. Codes are available in https://github.com/lliai/D2MoE.",
        "tags": [
            "DeepSeek",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "308",
        "title": "Child vs. machine language learning: Can the logical structure of human language unleash LLMs?",
        "author": [
            "Uli Sauerland",
            "Celia Matthaei",
            "Felix Salfner"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17304",
        "abstract": "We argue that human language learning proceeds in a manner that is different in nature from current approaches to training LLMs, predicting a difference in learning biases. We then present evidence from German plural formation by LLMs that confirm our hypothesis that even very powerful implementations produce results that miss aspects of the logic inherent to language that humans have no problem with. We conclude that attention to the different structures of human language and artificial neural networks is likely to be an avenue to improve LLM performance.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "309",
        "title": "AnyTop: Character Animation Diffusion with Any Topology",
        "author": [
            "Inbar Gat",
            "Sigal Raab",
            "Guy Tevet",
            "Yuval Reshef",
            "Amit H. Bermano",
            "Daniel Cohen-Or"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17327",
        "abstract": "Generating motion for arbitrary skeletons is a longstanding challenge in computer graphics, remaining largely unexplored due to the scarcity of diverse datasets and the irregular nature of the data. In this work, we introduce AnyTop, a diffusion model that generates motions for diverse characters with distinct motion dynamics, using only their skeletal structure as input. Our work features a transformer-based denoising network, tailored for arbitrary skeleton learning, integrating topology information into the traditional attention mechanism. Additionally, by incorporating textual joint descriptions into the latent feature representation, AnyTop learns semantic correspondences between joints across diverse skeletons. Our evaluation demonstrates that AnyTop generalizes well, even with as few as three training examples per topology, and can produce motions for unseen skeletons as well. Furthermore, our model's latent space is highly informative, enabling downstream tasks such as joint correspondence, temporal segmentation and motion editing. Our webpage, https://anytop2025.github.io/Anytop-page, includes links to videos and code.",
        "tags": [
            "Diffusion",
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "310",
        "title": "Mutual Reinforcement of LLM Dialogue Synthesis and Summarization Capabilities for Few-Shot Dialogue Summarization",
        "author": [
            "Yen-Ju Lu",
            "Ting-Yao Hu",
            "Hema Swetha Koppula",
            "Hadi Pouransari",
            "Jen-Hao Rick Chang",
            "Yin Xia",
            "Xiang Kong",
            "Qi Zhu",
            "Simon Wang",
            "Oncel Tuzel",
            "Raviteja Vemulapalli"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17328",
        "abstract": "In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs to improve few-shot dialogue summarization task. Unlike prior methods that require external knowledge, we mutually reinforce the LLMÅ dialogue synthesis and summarization capabilities, allowing them to complement each other during training and enhance overall performances. The dialogue synthesis capability is enhanced by directed preference optimization with preference scoring from summarization capability. The summarization capability is enhanced by the additional high quality dialogue-summary paired data produced by the dialogue synthesis capability. By leveraging the proposed MRDS mechanism, we elicit the internal knowledge of LLM in the format of synthetic data, and use it to augment the few-shot real training dataset. Empirical results demonstrate that our method improves dialogue summarization, achieving a 1.5% increase in ROUGE scores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore, our method attains the highest average scores in human evaluations, surpassing both the pre-trained models and the baselines fine-tuned solely for summarization tasks.",
        "tags": [
            "BERT",
            "LLMs"
        ]
    },
    {
        "id": "311",
        "title": "How Scientists Use Large Language Models to Program",
        "author": [
            "Gabrielle O'Brien"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17348",
        "abstract": "Scientists across disciplines write code for critical activities like data collection and generation, statistical modeling, and visualization. As large language models that can generate code have become widely available, scientists may increasingly use these models during research software development. We investigate the characteristics of scientists who are early-adopters of code generating models and conduct interviews with scientists at a public, research-focused university. Through interviews and reviews of user interaction logs, we see that scientists often use code generating models as an information retrieval tool for navigating unfamiliar programming languages and libraries. We present findings about their verification strategies and discuss potential vulnerabilities that may emerge from code generation practices unknowingly influencing the parameters of scientific analyses.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "312",
        "title": "On Relation-Specific Neurons in Large Language Models",
        "author": [
            "Yihong Liu",
            "Runsheng Chen",
            "Lea Hirlimann",
            "Ahmad Dawar Hakimi",
            "Mingyang Wang",
            "Amir Hossein Kargaran",
            "Sascha Rothe",
            "FranÃ§ois Yvon",
            "Hinrich SchÃ¼tze"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17355",
        "abstract": "In large language models (LLMs), certain neurons can store distinct pieces of knowledge learned during pretraining. While knowledge typically appears as a combination of relations and entities, it remains unclear whether some neurons focus on a relation itself -- independent of any entity. We hypothesize such neurons detect a relation in the input text and guide generation involving such a relation. To investigate this, we study the Llama-2 family on a chosen set of relations with a statistics-based method. Our experiments demonstrate the existence of relation-specific neurons. We measure the effect of selectively deactivating candidate neurons specific to relation $r$ on the LLM's ability to handle (1) facts whose relation is $r$ and (2) facts whose relation is a different relation $r' \\neq r$. With respect to their capacity for encoding relation information, we give evidence for the following three properties of relation-specific neurons. $\\textbf{(i) Neuron cumulativity.}$ The neurons for $r$ present a cumulative effect so that deactivating a larger portion of them results in the degradation of more facts in $r$. $\\textbf{(ii) Neuron versatility.}$ Neurons can be shared across multiple closely related as well as less related relations. Some relation neurons transfer across languages. $\\textbf{(iii) Neuron interference.}$ Deactivating neurons specific to one relation can improve LLM generation performance for facts of other relations. We will make our code publicly available at https://github.com/cisnlp/relation-specific-neurons.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "313",
        "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
        "author": [
            "Tianrui Zhu",
            "Shiyi Zhang",
            "Jiawei Shao",
            "Yansong Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17363",
        "abstract": "Background consistency remains a significant challenge in image editing tasks. Despite extensive developments, existing works still face a trade-off between maintaining similarity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, a training-free approach that uses KV cache in DiTs to maintain background consistency, where background tokens are preserved rather than regenerated, eliminating the need for complex mechanisms or expensive training, ultimately generating new content that seamlessly integrates with the background within user-provided regions. We further explore the memory consumption of the KV cache during editing and optimize the space complexity to $O(1)$ using an inversion-free method. Our approach is compatible with any DiT-based generative model without additional training. Experiments demonstrate that KV-Edit significantly outperforms existing approaches in terms of both background and image quality, even surpassing training-based methods. Project webpage is available at https://xilluill.github.io/projectpages/KV-Edit",
        "tags": [
            "DiT",
            "Image Editing"
        ]
    },
    {
        "id": "314",
        "title": "Graph-Guided Scene Reconstruction from Images with 3D Gaussian Splatting",
        "author": [
            "Chong Cheng",
            "Gaochao Song",
            "Yiyang Yao",
            "Qinzheng Zhou",
            "Gangjian Zhang",
            "Hao Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17377",
        "abstract": "This paper investigates an open research challenge of reconstructing high-quality, large 3D open scenes from images. It is observed existing methods have various limitations, such as requiring precise camera poses for input and dense viewpoints for supervision. To perform effective and efficient 3D scene reconstruction, we propose a novel graph-guided 3D scene reconstruction framework, GraphGS. Specifically, given a set of images captured by RGB cameras on a scene, we first design a spatial prior-based scene structure estimation method. This is then used to create a camera graph that includes information about the camera topology. Further, we propose to apply the graph-guided multi-view consistency constraint and adaptive sampling strategy to the 3D Gaussian Splatting optimization process. This greatly alleviates the issue of Gaussian points overfitting to specific sparse viewpoints and expedites the 3D reconstruction process. We demonstrate GraphGS achieves high-fidelity 3D reconstruction from images, which presents state-of-the-art performance through quantitative and qualitative evaluation across multiple datasets. Project Page: https://3dagentworld.github.io/graphgs.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "315",
        "title": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models",
        "author": [
            "Alon Albalak",
            "Duy Phung",
            "Nathan Lile",
            "Rafael Rafailov",
            "Kanishk Gandhi",
            "Louis Castricato",
            "Anikait Singh",
            "Chase Blagden",
            "Violet Xiang",
            "Dakota Mahan",
            "Nick Haber"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17387",
        "abstract": "Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements. However, existing open math datasets either contain a small collection of high-quality, human-written problems or a large corpus of machine-generated problems of uncertain quality, forcing researchers to choose between quality and quantity. In this work, we present Big-Math, a dataset of over 250,000 high-quality math questions with verifiable answers, purposefully made for reinforcement learning (RL). To create Big-Math, we rigorously filter, clean, and curate openly available datasets, extracting questions that satisfy our three desiderata: (1) problems with uniquely verifiable solutions, (2) problems that are open-ended, (3) and problems with a closed-form solution. To ensure the quality of Big-Math, we manually verify each step in our filtering process. Based on the findings from our filtering process, we introduce 47,000 new questions with verified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple choice questions) that have been reformulated as open-ended questions through a systematic reformulation algorithm. Compared to the most commonly used existing open-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order of magnitude larger, while our rigorous filtering ensures that we maintain the questions most suitable for RL. We also provide a rigorous analysis of the dataset, finding that Big-Math contains a high degree of diversity across problem domains, and incorporates a wide range of problem difficulties, enabling a wide range of downstream uses for models of varying capabilities and training requirements. By bridging the gap between data quality and quantity, Big-Math establish a robust foundation for advancing reasoning in LLMs.",
        "tags": [
            "LLMs",
            "RL"
        ]
    },
    {
        "id": "316",
        "title": "Mitigating Bias in RAG: Controlling the Embedder",
        "author": [
            "Taeyoun Kim",
            "Jacob Springer",
            "Aditi Raghunathan",
            "Maarten Sap"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17390",
        "abstract": "In retrieval augmented generation (RAG) systems, each individual component -- the LLM, embedder, and corpus -- could introduce biases in the form of skews towards outputting certain perspectives or identities. In this work, we study the conflict between biases of each component and their relationship to the overall bias of the RAG system, which we call bias conflict. Examining both gender and political biases as case studies, we show that bias conflict can be characterized through a linear relationship among components despite its complexity in 6 different LLMs. Through comprehensive fine-tuning experiments creating 120 differently biased embedders, we demonstrate how to control bias while maintaining utility and reveal the importance of reverse-biasing the embedder to mitigate bias in the overall system. Additionally, we find that LLMs and tasks exhibit varying sensitivities to the embedder bias, a crucial factor to consider for debiasing. Our results underscore that a fair RAG system can be better achieved by carefully controlling the bias of the embedder rather than increasing its fairness.",
        "tags": [
            "LLMs",
            "RAG"
        ]
    },
    {
        "id": "317",
        "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning",
        "author": [
            "Guijin Son",
            "Jiwoo Hong",
            "Hyunwoo Ko",
            "James Thorne"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17407",
        "abstract": "Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing (BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although \"thinking LLMs\" have recently garnered significant attention, we find that their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. Moreover, while BF yields a 20-point improvement on English AIME, it provides only a 1.94-point average gain across other languages-a pattern consistent across the other test-time scaling methods we studied-higlighting that test-time scaling may not generalize as effectively to multilingual tasks. To foster further research, we release MCLM, MR1-1.5B, and evaluation results.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "318",
        "title": "COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of LLMs",
        "author": [
            "Liming Liu",
            "Zhenghao Xu",
            "Zixuan Zhang",
            "Hao Kang",
            "Zichong Li",
            "Chen Liang",
            "Weizhu Chen",
            "Tuo Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17410",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable success across various domains, yet their optimization remains a significant challenge due to the complex and high-dimensional loss landscapes they inhabit. While adaptive optimizers such as AdamW are widely used, they suffer from critical limitations, including an inability to capture interdependencies between coordinates and high memory consumption. Subsequent research, exemplified by SOAP, attempts to better capture coordinate interdependence but incurs greater memory overhead, limiting scalability for massive LLMs. An alternative approach aims to reduce memory consumption through low-dimensional projection, but this leads to substantial approximation errors, resulting in less effective optimization (e.g., in terms of per-token efficiency). In this paper, we propose COSMOS, a novel hybrid optimizer that leverages the varying importance of eigensubspaces in the gradient matrix to achieve memory efficiency without compromising optimization performance. The design of COSMOS is motivated by our empirical insights and practical considerations. Specifically, COSMOS applies SOAP to the leading eigensubspace, which captures the primary optimization dynamics, and MUON to the remaining eigensubspace, which is less critical but computationally expensive to handle with SOAP. This hybrid strategy significantly reduces memory consumption while maintaining robust optimization performance, making it particularly suitable for massive LLMs. Numerical experiments on various datasets and transformer architectures are provided to demonstrate the effectiveness of COSMOS. Our code is available at https://github.com/lliu606/COSMOS.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "319",
        "title": "X-Dancer: Expressive Music to Human Dance Video Generation",
        "author": [
            "Zeyuan Chen",
            "Hongyi Xu",
            "Guoxian Song",
            "You Xie",
            "Chenxu Zhang",
            "Xin Chen",
            "Chao Wang",
            "Di Chang",
            "Linjie Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17414",
        "abstract": "We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. Code and model will be available for research purposes.",
        "tags": [
            "3D",
            "Diffusion",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": "320",
        "title": "Reasoning with Latent Thoughts: On the Power of Looped Transformers",
        "author": [
            "Nikunj Saunshi",
            "Nishanth Dikkala",
            "Zhiyuan Li",
            "Sanjiv Kumar",
            "Sashank J. Reddi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17416",
        "abstract": "Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. In this work, we make a stronger claim -- many reasoning problems require a large depth but not necessarily many parameters. This unlocks a novel application of looped models for reasoning. Firstly, we show that for many synthetic reasoning problems like addition, $p$-hop induction, and math problems, a $k$-layer transformer looped $L$ times nearly matches the performance of a $kL$-layer non-looped model, and is significantly better than a $k$-layer model. This is further corroborated by theoretical results showing that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved effectively using looped models with nearly optimal depth. Perhaps surprisingly, these benefits also translate to practical settings of language modeling -- on many downstream reasoning tasks, a language model with $k$-layers looped $L$ times can be competitive to, if not better than, a $kL$-layer language model. In fact, our empirical analysis reveals an intriguing phenomenon: looped and non-looped models exhibit scaling behavior that depends on their effective depth, akin to the inference-time scaling of chain-of-thought (CoT) reasoning. We further elucidate the connection to CoT reasoning by proving that looped models implicitly generate latent thoughts and can simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we also present an interesting dichotomy between reasoning and memorization, and design a looping-based regularization that is effective on both fronts.",
        "tags": [
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "321",
        "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
        "author": [
            "Zhong-Zhi Li",
            "Duzhen Zhang",
            "Ming-Liang Zhang",
            "Jiaxin Zhang",
            "Zengyan Liu",
            "Yuxuan Yao",
            "Haotian Xu",
            "Junhao Zheng",
            "Pei-Jie Wang",
            "Xiuyi Chen",
            "Yingying Zhang",
            "Fei Yin",
            "Jiahua Dong",
            "Zhijiang Guo",
            "Le Song",
            "Cheng-Lin Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17419",
        "abstract": "Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field.",
        "tags": [
            "DeepSeek",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "322",
        "title": "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence",
        "author": [
            "Tom WollschlÃ¤ger",
            "Jannes Elstner",
            "Simon Geisler",
            "Vincent Cohen-Addad",
            "Stephan GÃ¼nnemann",
            "Johannes Gasteiger"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17420",
        "abstract": "The safety alignment of large language models (LLMs) can be circumvented through adversarially crafted inputs, yet the mechanisms by which these attacks bypass safety barriers remain poorly understood. Prior work suggests that a single refusal direction in the model's activation space determines whether an LLM refuses a request. In this study, we propose a novel gradient-based approach to representation engineering and use it to identify refusal directions. Contrary to prior work, we uncover multiple independent directions and even multi-dimensional concept cones that mediate refusal. Moreover, we show that orthogonality alone does not imply independence under intervention, motivating the notion of representational independence that accounts for both linear and non-linear effects. Using this framework, we identify mechanistically independent refusal directions. We show that refusal mechanisms in LLMs are governed by complex spatial structures and identify functionally independent directions, confirming that multiple distinct mechanisms drive refusal behavior. Our gradient-based approach uncovers these mechanisms and can further serve as a foundation for future work on understanding LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "323",
        "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification",
        "author": [
            "Penghui Yang",
            "Cunxiao Du",
            "Fengzhuo Zhang",
            "Haonan Wang",
            "Tianyu Pang",
            "Chao Du",
            "Bo An"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17421",
        "abstract": "Speculative decoding has become a promising technique to mitigate the high inference latency of autoregressive decoding in Large Language Models (LLMs). Despite its promise, the effective application of speculative decoding in LLMs still confronts three key challenges: the increasing memory demands of the draft model, the distribution shift between the short-training corpora and long-context inference, and inefficiencies in attention implementation. In this work, we enhance the performance of speculative decoding in long-context settings by addressing these challenges. First, we propose a memory-efficient draft model with a constant-sized Key-Value (KV) cache. Second, we introduce novel position indices for short-training data, enabling seamless adaptation from short-context training to long-context inference. Finally, we present an innovative attention aggregation method that combines fast implementations for prefix computation with standard attention for tree mask handling, effectively resolving the latency and memory inefficiencies of tree decoding. Our approach achieves strong results on various long-context tasks, including repository-level code completion, long-context summarization, and o1-like long reasoning tasks, demonstrating significant improvements in latency reduction. The code is available at https://github.com/sail-sg/LongSpec.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "324",
        "title": "MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs",
        "author": [
            "Jiarui Zhang",
            "Mahyar Khayatkhoei",
            "Prateek Chhikara",
            "Filip Ilievski"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17422",
        "abstract": "Multimodal Large Language Models (MLLMs) have experienced rapid progress in visual recognition tasks in recent years. Given their potential integration into many critical applications, it is important to understand the limitations of their visual perception. In this work, we study whether MLLMs can perceive small visual details as effectively as large ones when answering questions about images. We observe that their performance is very sensitive to the size of the visual subject of the question, and further show that this effect is in fact causal by conducting an intervention study. Next, we study the attention patterns of MLLMs when answering visual questions, and intriguingly find that they consistently know where to look, even when they provide the wrong answer. Based on these findings, we then propose training-free visual intervention methods that leverage the internal knowledge of any MLLM itself, in the form of attention and gradient maps, to enhance its perception of small visual details. We evaluate our proposed methods on two widely-used MLLMs and seven visual question answering benchmarks and show that they can significantly improve MLLMs' accuracy without requiring any training. Our results elucidate the risk of applying MLLMs to visual recognition tasks concerning small details and indicate that visual intervention using the model's internal state is a promising direction to mitigate this risk.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "325",
        "title": "S4S: Solving for a Diffusion Model Solver",
        "author": [
            "Eric Frankel",
            "Sitan Chen",
            "Jerry Li",
            "Pang Wei Koh",
            "Lillian J. Ratliff",
            "Sewoong Oh"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17423",
        "abstract": "Diffusion models (DMs) create samples from a data distribution by starting from random noise and iteratively solving a reverse-time ordinary differential equation (ODE). Because each step in the iterative solution requires an expensive neural function evaluation (NFE), there has been significant interest in approximately solving these diffusion ODEs with only a few NFEs without modifying the underlying model. However, in the few NFE regime, we observe that tracking the true ODE evolution is fundamentally impossible using traditional ODE solvers. In this work, we propose a new method that learns a good solver for the DM, which we call Solving for the Solver (S4S). S4S directly optimizes a solver to obtain good generation quality by learning to match the output of a strong teacher solver. We evaluate S4S on six different pre-trained DMs, including pixel-space and latent-space DMs for both conditional and unconditional sampling. In all settings, S4S uniformly improves the sample quality relative to traditional ODE solvers. Moreover, our method is lightweight, data-free, and can be plugged in black-box on top of any discretization schedule or architecture to improve performance. Building on top of this, we also propose S4S-Alt, which optimizes both the solver and the discretization schedule. By exploiting the full design space of DM solvers, with 5 NFEs, we achieve an FID of 3.73 on CIFAR10 and 13.26 on MS-COCO, representing a $1.5\\times$ improvement over previous training-free ODE methods.",
        "tags": [
            "Diffusion",
            "ODE"
        ]
    },
    {
        "id": "326",
        "title": "FACTR: Force-Attending Curriculum Training for Contact-Rich Policy Learning",
        "author": [
            "Jason Jingzhou Liu",
            "Yulong Li",
            "Kenneth Shaw",
            "Tony Tao",
            "Ruslan Salakhutdinov",
            "Deepak Pathak"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17432",
        "abstract": "Many contact-rich tasks humans perform, such as box pickup or rolling dough, rely on force feedback for reliable execution. However, this force information, which is readily available in most robot arms, is not commonly used in teleoperation and policy learning. Consequently, robot behavior is often limited to quasi-static kinematic tasks that do not require intricate force-feedback. In this paper, we first present a low-cost, intuitive, bilateral teleoperation setup that relays external forces of the follower arm back to the teacher arm, facilitating data collection for complex, contact-rich tasks. We then introduce FACTR, a policy learning method that employs a curriculum which corrupts the visual input with decreasing intensity throughout training. The curriculum prevents our transformer-based policy from over-fitting to the visual input and guides the policy to properly attend to the force modality. We demonstrate that by fully utilizing the force information, our method significantly improves generalization to unseen objects by 43\\% compared to baseline approaches without a curriculum. Video results and instructions at https://jasonjzliu.com/factr/",
        "tags": [
            "Robot",
            "Transformer"
        ]
    },
    {
        "id": "327",
        "title": "V-HOP: Visuo-Haptic 6D Object Pose Tracking",
        "author": [
            "Hongyu Li",
            "Mingxi Jia",
            "Tuluhan Akbulut",
            "Yu Xiang",
            "George Konidaris",
            "Srinath Sridhar"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17434",
        "abstract": "Humans naturally integrate vision and haptics for robust object perception during manipulation. The loss of either modality significantly degrades performance. Inspired by this multisensory integration, prior object pose estimation research has attempted to combine visual and haptic/tactile feedback. Although these works demonstrate improvements in controlled environments or synthetic datasets, they often underperform vision-only approaches in real-world settings due to poor generalization across diverse grippers, sensor layouts, or sim-to-real environments. Furthermore, they typically estimate the object pose for each frame independently, resulting in less coherent tracking over sequences in real-world deployments. To address these limitations, we introduce a novel unified haptic representation that effectively handles multiple gripper embodiments. Building on this representation, we introduce a new visuo-haptic transformer-based object pose tracker that seamlessly integrates visual and haptic input. We validate our framework in our dataset and the Feelsight dataset, demonstrating significant performance improvement on challenging sequences. Notably, our method achieves superior generalization and robustness across novel embodiments, objects, and sensor types (both taxel-based and vision-based tactile sensors). In real-world experiments, we demonstrate that our approach outperforms state-of-the-art visual trackers by a large margin. We further show that we can achieve precise manipulation tasks by incorporating our real-time object tracking result into motion plans, underscoring the advantages of visuo-haptic perception. Our model and dataset will be made open source upon acceptance of the paper. Project website: https://lhy.xyz/projects/v-hop/",
        "tags": [
            "Pose Estimation",
            "Transformer"
        ]
    },
    {
        "id": "328",
        "title": "GCC: Generative Color Constancy via Diffusing a Color Checker",
        "author": [
            "Chen-Wei Chang",
            "Cheng-De Fan",
            "Chia-Che Chang",
            "Yi-Chen Lo",
            "Yu-Chee Tseng",
            "Jiun-Long Huang",
            "Yu-Lun Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17435",
        "abstract": "Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities. We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation. Our key innovations include (1) a single-step deterministic inference approach that inpaints color checkers reflecting scene illumination, (2) a Laplacian decomposition technique that preserves checker structure while allowing illumination-dependent color adaptation, and (3) a mask-based data augmentation strategy for handling imprecise color checker annotations. GCC demonstrates superior robustness in cross-camera scenarios, achieving state-of-the-art worst-25% error rates of 5.15Â° and 4.32Â° in bi-directional evaluations. These results highlight our method's stability and generalization capability across different camera characteristics without requiring sensor-specific training, making it a versatile solution for real-world applications.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "329",
        "title": "Towards Hierarchical Rectified Flow",
        "author": [
            "Yichi Zhang",
            "Yici Yan",
            "Alex Schwing",
            "Zhizhen Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17436",
        "abstract": "We formulate a hierarchical rectified flow to model data distributions. It hierarchically couples multiple ordinary differential equations (ODEs) and defines a time-differentiable stochastic process that generates a data distribution from a known source distribution. Each ODE resembles the ODE that is solved in a classic rectified flow, but differs in its domain, i.e., location, velocity, acceleration, etc. Unlike the classic rectified flow formulation, which formulates a single ODE in the location domain and only captures the expected velocity field (sufficient to capture a multi-modal data distribution), the hierarchical rectified flow formulation models the multi-modal random velocity field, acceleration field, etc., in their entirety. This more faithful modeling of the random velocity field enables integration paths to intersect when the underlying ODE is solved during data generation. Intersecting paths in turn lead to integration trajectories that are more straight than those obtained in the classic rectified flow formulation, where integration paths cannot intersect. This leads to modeling of data distributions with fewer neural function evaluations. We empirically verify this on synthetic 1D and 2D data as well as MNIST, CIFAR-10, and ImageNet-32 data. Code is available at: https://riccizz.github.io/HRF/.",
        "tags": [
            "ODE",
            "Rectified Flow"
        ]
    },
    {
        "id": "330",
        "title": "TLOB: A Novel Transformer Model with Dual Attention for Stock Price Trend Prediction with Limit Order Book Data",
        "author": [
            "Leonardo Berti",
            "Gjergji Kasneci"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15757",
        "abstract": "Stock Price Trend Prediction (SPTP) based on Limit Order Book (LOB) data is a fundamental challenge in financial markets. Despite advances in deep learning, existing models fail to generalize across different market conditions and struggle to reliably predict short-term trends. Surprisingly, by adapting a simple MLP-based architecture to LOB, we show that we surpass SoTA performance; thus, challenging the necessity of complex architectures. Unlike past work that shows robustness issues, we propose TLOB, a transformer-based model that uses a dual attention mechanism to capture spatial and temporal dependencies in LOB data. This allows it to adaptively focus on the market microstructure, making it particularly effective for longer-horizon predictions and volatile market conditions. We also introduce a new labeling method that improves on previous ones, removing the horizon bias. To assess TLOB's effectiveness, we evaluate it on the well-known FI-2010 benchmark (F1 of 92.8\\%) and on Tesla (+2.67\\% on F1) and Intel (+14.16\\% on F1). Additionally, we empirically show how stock price predictability has declined over time (-6.68 absolute points in F1), highlighting the growing market efficiencies. Predictability must be considered in relation to transaction costs, so we experimented with defining trends using an average spread, reflecting the primary transaction cost. The resulting performance deterioration underscores the complexity of translating trend classification into profitable trading strategies. We argue that our work provides new insights into the evolving landscape of stock price trend prediction and sets a strong foundation for future advancements in financial AI. We release the code at http://github.com/LeonardoBerti00/TLOB.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "331",
        "title": "Multi-Agent Stock Prediction Systems: Machine Learning Models, Simulations, and Real-Time Trading Strategies",
        "author": [
            "Daksh Dave",
            "Gauransh Sawhney",
            "Vikhyat Chauhan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.15853",
        "abstract": "This paper presents a comprehensive study on stock price prediction, leveragingadvanced machine learning (ML) and deep learning (DL) techniques to improve financial forecasting accuracy. The research evaluates the performance of various recurrent neural network (RNN) architectures, including Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRU), and attention-based models. These models are assessed for their ability to capture complex temporal dependencies inherent in stock market data. Our findings show that attention-based models outperform other architectures, achieving the highest accuracy by capturing both short and long-term dependencies. This study contributes valuable insights into AI-driven financial forecasting, offering practical guidance for developing more accurate and efficient trading systems.",
        "tags": [
            "RNN"
        ]
    },
    {
        "id": "332",
        "title": "Orthogonality Analysis in LoRa Uplink Satellite Communications Affected by Doppler Effect",
        "author": [
            "Jikang Deng",
            "Fatma Benkhelifa",
            "Mohamed-Slim Alouini"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16179",
        "abstract": "This paper provides, for the first time, analytical expressions for the Long-Range (LoRa) waveform and cross-correlation in both continuous and discrete time domains under the Doppler effect in satellite communication. We propose the concept and formulas of the shared visibility window for satellites toward two ground devices. Our analysis covers cross-correlation results with varying spreading factors (SF) for no-Doppler and with-Doppler cases. We find the maximum cross-correlation with different SFs and the mean cross-correlation are immune to the Doppler effect. However, the maximum cross-correlation with the same SFs is only immune to high Doppler shift, with its value fluctuating between 0.6 and 1 under high Doppler rate. We interpret this fluctuation by introducing the relationship between transmission start time and cross-correlation. We provide a parameter analysis for orbit height, ground device distance, and inclination angle. Additionally, we analyze the bit error rate (BER) for LoRa signals and observe worse performance under high Doppler shift or interference with same SF. Increasing the SNR or the SIR improves the BER only when Doppler effect is below a frequency threshold. Notably, under Doppler effect, the performance behaviors of BER no longer align with those of maximum cross-correlation. Finally, our results lead to two recommendations: 1) To mitigate Doppler impact on cross-correlation, we recommend utilizing low SFs, high orbit height, short ground device distance, and the transmission start time with high Doppler shift; 2) To mitigate Doppler impact on BER, we recommend employing low SFs, high bandwidth, and transmission start time with high Doppler rate. These conflicting recommendations regarding transmission start time highlight the necessity of Doppler shift compensation techniques to help operate LoRa in space properly.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "333",
        "title": "Sampling through Algorithmic Diffusion in non-convex Perceptron problems",
        "author": [
            "Elizaveta Demyanenko",
            "Davide Straziota",
            "Carlo Baldassi",
            "Carlo Lucibello"
        ],
        "pdf": "https://arxiv.org/pdf/2502.16292",
        "abstract": "We analyze the problem of sampling from the solution space of simple yet non-convex neural network models by employing a denoising diffusion process known as Algorithmic Stochastic Localization, where the score function is provided by Approximate Message Passing. We introduce a formalism based on the replica method to characterize the process in the infinite-size limit in terms of a few order parameters, and, in particular, we provide criteria for the feasibility of sampling.\nWe show that, in the case of the spherical perceptron problem with negative stability, approximate uniform sampling is achievable across the entire replica symmetric region of the phase diagram. In contrast, for the binary perceptron, uniform sampling via diffusion invariably fails due to the overlap gap property exhibited by the typical set of solutions. We discuss the first steps in defining alternative measures that can be efficiently sampled.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "334",
        "title": "Conditional Diffusion-Flow models for generating 3D cosmic density fields: applications to f(R) cosmologies",
        "author": [
            "Julieth Katherine Riveros",
            "Paola Saavedra",
            "Hector J. Hortua",
            "Jorge Enrique Garcia-Farieta",
            "Ivan Olier"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17087",
        "abstract": "Next-generation galaxy surveys promise unprecedented precision in testing gravity at cosmological scales. However, realising this potential requires accurately modelling the non-linear cosmic web. We address this challenge by exploring conditional generative modelling to create 3D dark matter density fields via score-based (diffusion) and flow-based methods. Our results demonstrate the power of diffusion models to accurately reproduce the matter power spectra and bispectra, even for unseen configurations. They also offer a significant speed-up with slightly reduced accuracy, when flow-based reconstructing the probability distribution function, but they struggle with higher-order statistics. To improve conditional generation, we introduce a novel multi-output model to develop feature representations of the cosmological parameters. Our findings offer a powerful tool for exploring deviations from standard gravity, combining high precision with reduced computational cost, thus paving the way for more comprehensive and efficient cosmological analyses",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "335",
        "title": "Joint multi-band deconvolution for $Euclid$ and $Vera$ $C.$ $Rubin$ images",
        "author": [
            "Utsav Akhaury",
            "Pascale Jablonka",
            "FrÃ©dÃ©ric Courbin",
            "Jean-Luc Starck"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17177",
        "abstract": "With the advent of surveys like $Euclid$ and $Vera$ $C.$ $Rubin$, astrophysicists will have access to both deep, high-resolution images, and multi-band images. However, these two conditions are not simultaneously available in any single dataset. It is therefore vital to devise image deconvolution algorithms that exploit the best of the two worlds and that can jointly analyze datasets spanning a range of resolutions and wavelengths. In this work, we introduce a novel multi-band deconvolution technique aimed at improving the resolution of ground-based astronomical images by leveraging higher-resolution space-based observations. The method capitalizes on the fortunate fact that the $Vera$ $C.$ $Rubin$ $r$-, $i$-, and $z$-bands lie within the $Euclid$ $VIS$ band. The algorithm jointly deconvolves all the data to turn the $r$-, $i$-, and $z$-band $Vera$ $C.$ $Rubin$ images to the resolution of $Euclid$ by enabling us to leverage the correlations between the different bands. We also investigate the performance of deep learning-based denoising with DRUNet to further improve the results. We illustrate the effectiveness of our method in terms of resolution and morphology recovery, flux preservation, and generalization to different noise levels. This approach extends beyond the specific $Euclid$-$Rubin$ combination, offering a versatile solution to improve the resolution of ground-based images in multiple photometric bands by jointly using any space-based images with overlapping filters.",
        "tags": [
            "FLUX"
        ]
    },
    {
        "id": "336",
        "title": "Unraveling the geometry of visual relational reasoning",
        "author": [
            "Jiaqi Shang",
            "Gabriel Kreiman",
            "Haim Sompolinsky"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17382",
        "abstract": "Humans and other animals readily generalize abstract relations, such as recognizing constant in shape or color, whereas neural networks struggle. To investigate how neural networks generalize abstract relations, we introduce SimplifiedRPM, a novel benchmark for systematic evaluation. In parallel, we conduct human experiments to benchmark relational difficulty, enabling direct model-human comparisons. Testing four architectures--ResNet-50, Vision Transformer, Wild Relation Network, and Scattering Compositional Learner (SCL)--we find that SCL best aligns with human behavior and generalizes best. Building on a geometric theory of neural representations, we show representational geometries that predict generalization. Layer-wise analysis reveals distinct relational reasoning strategies across models and suggests a trade-off where unseen rule representations compress into training-shaped subspaces. Guided by our geometric perspective, we propose and evaluate SNRloss, a novel objective balancing representation geometry. Our findings offer geometric insights into how neural networks generalize abstract relations, paving the way for more human-like visual reasoning in AI.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "337",
        "title": "Function-Space Learning Rates",
        "author": [
            "Edward Milsom",
            "Ben Anson",
            "Laurence Aitchison"
        ],
        "pdf": "https://arxiv.org/pdf/2502.17405",
        "abstract": "We consider layerwise function-space learning rates, which measure the magnitude of the change in a neural network's output function in response to an update to a parameter tensor. This contrasts with traditional learning rates, which describe the magnitude of changes in parameter space. We develop efficient methods to measure and set function-space learning rates in arbitrary neural networks, requiring only minimal computational overhead through a few additional backward passes that can be performed at the start of, or periodically during, training. We demonstrate two key applications: (1) analysing the dynamics of standard neural network optimisers in function space, rather than parameter space, and (2) introducing FLeRM (Function-space Learning Rate Matching), a novel approach to hyperparameter transfer across model scales. FLeRM records function-space learning rates while training a small, cheap base model, then automatically adjusts parameter-space layerwise learning rates when training larger models to maintain consistent function-space updates. FLeRM gives hyperparameter transfer across model width, depth, initialisation scale, and LoRA rank in various architectures including MLPs with residual connections and transformers with different layer normalisation schemes.",
        "tags": [
            "LoRA"
        ]
    }
]