[
    {
        "id": "1",
        "title": "Counterexample Guided Program Repair Using Zero-Shot Learning and MaxSAT-based Fault Localization",
        "author": [
            "Pedro Orvalho",
            "MikolÃ¡Å¡ Janota",
            "Vasco Manquinho"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07786",
        "abstract": "Automated Program Repair (APR) for introductory programming assignments (IPAs) is motivated by the large number of student enrollments in programming courses each year. Since providing feedback on IPAs requires substantial time and effort from faculty, personalized feedback often involves suggesting fixes to students' programs. Formal Methods (FM)-based semantic repair approaches, check a program's execution against a test suite or reference solution, are effective but limited. These tools excel at identifying buggy parts but can only fix programs if the correct implementation and the faulty one share the same control flow graph. Conversely, Large Language Models (LLMs) are used for APR but often make extensive instead of minimal rewrites. This leads to more invasive fixes, making it harder for students to learn from their mistakes. In summary, LLMs excel at completing strings, while FM-based fault localization excel at identifying buggy parts of a program. In this paper, we propose a novel approach that combines the strengths of both FM-based fault localization and LLMs, via zero-shot learning, to enhance APR for IPAs. Our method uses MaxSAT-based fault localization to identify buggy parts of a program, then presents the LLM with a program sketch devoid of these buggy statements. This hybrid approach follows a CEGIS loop to iteratively refine the program. We ask the LLM to synthesize the missing parts, which are then checked against a test suite. If the suggested program is incorrect, a counterexample from the test suite is fed back to the LLM. Our experiments show that our counterexample guided approach, using MaxSAT-based bug-free program sketches, significantly improves the repair capabilities of all six evaluated LLMs. This method allows LLMs to repair more programs with smaller fixes, outperforming other configurations and state-of-the-art symbolic program repair tools.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "2",
        "title": "Do AI assistants help students write formal specifications? A study with ChatGPT and the B-Method",
        "author": [
            "Alfredo Capozucca",
            "Daniil Yampolskyi",
            "Alexander Goldberg",
            "Maximiliano CristiÃ¡"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07789",
        "abstract": "This paper investigates the role of AI assistants, specifically OpenAI's ChatGPT, in teaching formal methods (FM) to undergraduate students, using the B-method as a formal specification technique. While existing studies demonstrate the effectiveness of AI in coding tasks, no study reports on its impact on formal specifications. We examine whether ChatGPT provides an advantage when writing B-specifications and analyse student trust in its outputs. Our findings indicate that the AI does not help students to enhance the correctness of their specifications, with low trust correlating to better outcomes. Additionally, we identify a behavioural pattern with which to interact with ChatGPT which may influence the correctness of B-specifications.",
        "tags": [
            "ChatGPT"
        ]
    },
    {
        "id": "3",
        "title": "High Order in Space and Time Schemes Through an Approximate Lax-Wendroff Procedure",
        "author": [
            "Antonio Baeza",
            "Pep Mulet",
            "David ZorÃ­o"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07798",
        "abstract": "This paper deals with the scheme proposed by the authors in ZorÃ­o, Baeza and Mulet (J Sci Comput 71(1):246-273, 2017). This scheme is an alternative to the techniques proposed in Qiu and Shu (SIAM J Sci Comput 24(6):2185-2198, 2003) to obtain high-order accurate schemes using Weighted Essentially Non Oscillatory finite differences and approximating the flux derivatives required by the Cauchy-Kovalevskaya procedure by simple centered finite differences. We analyse how errors in first-order terms near discontinuities propagate through both versions of the Cauchy-Kovalevskaya procedure. We propose a fluctuation control, for which the approximation of the first-order derivative to be used in the Cauchy-Kovalevskaya procedure is obtained from a Weighted Essentially Non Oscillatory (WENO) interpolation of flux derivatives, instead of the usual finite difference of WENO flux reconstructions. The numerical results that we obtain confirm the benefits of this fluctuation control.",
        "tags": [
            "FLUX"
        ]
    },
    {
        "id": "4",
        "title": "Reasoning-as-Logic-Units: Scaling Test-Time Reasoning in Large Language Models Through Logic Unit Alignment",
        "author": [
            "Cheryl Li",
            "Tianyuan Xu",
            "Yiwen Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07803",
        "abstract": "Chain-of-Thought (CoT) prompting has shown promise in enhancing the reasoning capabilities of large language models (LLMs) by generating natural language (NL) rationales that lead to the final answer. However, it struggles with numerical computation, which has somehow led to the development of program-aided techniques. Despite their potential, a persistent challenge remains: inconsistencies between LLM-reported reasoning steps and the logic in generated programs, which we term ``reasoning hallucinations.\" This stems from the inherent ambiguities of NL and the statistical nature of LLMs, which often lack rigorous logical coherence. To address this challenge, we propose a novel test-time scaling framework, Reasoning-as-Logic-Units (RaLU), which constructs a more reliable reasoning path by aligning logical units between the generated program and their corresponding NL descriptions. By decomposing the initially generated program into discrete units using static analysis, RaLU engages in an iterative dialogue with the LLM to judge, refine, and explain each unit. A rewind-and-correct mechanism ensures alignment between code statements and task requirements in each unit, ultimately forming a cohesive reasoning path under the program's logic, from which the model reaches a final solution. Our experiments demonstrate that RaLU significantly outperforms existing baselines in mathematical reasoning (GSM8K, MATH) and algorithmic reasoning (HumanEval+, MBPP+), underscoring its potential to advance LLM reasoning and programming by offering enhanced accuracy and interpretability.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "5",
        "title": "Unpaired Image Dehazing via Kolmogorov-Arnold Transformation of Latent Features",
        "author": [
            "Le-Anh Tran"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07812",
        "abstract": "This paper proposes an innovative framework for Unsupervised Image Dehazing via Kolmogorov-Arnold Transformation, termed UID-KAT. Image dehazing is recognized as a challenging and ill-posed vision task that requires complex transformations and interpretations in the feature space. Recent advancements have introduced Kolmogorov-Arnold Networks (KANs), inspired by the Kolmogorov-Arnold representation theorem, as promising alternatives to Multi-Layer Perceptrons (MLPs) since KANs can leverage their polynomial foundation to more efficiently approximate complex functions while requiring fewer layers than MLPs. Motivated by this potential, this paper explores the use of KANs combined with adversarial training and contrastive learning to model the intricate relationship between hazy and clear images. Adversarial training is employed due to its capacity in producing high-fidelity images, and contrastive learning promotes the model's emphasis on significant features while suppressing the influence of irrelevant information. The proposed UID-KAT framework is trained in an unsupervised setting to take advantage of the abundance of real-world data and address the challenge of preparing paired hazy/clean images. Experimental results show that UID-KAT achieves state-of-the-art dehazing performance across multiple datasets and scenarios, outperforming existing unpaired methods while reducing model complexity. The source code for this work is publicly available at https://github.com/tranleanh/uid-kat.",
        "tags": [
            "Kolmogorov-Arnold Networks"
        ]
    },
    {
        "id": "6",
        "title": "CryptoX : Compositional Reasoning Evaluation of Large Language Models",
        "author": [
            "Jiajun Shi",
            "Chaoren Wei",
            "Liqun Yang",
            "Zekun Moore Wang",
            "Chenghao Yang",
            "Ge Zhang",
            "Stephen Huang",
            "Tao Peng",
            "Jian Yang",
            "Zhoufutu Wen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07813",
        "abstract": "The compositional reasoning capacity has long been regarded as critical to the generalization and intelligence emergence of large language models LLMs. However, despite numerous reasoning-related benchmarks, the compositional reasoning capacity of LLMs is rarely studied or quantified in the existing benchmarks. In this paper, we introduce CryptoX, an evaluation framework that, for the first time, combines existing benchmarks and cryptographic, to quantify the compositional reasoning capacity of LLMs. Building upon CryptoX, we construct CryptoBench, which integrates these principles into several benchmarks for systematic evaluation. We conduct detailed experiments on widely used open-source and closed-source LLMs using CryptoBench, revealing a huge gap between open-source and closed-source LLMs. We further conduct thorough mechanical interpretability experiments to reveal the inner mechanism of LLMs' compositional reasoning, involving subproblem decomposition, subproblem inference, and summarizing subproblem conclusions. Through analysis based on CryptoBench, we highlight the value of independently studying compositional reasoning and emphasize the need to enhance the compositional reasoning capabilities of LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "7",
        "title": "Satellite Observations Guided Diffusion Model for Accurate Meteorological States at Arbitrary Resolution",
        "author": [
            "Siwei Tu",
            "Ben Fei",
            "Weidong Yang",
            "Fenghua Ling",
            "Hao Chen",
            "Zili Liu",
            "Kun Chen",
            "Hang Fan",
            "Wanli Ouyang",
            "Lei Bai"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07814",
        "abstract": "Accurate acquisition of surface meteorological conditions at arbitrary locations holds significant importance for weather forecasting and climate simulation. Due to the fact that meteorological states derived from satellite observations are often provided in the form of low-resolution grid fields, the direct application of spatial interpolation to obtain meteorological states for specific locations often results in significant discrepancies when compared to actual observations. Existing downscaling methods for acquiring meteorological state information at higher resolutions commonly overlook the correlation with satellite observations. To bridge the gap, we propose Satellite-observations Guided Diffusion Model (SGD), a conditional diffusion model pre-trained on ERA5 reanalysis data with satellite observations (GridSat) as conditions, which is employed for sampling downscaled meteorological states through a zero-shot guided sampling strategy and patch-based methods. During the training process, we propose to fuse the information from GridSat satellite observations into ERA5 maps via the attention mechanism, enabling SGD to generate atmospheric states that align more accurately with actual conditions. In the sampling, we employed optimizable convolutional kernels to simulate the upscale process, thereby generating high-resolution ERA5 maps using low-resolution ERA5 maps as well as observations from weather stations as guidance. Moreover, our devised patch-based method promotes SGD to generate meteorological states at arbitrary resolutions. Experiments demonstrate SGD fulfills accurate meteorological states downscaling to 6.25km.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "8",
        "title": "Pre-Trained Video Generative Models as World Simulators",
        "author": [
            "Haoran He",
            "Yang Zhang",
            "Liang Lin",
            "Zhongwen Xu",
            "Ling Pan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07825",
        "abstract": "Video generative models pre-trained on large-scale internet datasets have achieved remarkable success, excelling at producing realistic synthetic videos. However, they often generate clips based on static prompts (e.g., text or images), limiting their ability to model interactive and dynamic scenarios. In this paper, we propose Dynamic World Simulation (DWS), a novel approach to transform pre-trained video generative models into controllable world simulators capable of executing specified action trajectories. To achieve precise alignment between conditioned actions and generated visual changes, we introduce a lightweight, universal action-conditioned module that seamlessly integrates into any existing model. Instead of focusing on complex visual details, we demonstrate that consistent dynamic transition modeling is the key to building powerful world simulators. Building upon this insight, we further introduce a motion-reinforced loss that enhances action controllability by compelling the model to capture dynamic changes more effectively. Experiments demonstrate that DWS can be versatilely applied to both diffusion and autoregressive transformer models, achieving significant improvements in generating action-controllable, dynamically consistent videos across games and robotics domains. Moreover, to facilitate the applications of the learned world simulator in downstream tasks such as model-based reinforcement learning, we propose prioritized imagination to improve sample efficiency, demonstrating competitive performance compared with state-of-the-art methods.",
        "tags": [
            "Diffusion",
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "9",
        "title": "Implicit Language Models are RNNs: Balancing Parallelization and Expressivity",
        "author": [
            "Mark SchÃ¶ne",
            "Babak Rahmani",
            "Heiner Kremer",
            "Fabian Falck",
            "Hitesh Ballani",
            "Jannes Gladrow"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07827",
        "abstract": "State-space models (SSMs) and transformers dominate the language modeling landscape. However, they are constrained to a lower computational complexity than classical recurrent neural networks (RNNs), limiting their expressivity. In contrast, RNNs lack parallelization during training, raising fundamental questions about the trade off between parallelization and expressivity. We propose implicit SSMs, which iterate a transformation until convergence to a fixed point. Theoretically, we show that implicit SSMs implement the non-linear state-transitions of RNNs. Empirically, we find that only approximate fixed-point convergence suffices, enabling the design of a scalable training curriculum that largely retains parallelization, with full convergence required only for a small subset of tokens. Our approach demonstrates superior state-tracking capabilities on regular languages, surpassing transformers and SSMs. We further scale implicit SSMs to natural language reasoning tasks and pretraining of large-scale language models up to 1.3B parameters on 207B tokens - representing, to our knowledge, the largest implicit model trained to date. Notably, our implicit models outperform their explicit counterparts on standard benchmarks.",
        "tags": [
            "SSMs",
            "State Space Models"
        ]
    },
    {
        "id": "10",
        "title": "Captured by Captions: On Memorization and its Mitigation in CLIP Models",
        "author": [
            "Wenhao Wang",
            "Adam Dziedzic",
            "Grace C. Kim",
            "Michael Backes",
            "Franziska Boenisch"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07830",
        "abstract": "Multi-modal models, such as CLIP, have demonstrated strong performance in aligning visual and textual representations, excelling in tasks like image retrieval and zero-shot classification. Despite this success, the mechanisms by which these models utilize training data, particularly the role of memorization, remain unclear. In uni-modal models, both supervised and self-supervised, memorization has been shown to be essential for generalization. However, it is not well understood how these findings would apply to CLIP, which incorporates elements from both supervised learning via captions that provide a supervisory signal similar to labels, and from self-supervised learning via the contrastive objective. To bridge this gap in understanding, we propose a formal definition of memorization in CLIP (CLIPMem) and use it to quantify memorization in CLIP models. Our results indicate that CLIP's memorization behavior falls between the supervised and self-supervised paradigms, with \"mis-captioned\" samples exhibiting highest levels of memorization. Additionally, we find that the text encoder contributes more to memorization than the image encoder, suggesting that mitigation strategies should focus on the text domain. Building on these insights, we propose multiple strategies to reduce memorization while at the same time improving utility--something that had not been shown before for traditional learning paradigms where reducing memorization typically results in utility decrease.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "11",
        "title": "SHARP: Accelerating Language Model Inference by SHaring Adjacent layers with Recovery Parameters",
        "author": [
            "Yiping Wang",
            "Hanxian Huang",
            "Yifang Chen",
            "Jishen Zhao",
            "Simon Shaolei Du",
            "Yuandong Tian"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07832",
        "abstract": "While Large language models (LLMs) have advanced natural language processing tasks, their growing computational and memory demands make deployment on resource-constrained devices like mobile phones increasingly challenging. In this paper, we propose SHARP (SHaring Adjacent Layers with Recovery Parameters), a novel approach to accelerate LLM inference by sharing parameters across adjacent layers, thus reducing memory load overhead, while introducing low-rank recovery parameters to maintain performance. Inspired by observations that consecutive layers have similar outputs, SHARP employs a two-stage recovery process: Single Layer Warmup (SLW), and Supervised Fine-Tuning (SFT). The SLW stage aligns the outputs of the shared layers using L_2 loss, providing a good initialization for the following SFT stage to further restore the model performance. Extensive experiments demonstrate that SHARP can recover the model's perplexity on various in-distribution tasks using no more than 50k fine-tuning data while reducing the number of stored MLP parameters by 38% to 65%. We also conduct several ablation studies of SHARP and show that replacing layers towards the later parts of the model yields better performance retention, and that different recovery parameterizations perform similarly when parameter counts are matched. Furthermore, SHARP saves 42.8% in model storage and reduces the total inference time by 42.2% compared to the original Llama2-7b model on mobile devices. Our results highlight SHARP as an efficient solution for reducing inference costs in deploying LLMs without the need for pretraining-scale resources.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "12",
        "title": "Bridging LLM-Generated Code and Requirements: Reverse Generation technique and SBC Metric for Developer Insights",
        "author": [
            "Ahilan Ayyachamy Nadar Ponnusamy"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07835",
        "abstract": "The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics that align well with human judgment. Traditional token-based metrics such as BLEU and ROUGE, while commonly used in natural language processing, exhibit weak correlations with human assessments in code intelligence and verification tasks. Furthermore, these metrics are primarily research focused and are not designed for seamless integration into the software development lifecycle, limiting their practical utility for developers seeking to improve code quality and security.\nAI-assisted coding has been shown to be more beneficial for senior developers, as they possess the expertise to critically evaluate the generated code for correctness, completeness, and compliance. In contrast, junior developers may struggle to identify hallucinations, missing functionality, or incorrect logic in AI-generated code. To bridge this gap, This paper introduces a novel scoring mechanism called the SBC score, which is based on a reverse generation technique that leverages the natural language generation capabilities of LLMs. Unlike direct code analysis, our approach reconstructs system requirements from AI-generated code and compares them with the original specifications to quantify accuracy. The SBC score combines semantic similarity, BLEU, and completeness analysis, providing actionable insights to developers by highlighting missing features and hallucinations. Our code and datasets are available on GitHub",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "13",
        "title": "RoboBERT: An End-to-end Multimodal Robotic Manipulation Model",
        "author": [
            "Sicheng Wang",
            "Jianhua Shan",
            "Jianwei Zhang",
            "Haozhang Gao",
            "Hailiang Han",
            "Yipeng Chen",
            "Kang Wei",
            "Chengkun Zhang",
            "Kairos Wong",
            "Jie Zhao",
            "Lei Zhao",
            "Bin Fang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07837",
        "abstract": "Embodied intelligence integrates multiple modalities, enabling agents to understand images, language, and actions simultaneously. However, existing models always depend on additional datasets or extensive pre-training to maximize performance improvements, consuming abundant training time and expensive hardware cost. To tackle this issue, we present RoboBERT, a novel end-to-end robotic manipulation model integrated with a unique training strategy. This model utilizes a CNN-based diffusion policy, enhancing and stabilizing the effectiveness of this model by separating training processes for different modalities. It also underscores the importance of data augmentation, verifying various techniques to significantly boost performance. Unlike models that depend on extra data or large foundation models, RoboBERT achieves a highly competitive success rate while using only language-labeled expert demonstrations and maintaining a relatively smaller model size. Specifically, RoboBERT achieves an average length of 4.52 on the CALVIN benchmark for \\(ABCD \\rightarrow D\\) task, setting a new state-of-the-art (SOTA) record. Furthermore, when tested on a real robot, the model demonstrates superior performance, achieving a higher success rate than other methods trained with the same data. We propose that these concepts and methodologies of RoboBERT demonstrate extensive versatility and compatibility, contributing significantly to the development of lightweight multimodal robotic models. The code can be accessed on https://github.com/PeterWangsicheng/RoboBERT",
        "tags": [
            "Diffusion",
            "Robot"
        ]
    },
    {
        "id": "14",
        "title": "NanoVLMs: How small can we go and still make coherent Vision Language Models?",
        "author": [
            "Mukund Agarwalla",
            "Himanshu Kumar",
            "Raj Dandekar",
            "Rajat Dandekar",
            "Sreedath Panat"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07838",
        "abstract": "Vision-Language Models (VLMs), such as GPT-4V and Llama 3.2 vision, have garnered significant research attention for their ability to leverage Large Language Models (LLMs) in multimodal tasks. However, their potential is constrained by inherent challenges, including proprietary restrictions, substantial computational demands, and limited accessibility. Smaller models, such as GIT and BLIP, exhibit marked limitations, often failing to generate coherent and consistent text beyond a few tokens, even with extensive training. This underscores a pivotal inquiry: how small can a VLM be and still produce fluent and consistent text? Drawing inspiration from the exceptional learning process of 3-4 year old children, who rely heavily on visual cues for understanding and communication, we introduce two novel datasets: ShortDesc (featuring concise image descriptions) and LongDesc (containing more detailed image descriptions). These datasets consist of image-text pairs where the text is restricted to the simple vocabulary and syntax typically used by young children, generated with a scaled- down model, GPT-4o. Using these datasets, we demonstrate that it is possible to train VLMs that are significantly smaller, up to 10 times smaller than state of the art(SOTA) small VLMs while maintaining architectural simplicity. To evaluate the outputs, we leverage GPT-4o to grade the text, as if stories written by students, on creativity, meaningfulness, and consistency, assigning scores out of 10. This method addresses limitations of standard benchmarks by accommodating unstructured outputs and providing a multidimensional evaluation of the model capabilities. Our findings contribute to the development of lightweight, accessible multimodal models for resource constrained environments.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "15",
        "title": "TranSplat: Surface Embedding-guided 3D Gaussian Splatting for Transparent Object Manipulation",
        "author": [
            "Jeongyun Kim",
            "Jeongho Noh",
            "Dong-Guw Lee",
            "Ayoung Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07840",
        "abstract": "Transparent object manipulation remains a sig- nificant challenge in robotics due to the difficulty of acquiring accurate and dense depth measurements. Conventional depth sensors often fail with transparent objects, resulting in in- complete or erroneous depth data. Existing depth completion methods struggle with interframe consistency and incorrectly model transparent objects as Lambertian surfaces, leading to poor depth reconstruction. To address these challenges, we propose TranSplat, a surface embedding-guided 3D Gaussian Splatting method tailored for transparent objects. TranSplat uses a latent diffusion model to generate surface embeddings that provide consistent and continuous representations, making it robust to changes in viewpoint and lighting. By integrating these surface embeddings with input RGB images, TranSplat effectively captures the complexities of transparent surfaces, enhancing the splatting of 3D Gaussians and improving depth completion. Evaluations on synthetic and real-world transpar- ent object benchmarks, as well as robot grasping tasks, show that TranSplat achieves accurate and dense depth completion, demonstrating its effectiveness in practical applications. We open-source synthetic dataset and model: https://github. com/jeongyun0609/TranSplat",
        "tags": [
            "3D",
            "Diffusion",
            "Gaussian Splatting",
            "Robot",
            "Robotics"
        ]
    },
    {
        "id": "16",
        "title": "Memory Analysis on the Training Course of DeepSeek Models",
        "author": [
            "Ping Zhang",
            "Lei Su"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07846",
        "abstract": "We present a theoretical analysis of GPU memory consumption during the training of DeepSeek models such as DeepSeek-v2 and DeepSeek-v3. Our primary objective is to clarify the device-level memory requirements associated with various distributed training configurations. Specifically, we examine critical factors influencing memory usage, including micro-batch size, activation recomputation policies, 3D parallelism, and ZeRO optimizations. It is important to emphasize that the training policies discussed in this report are not representative of DeepSeek's official configurations. Instead, they are explored to provide a deeper understanding of memory dynamics in training of large-scale mixture-of-experts model.",
        "tags": [
            "3D",
            "DeepSeek"
        ]
    },
    {
        "id": "17",
        "title": "Understanding Classifier-Free Guidance: High-Dimensional Theory and Non-Linear Generalizations",
        "author": [
            "Krunoslav Lehman Pavasovic",
            "Jakob Verbeek",
            "Giulio Biroli",
            "Marc Mezard"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07849",
        "abstract": "Recent studies have raised concerns about the effectiveness of Classifier-Free Guidance (CFG), indicating that in low-dimensional settings, it can lead to overshooting the target distribution and reducing sample diversity. In this work, we demonstrate that in infinite and sufficiently high-dimensional contexts CFG effectively reproduces the target distribution, revealing a blessing-of-dimensionality result. Additionally, we explore finite-dimensional effects, precisely characterizing overshoot and variance reduction. Based on our analysis, we introduce non-linear generalizations of CFG. Through numerical simulations on Gaussian mixtures and experiments on class-conditional and text-to-image diffusion models, we validate our analysis and show that our non-linear CFG offers improved flexibility and generation quality without additional computation cost.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "18",
        "title": "Mathematical reasoning and the computer",
        "author": [
            "Kevin Buzzard"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07850",
        "abstract": "Computers have already changed the way that humans do mathematics: they enable us to compute efficiently. But will they soon be helping us to reason? And will they one day start reasoning themselves? We give an overview of recent developments in neural networks, computer theorem provers and large language models.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "19",
        "title": "Vision-Language Models for Edge Networks: A Comprehensive Survey",
        "author": [
            "Ahmed Sharshar",
            "Latif U. Khan",
            "Waseem Ullah",
            "Mohsen Guizani"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07855",
        "abstract": "Vision Large Language Models (VLMs) combine visual understanding with natural language processing, enabling tasks like image captioning, visual question answering, and video analysis. While VLMs show impressive capabilities across domains such as autonomous vehicles, smart surveillance, and healthcare, their deployment on resource-constrained edge devices remains challenging due to processing power, memory, and energy limitations. This survey explores recent advancements in optimizing VLMs for edge environments, focusing on model compression techniques, including pruning, quantization, knowledge distillation, and specialized hardware solutions that enhance efficiency. We provide a detailed discussion of efficient training and fine-tuning methods, edge deployment challenges, and privacy considerations. Additionally, we discuss the diverse applications of lightweight VLMs across healthcare, environmental monitoring, and autonomous systems, illustrating their growing impact. By highlighting key design strategies, current challenges, and offering recommendations for future directions, this survey aims to inspire further research into the practical deployment of VLMs, ultimately making advanced AI accessible in resource-limited settings.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "20",
        "title": "MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers",
        "author": [
            "Ao Li",
            "Wei Fang",
            "Hongbo Zhao",
            "Le Lu",
            "Ge Yang",
            "Minfeng Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07856",
        "abstract": "In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation.",
        "tags": [
            "Diffusion",
            "ODE",
            "SDE"
        ]
    },
    {
        "id": "21",
        "title": "BalanceKV: KV Cache Compression through Discrepancy Theory",
        "author": [
            "Insu Han",
            "Michael Kapralov",
            "Ekaterina Kochetkova",
            "Kshiteej Sheth",
            "Amir Zandieh"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07861",
        "abstract": "Large language models (LLMs) have achieved impressive success, but their high memory requirements present challenges for long-context token generation. The memory complexity of long-context LLMs is primarily due to the need to store Key-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache compression method based on geometric sampling process stemming from Banaszczyk's vector balancing theory, which introduces dependencies informed by the geometry of keys and value tokens, and improves precision. BalanceKV offers both theoretically proven and empirically validated performance improvements over existing methods.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "22",
        "title": "TransMLA: Multi-head Latent Attention Is All You Need",
        "author": [
            "Fanxu Meng",
            "Zengwei Yao",
            "Muhan Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07864",
        "abstract": "Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce **TransMLA**, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1.",
        "tags": [
            "DeepSeek",
            "LLMs",
            "LLaMA",
            "Large Language Models",
            "Qwen"
        ]
    },
    {
        "id": "23",
        "title": "Sentiment Analysis Tools in Software Engineering: A Systematic Mapping Study",
        "author": [
            "Martin Obaidi",
            "Lukas Nagel",
            "Alexander Specht",
            "Jil KlÃ¼nder"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07893",
        "abstract": "Software development is a collaborative task. Previous research has shown social aspects within development teams to be highly relevant for the success of software projects. A team's mood has been proven to be particularly important. It is paramount for project managers to be aware of negative moods within their teams, as such awareness enables them to intervene. Sentiment analysis tools offer a way to determine the mood of a team based on textual communication. We aim to help developers or stakeholders in their choice of sentiment analysis tools for their specific purpose. Therefore, we conducted a systematic mapping study (SMS). We present the results of our SMS of sentiment analysis tools developed for or applied in the context of software engineering (SE). Our results summarize insights from 106 papers with respect to (1) the application domain, (2) the purpose, (3) the used data sets, (4) the approaches for developing sentiment analysis tools, (5) the usage of already existing tools, and (6) the difficulties researchers face. We analyzed in more detail which tools and approaches perform how in terms of their performance. According to our results, sentiment analysis is frequently applied to open-source software projects, and most approaches are neural networks or support-vector machines. The best performing approach in our analysis is neural networks and the best tool is BERT. Despite the frequent use of sentiment analysis in SE, there are open issues, e.g. regarding the identification of irony or sarcasm, pointing to future research directions. We conducted an SMS to gain an overview of the current state of sentiment analysis in order to help developers or stakeholders in this matter. Our results include interesting findings e.g. on the used tools and their difficulties. We present several suggestions on how to solve these identified problems.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "24",
        "title": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment",
        "author": [
            "Youhe Jiang",
            "Ran Yan",
            "Binhang Yuan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07903",
        "abstract": "Disaggregating the prefill and decoding phases represents an effective new paradigm for generative inference of large language models (LLM), which eliminates prefill-decoding interference and optimizes resource allocation. However, it is still an open problem about how to deploy the disaggregated inference paradigm across a group of heterogeneous GPUs, which can be an economical alternative to deployment over homogeneous high-performance GPUs. Towards this end, we introduce HexGen-2, a distributed system for efficient and economical LLM serving on heterogeneous GPUs following the disaggregated paradigm. Built on top of HexGen, the core component of HexGen-2 is a scheduling algorithm that formalizes the allocation of disaggregated LLM inference computations and communications over heterogeneous GPUs and network connections as a constraint optimization problem. We leverage the graph partitioning and max-flow algorithms to co-optimize resource allocation, parallel strategies for distinct inference phases, and the efficiency of inter-phase key-value (KV) cache communications. We conduct extensive experiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models in various real-world settings, the results reveal that HexGen-2 delivers up to a 2.0 times and on average a 1.3 times improvement in serving throughput, reduces the average inference latency by 1.5 times compared with state-of-the-art systems given the same price budget, and achieves comparable inference performance with a 30% lower price budget.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "25",
        "title": "Intelligent Legal Assistant: An Interactive Clarification System for Legal Question Answering",
        "author": [
            "Rujing Yao",
            "Yiquan Wu",
            "Tong Zhang",
            "Xuhui Zhang",
            "Yuting Huang",
            "Yang Wu",
            "Jiayin Yang",
            "Changlong Sun",
            "Fang Wang",
            "Xiaozhong Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07904",
        "abstract": "The rise of large language models has opened new avenues for users seeking legal advice. However, users often lack professional legal knowledge, which can lead to questions that omit critical information. This deficiency makes it challenging for traditional legal question-answering systems to accurately identify users' actual needs, often resulting in imprecise or generalized advice. In this work, we develop a legal question-answering system called Intelligent Legal Assistant, which interacts with users to precisely capture their needs. When a user poses a question, the system requests that the user select their geographical location to pinpoint the applicable laws. It then generates clarifying questions and options based on the key information missing from the user's initial question. This allows the user to select and provide the necessary details. Once all necessary information is provided, the system produces an in-depth legal analysis encompassing three aspects: overall conclusion, jurisprudential analysis, and resolution suggestions.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "26",
        "title": "DeepSeek on a Trip: Inducing Targeted Visual Hallucinations via Representation Vulnerabilities",
        "author": [
            "Chashi Mahiul Islam",
            "Samuel Jacob Chacko",
            "Preston Horne",
            "Xiuwen Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07905",
        "abstract": "Multimodal Large Language Models (MLLMs) represent the cutting edge of AI technology, with DeepSeek models emerging as a leading open-source alternative offering competitive performance to closed-source systems. While these models demonstrate remarkable capabilities, their vision-language integration mechanisms introduce specific vulnerabilities. We implement an adapted embedding manipulation attack on DeepSeek Janus that induces targeted visual hallucinations through systematic optimization of image embeddings. Through extensive experimentation across COCO, DALL-E 3, and SVIT datasets, we achieve hallucination rates of up to 98.0% while maintaining high visual fidelity (SSIM > 0.88) of the manipulated images on open-ended questions. Our analysis demonstrates that both 1B and 7B variants of DeepSeek Janus are susceptible to these attacks, with closed-form evaluation showing consistently higher hallucination rates compared to open-ended questioning. We introduce a novel multi-prompt hallucination detection framework using LLaMA-3.1 8B Instruct for robust evaluation. The implications of these findings are particularly concerning given DeepSeek's open-source nature and widespread deployment potential. This research emphasizes the critical need for embedding-level security measures in MLLM deployment pipelines and contributes to the broader discussion of responsible AI implementation.",
        "tags": [
            "DeepSeek",
            "Detection",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "27",
        "title": "Elevating Legal LLM Responses: Harnessing Trainable Logical Structures and Semantic Knowledge with Legal Reasoning",
        "author": [
            "Rujing Yao",
            "Yang Wu",
            "Chenghao Wang",
            "Jingwei Xiong",
            "Fang Wang",
            "Xiaozhong Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07912",
        "abstract": "Large Language Models (LLMs) have achieved impressive results across numerous domains, yet they experience notable deficiencies in legal question-answering tasks. LLMs often generate generalized responses that lack the logical specificity required for expert legal advice and are prone to hallucination, providing answers that appear correct but are unreliable. Retrieval-Augmented Generation (RAG) techniques offer partial solutions to address this challenge, but existing approaches typically focus only on semantic similarity, neglecting the logical structure essential to legal reasoning. In this paper, we propose the Logical-Semantic Integration Model (LSIM), a novel supervised framework that bridges semantic and logical coherence. LSIM comprises three components: reinforcement learning predicts a structured fact-rule chain for each question, a trainable Deep Structured Semantic Model (DSSM) retrieves the most relevant candidate questions by integrating semantic and logical features, and in-context learning generates the final answer using the retrieved content. Our experiments on a real-world legal QA dataset-validated through both automated metrics and human evaluation-demonstrate that LSIM significantly enhances accuracy and reliability compared to existing methods.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "28",
        "title": "Distributed Approach to Haskell Based Applications Refactoring with LLMs Based Multi-Agent Systems",
        "author": [
            "Shahbaz Siddeeq",
            "Zeeshan Rasheed",
            "Malik Abdul Sami",
            "Mahade Hasan",
            "Muhammad Waseem",
            "Jussi Rasku",
            "Mika Saari",
            "Kai-Kristian Kemell",
            "Pekka Abrahamsson"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07928",
        "abstract": "We present a large language models (LLMs) based multi-agent system to automate the refactoring of Haskell codebases. The multi-agent system consists of specialized agents performing tasks such as context analysis, refactoring, validation, and testing. Refactoring improvements are using metrics such as cyclomatic complexity, run-time, and memory allocation. Experimental evaluations conducted on Haskell codebases demonstrate improvements in code quality. Cyclomatic complexity was reduced by 13.64% and 47.06% in the respective codebases. Memory allocation improved by 4.17% and 41.73%, while runtime efficiency increased by up to 50%. These metrics highlight the systems ability to optimize Haskells functional paradigms while maintaining correctness and scalability. Results show reductions in complexity and performance enhancements across codebases. The integration of LLMs based multi-agent system enables precise task execution and inter-agent collaboration, addressing the challenges of refactoring in functional programming. This approach aims to address the challenges of refactoring functional programming languages through distributed and modular systems.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "29",
        "title": "Adapting Multilingual Embedding Models to Historical Luxembourgish",
        "author": [
            "Andrianos Michail",
            "Corina Julia RaclÃ©",
            "Juri Opitz",
            "Simon Clematide"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07938",
        "abstract": "The growing volume of digitized historical texts requires effective semantic search using text embeddings. However, pre-trained multilingual models, typically evaluated on contemporary texts, face challenges with historical digitized content due to OCR noise and outdated spellings. We explore the use of multilingual embeddings for cross-lingual semantic search on historical Luxembourgish, a low-resource language. We collect historical Luxembourgish news articles spanning various time periods and use GPT-4o to segment and translate them into closely related languages, creating 20,000 parallel training sentences per language pair. We further create a historical bitext mining evaluation set and find that these models struggle to perform cross-lingual search on historical Luxembourgish. To address this, we propose a simple adaptation method using in-domain training data, achieving up to 98\\% accuracy in cross-lingual evaluations. We release our adapted models and historical Luxembourgish-German/French bitexts to support further research.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "30",
        "title": "Symbiotic Cooperation for Web Agents: Harnessing Complementary Strengths of Large and Small LLMs",
        "author": [
            "Ruichen Zhang",
            "Mufan Qiu",
            "Zhen Tan",
            "Mohan Zhang",
            "Vincent Lu",
            "Jie Peng",
            "Kaidi Xu",
            "Leandro Z. Agudelo",
            "Peter Qian",
            "Tianlong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07942",
        "abstract": "Web browsing agents powered by large language models (LLMs) have shown tremendous potential in automating complex web-based tasks. Existing approaches typically rely on large LLMs (e.g., GPT-4o) to explore web environments and generate trajectory data, which is then used either for demonstration retrieval (for large LLMs) or to distill small LLMs (e.g., Llama3) in a process that remains decoupled from the exploration. In this paper, we propose AgentSymbiotic, an iterative framework that couples data synthesis with task-performance, yielding a \"symbiotic improvement\" for both large and small LLMs. Our study uncovers a complementary dynamic between LLM types: while large LLMs excel at generating high-quality trajectories for distillation, the distilled small LLMs-owing to their distinct reasoning capabilities-often choose actions that diverge from those of their larger counterparts. This divergence drives the exploration of novel trajectories, thereby enriching the synthesized data. However, we also observe that the performance of small LLMs becomes a bottleneck in this iterative enhancement process. To address this, we propose two innovations in LLM distillation: a speculative data synthesis strategy that mitigates off-policy bias, and a multi-task learning approach designed to boost the reasoning capabilities of the student LLM. Furthermore, we introduce a Hybrid Mode for Privacy Preservation to address user privacy concerns. Evaluated on the WEBARENA benchmark, AgentSymbiotic achieves SOTA performance with both LLM types. Our best Large LLM agent reaches 52%, surpassing the previous best of 45%, while our 8B distilled model demonstrates a competitive 49%, exceeding the prior best of 28%. Code will be released upon acceptance.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "31",
        "title": "SurGrID: Controllable Surgical Simulation via Scene Graph to Image Diffusion",
        "author": [
            "Yannik Frisch",
            "Ssharvien Kumar Sivakumar",
            "ÃaÄhan KÃ¶ksal",
            "Elsa BÃ¶hm",
            "Felix Wagner",
            "Adrian Gericke",
            "Ghazal Ghazaei",
            "Anirban Mukhopadhyay"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07945",
        "abstract": "Surgical simulation offers a promising addition to conventional surgical training. However, available simulation tools lack photorealism and rely on hardcoded behaviour. Denoising Diffusion Models are a promising alternative for high-fidelity image synthesis, but existing state-of-the-art conditioning methods fall short in providing precise control or interactivity over the generated scenes.\nWe introduce SurGrID, a Scene Graph to Image Diffusion Model, allowing for controllable surgical scene synthesis by leveraging Scene Graphs. These graphs encode a surgical scene's components' spatial and semantic information, which are then translated into an intermediate representation using our novel pre-training step that explicitly captures local and global information.\nOur proposed method improves the fidelity of generated images and their coherence with the graph input over the state-of-the-art. Further, we demonstrate the simulation's realism and controllability in a user assessment study involving clinical experts.\nScene Graphs can be effectively used for precise and interactive conditioning of Denoising Diffusion Models for simulating surgical scenes, enabling high fidelity and interactive control over the generated content.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "32",
        "title": "Bridging HCI and AI Research for the Evaluation of Conversational SE Assistants",
        "author": [
            "Jonan Richards",
            "Mairieli Wessel"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07956",
        "abstract": "As Large Language Models (LLMs) are increasingly adopted in software engineering, recently in the form of conversational assistants, ensuring these technologies align with developers' needs is essential. The limitations of traditional human-centered methods for evaluating LLM-based tools at scale raise the need for automatic evaluation. In this paper, we advocate combining insights from human-computer interaction (HCI) and artificial intelligence (AI) research to enable human-centered automatic evaluation of LLM-based conversational SE assistants. We identify requirements for such evaluation and challenges down the road, working towards a framework that ensures these assistants are designed and deployed in line with user needs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "33",
        "title": "Intrinsic Bias is Predicted by Pretraining Data and Correlates with Downstream Performance in Vision-Language Encoders",
        "author": [
            "Kshitish Ghate",
            "Isaac Slaughter",
            "Kyra Wilson",
            "Mona Diab",
            "Aylin Caliskan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07957",
        "abstract": "While recent work has found that vision-language models trained under the Contrastive Language Image Pre-training (CLIP) framework contain intrinsic social biases, the extent to which different upstream pre-training features of the framework relate to these biases, and hence how intrinsic bias and downstream performance are connected has been unclear. In this work, we present the largest comprehensive analysis to-date of how the upstream pre-training factors and downstream performance of CLIP models relate to their intrinsic biases. Studying 131 unique CLIP models, trained on 26 datasets, using 55 architectures, and in a variety of sizes, we evaluate bias in each model using 26 well-established unimodal and cross-modal principled Embedding Association Tests. We find that the choice of pre-training dataset is the most significant upstream predictor of bias, whereas architectural variations have minimal impact. Additionally, datasets curated using sophisticated filtering techniques aimed at enhancing downstream model performance tend to be associated with higher levels of intrinsic bias. Finally, we observe that intrinsic bias is often significantly correlated with downstream performance ($0.3 \\leq r \\leq 0.8$), suggesting that models optimized for performance inadvertently learn to amplify representational biases. Comparisons between unimodal and cross-modal association tests reveal that social group bias depends heavily on the modality. Our findings imply that more sophisticated strategies are needed to address intrinsic model bias for vision-language models across the entire model development pipeline.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "34",
        "title": "Training Sparse Mixture Of Experts Text Embedding Models",
        "author": [
            "Zach Nussbaum",
            "Brandon Duderstadt"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07972",
        "abstract": "Transformer-based text embedding models have improved their performance on benchmarks like MIRACL and BEIR by increasing their parameter counts. However, this scaling approach introduces significant deployment challenges, including increased inference latency and memory usage. These challenges are particularly severe in retrieval-augmented generation (RAG) applications, where large models' increased memory requirements constrain dataset ingestion capacity, and their higher latency directly impacts query-time performance. While causal language models have addressed similar efficiency challenges using Mixture of Experts (MoE) architectures, this approach hasn't been successfully adapted to the general text embedding setting. In this paper, we introduce Nomic Embed v2, the first general purpose MoE text embedding model. Our model outperforms models in the same parameter class on both monolingual and multilingual benchmarks while also maintaining competitive performance with models twice its size. We open-source all code, models, and evaluation data to ensure full reproducibility of our training pipeline.",
        "tags": [
            "RAG",
            "Transformer"
        ]
    },
    {
        "id": "35",
        "title": "From Hazard Identification to Controller Design: Proactive and LLM-Supported Safety Engineering for ML-Powered Systems",
        "author": [
            "Yining Hong",
            "Christopher S. Timperley",
            "Christian KÃ¤stner"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07974",
        "abstract": "Machine learning (ML) components are increasingly integrated into software products, yet their complexity and inherent uncertainty often lead to unintended and hazardous consequences, both for individuals and society at large. Despite these risks, practitioners seldom adopt proactive approaches to anticipate and mitigate hazards before they occur. Traditional safety engineering approaches, such as Failure Mode and Effects Analysis (FMEA) and System Theoretic Process Analysis (STPA), offer systematic frameworks for early risk identification but are rarely adopted. This position paper advocates for integrating hazard analysis into the development of any ML-powered software product and calls for greater support to make this process accessible to developers. By using large language models (LLMs) to partially automate a modified STPA process with human oversight at critical steps, we expect to address two key challenges: the heavy dependency on highly experienced safety engineering experts, and the time-consuming, labor-intensive nature of traditional hazard analysis, which often impedes its integration into real-world development workflows. We illustrate our approach with a running example, demonstrating that many seemingly unanticipated issues can, in fact, be anticipated.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "36",
        "title": "On positivity preservation of hybrid discontinuous Galerkin methods on hypergraphs",
        "author": [
            "Petr Knobloch",
            "Philip L. Lederer",
            "Andreas Rupp"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07976",
        "abstract": "Hybrid finite element methods, particularly hybridized discontinuous Galerkin (HDG) methods, are efficient numerical schemes for discretizing the diffusion equation, which encompasses two main physical principles: mass conservation and positivity preservation. While the former has been extensively analyzed in the literature, this paper investigates the latter. We state a theorem that guarantees the positivity of both the bulk and skeleton approximations to the primary unknown (concentration) and provide counterexamples for nonpositive discretizations. The theoretical findings are confirmed by numerical experiments.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "37",
        "title": "CIRCUIT: A Benchmark for Circuit Interpretation and Reasoning Capabilities of LLMs",
        "author": [
            "Lejla Skelic",
            "Yan Xu",
            "Matthew Cox",
            "Wenjie Lu",
            "Tao Yu",
            "Ruonan Han"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07980",
        "abstract": "The role of Large Language Models (LLMs) has not been extensively explored in analog circuit design, which could benefit from a reasoning-based approach that transcends traditional optimization techniques. In particular, despite their growing relevance, there are no benchmarks to assess LLMs' reasoning capability about circuits. Therefore, we created the CIRCUIT dataset consisting of 510 question-answer pairs spanning various levels of analog-circuit-related subjects. The best-performing model on our dataset, GPT-4o, achieves 48.04% accuracy when evaluated on the final numerical answer. To evaluate the robustness of LLMs on our dataset, we introduced a unique feature that enables unit-test-like evaluation by grouping questions into unit tests. In this case, GPT-4o can only pass 27.45% of the unit tests, highlighting that the most advanced LLMs still struggle with understanding circuits, which requires multi-level reasoning, particularly when involving circuit topologies. This circuit-specific benchmark highlights LLMs' limitations, offering valuable insights for advancing their application in analog integrated circuit design.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "38",
        "title": "AI Humor Generation: Cognitive, Social and Creative Skills for Effective Humor",
        "author": [
            "Sean Kim",
            "Lydia B. Chilton"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07981",
        "abstract": "Humor is a social binding agent. It is an act of creativity that can provoke emotional reactions on a broad range of topics. Humor has long been thought to be \"too human\" for AI to generate. However, humans are complex, and humor requires our complex set of skills: cognitive reasoning, social understanding, a broad base of knowledge, creative thinking, and audience understanding. We explore whether giving AI such skills enables it to write humor. We target one audience: Gen Z humor fans. We ask people to rate meme caption humor from three sources: highly upvoted human captions, 2) basic LLMs, and 3) LLMs captions with humor skills. We find that users like LLMs captions with humor skills more than basic LLMs and almost on par with top-rated humor written by people. We discuss how giving AI human-like skills can help it generate communication that resonates with people.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "39",
        "title": "Deep Semantic Graph Learning via LLM based Node Enhancement",
        "author": [
            "Chuanqi Shi",
            "Yiyi Tao",
            "Hang Zhang",
            "Lun Wang",
            "Shaoshuai Du",
            "Yixian Shen",
            "Yanxin Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07982",
        "abstract": "Graph learning has attracted significant attention due to its widespread real-world applications. Current mainstream approaches rely on text node features and obtain initial node embeddings through shallow embedding learning using GNNs, which shows limitations in capturing deep textual semantics. Recent advances in Large Language Models (LLMs) have demonstrated superior capabilities in understanding text semantics, transforming traditional text feature processing. This paper proposes a novel framework that combines Graph Transformer architecture with LLM-enhanced node features. Specifically, we leverage LLMs to generate rich semantic representations of text nodes, which are then processed by a multi-head self-attention mechanism in the Graph Transformer to capture both local and global graph structural information. Our model utilizes the Transformer's attention mechanism to dynamically aggregate neighborhood information while preserving the semantic richness provided by LLM embeddings. Experimental results demonstrate that the LLM-enhanced node features significantly improve the performance of graph learning models on node classification tasks. This approach shows promising results across multiple graph learning tasks, offering a practical direction for combining graph networks with language models.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "40",
        "title": "Universal Adversarial Attack on Aligned Multimodal LLMs",
        "author": [
            "Temurbek Rahmatullaev",
            "Polina Druzhinina",
            "Matvey Mikhalchuk",
            "Andrey Kuznetsov",
            "Anton Razzhigaev"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07987",
        "abstract": "We propose a universal adversarial attack on multimodal Large Language Models (LLMs) that leverages a single optimized image to override alignment safeguards across diverse queries and even multiple models. By backpropagating through the vision encoder and language head, we craft a synthetic image that forces the model to respond with a targeted phrase (e.g., ''Sure, here it is'') or otherwise unsafe content-even for harmful prompts. In experiments on the SafeBench benchmark, our method achieves significantly higher attack success rates than existing baselines, including text-only universal prompts (e.g., up to 93% on certain models). We further demonstrate cross-model transferability by training on several multimodal LLMs simultaneously and testing on unseen architectures. Additionally, a multi-answer variant of our approach produces more natural-sounding (yet still malicious) responses. These findings underscore critical vulnerabilities in current multimodal alignment and call for more robust adversarial defenses. We will release code and datasets under the Apache-2.0 license. Warning: some content generated by Multimodal LLMs in this paper may be offensive to some readers.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "41",
        "title": "Towards Training One-Step Diffusion Models Without Distillation",
        "author": [
            "Mingtian Zhang",
            "Jiajun He",
            "Wenlin Chen",
            "Zijing Ou",
            "JosÃ© Miguel HernÃ¡ndez-Lobato",
            "Bernhard SchÃ¶lkopf",
            "David Barber"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08005",
        "abstract": "Recent advances in one-step generative models typically follow a two-stage process: first training a teacher diffusion model and then distilling it into a one-step student model. This distillation process traditionally relies on both the teacher model's score function to compute the distillation loss and its weights for student initialization. In this paper, we explore whether one-step generative models can be trained directly without this distillation process. First, we show that the teacher's score function is not essential and propose a family of distillation methods that achieve competitive results without relying on score estimation. Next, we demonstrate that initialization from teacher weights is indispensable in successful training. Surprisingly, we find that this benefit is not due to improved ``input-output\" mapping but rather the learned feature representations, which dominate distillation quality. Our findings provide a better understanding of the role of initialization in one-step model training and its impact on distillation quality.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "42",
        "title": "Greed is Good: Guided Generation from a Greedy Perspective",
        "author": [
            "Zander W. Blasingame",
            "Chen Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08006",
        "abstract": "Training-free guided generation is a widely used and powerful technique that allows the end user to exert further control over the generative process of diffusion models. In this work, we explore the guided generation from the perspective of optimizing the solution trajectory of a neural differential equation in a greedy manner. We present such a strategy as a unifying view on training-free guidance by showing that the greedy strategy is a first-order discretization of end-to-end optimization techniques. We show that a greedy guidance strategy makes good decisions and compare it to a guidance strategy using the ideal gradients found via the continuous adjoint equations. We then show how other popular training-free guidance strategies can be viewed in a unified manner from this perspective.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "43",
        "title": "An Interactive Framework for Implementing Privacy-Preserving Federated Learning: Experiments on Large Language Models",
        "author": [
            "Kasra Ahmadi",
            "Rouzbeh Behnia",
            "Reza Ebrahimi",
            "Mehran Mozaffari Kermani",
            "Jeremiah Birrell",
            "Jason Pacheco",
            "Attila A Yavuz"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08008",
        "abstract": "Federated learning (FL) enhances privacy by keeping user data on local devices. However, emerging attacks have demonstrated that the updates shared by users during training can reveal significant information about their data. This has greatly thwart the adoption of FL methods for training robust AI models in sensitive applications. Differential Privacy (DP) is considered the gold standard for safeguarding user data. However, DP guarantees are highly conservative, providing worst-case privacy guarantees. This can result in overestimating privacy needs, which may compromise the model's accuracy. Additionally, interpretations of these privacy guarantees have proven to be challenging in different contexts. This is further exacerbated when other factors, such as the number of training iterations, data distribution, and specific application requirements, can add further complexity to this problem. In this work, we proposed a framework that integrates a human entity as a privacy practitioner to determine an optimal trade-off between the model's privacy and utility. Our framework is the first to address the variable memory requirement of existing DP methods in FL settings, where resource-limited devices (e.g., cell phones) can participate. To support such settings, we adopt a recent DP method with fixed memory usage to ensure scalable private FL. We evaluated our proposed framework by fine-tuning a BERT-based LLM model using the GLUE dataset (a common approach in literature), leveraging the new accountant, and employing diverse data partitioning strategies to mimic real-world conditions. As a result, we achieved stable memory usage, with an average accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon = 6$, when compared to the state-of-the-art DP accountant which does not support fixed memory usage.",
        "tags": [
            "BERT",
            "Large Language Models"
        ]
    },
    {
        "id": "44",
        "title": "The Geometry of Prompting: Unveiling Distinct Mechanisms of Task Adaptation in Language Models",
        "author": [
            "Artem Kirsanov",
            "Chi-Ning Chou",
            "Kyunghyun Cho",
            "SueYeon Chung"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08009",
        "abstract": "Decoder-only language models have the ability to dynamically switch between various computational tasks based on input prompts. Despite many successful applications of prompting, there is very limited understanding of the internal mechanism behind such flexibility. In this work, we investigate how different prompting methods affect the geometry of representations in these models. Employing a framework grounded in statistical physics, we reveal that various prompting techniques, while achieving similar performance, operate through distinct representational mechanisms for task adaptation. Our analysis highlights the critical role of input distribution samples and label semantics in few-shot in-context learning. We also demonstrate evidence of synergistic and interfering interactions between different tasks on the representational level. Our work contributes to the theoretical understanding of large language models and lays the groundwork for developing more effective, representation-aware prompting strategies.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "45",
        "title": "Training-Free Safe Denoisers for Safe Use of Diffusion Models",
        "author": [
            "Mingyu Kim",
            "Dongjun Kim",
            "Amman Yusuf",
            "Stefano Ermon",
            "Mi Jung Park"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08011",
        "abstract": "There is growing concern over the safety of powerful diffusion models (DMs), as they are often misused to produce inappropriate, not-safe-for-work (NSFW) content or generate copyrighted material or data of individuals who wish to be forgotten. Many existing methods tackle these issues by heavily relying on text-based negative prompts or extensively retraining DMs to eliminate certain features or samples. In this paper, we take a radically different approach, directly modifying the sampling trajectory by leveraging a negation set (e.g., unsafe images, copyrighted data, or datapoints needed to be excluded) to avoid specific regions of data distribution, without needing to retrain or fine-tune DMs. We formally derive the relationship between the expected denoised samples that are safe and those that are not safe, leading to our $\\textit{safe}$ denoiser which ensures its final samples are away from the area to be negated. Inspired by the derivation, we develop a practical algorithm that successfully produces high-quality samples while avoiding negation areas of the data distribution in text-conditional, class-conditional, and unconditional image generation scenarios. These results hint at the great potential of our training-free safe denoiser for using DMs more safely.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "46",
        "title": "Speculate, then Collaborate: Fusing Knowledge of Language Models during Decoding",
        "author": [
            "Ziyao Wang",
            "Muneeza Azmart",
            "Ang Li",
            "Raya Horesh",
            "Mikhail Yurochkin"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08020",
        "abstract": "Large Language Models (LLMs) often excel in specific domains but fall short in others due to the limitations of their training. Thus, enabling LLMs to solve problems collaboratively by integrating their complementary knowledge promises to improve their performance across domains. To realize this potential, we introduce a novel Collaborative Speculative Decoding (CoSD) algorithm that enables efficient LLM knowledge fusion at test time without requiring additional model training. CoSD employs a draft model to generate initial sequences and an easy-to-learn rule or decision tree to decide when to invoke an assistant model to improve these drafts. CoSD not only enhances knowledge fusion but also improves inference efficiency, is transferable across domains and models, and offers greater explainability. Experimental results demonstrate that CoSD improves accuracy by up to 10\\% across benchmarks compared to existing methods, providing a scalable and effective solution for LLM-based applications",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "47",
        "title": "Contextual Subspace Manifold Projection for Structural Refinement of Large Language Model Representations",
        "author": [
            "Alistair Wren",
            "Beatrice Loxley",
            "Hamish Cadwallader",
            "Simon Beckwith",
            "Fabian Pargeter",
            "James Blades"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08026",
        "abstract": "Internal representations within deep neural architectures encode high-dimensional abstractions of linguistic structures, yet they often exhibit inefficiencies in feature distribution, limiting expressiveness and adaptability. Contextual Subspace Manifold Projection introduces a structured refinement technique that selectively reconfigures token embeddings through controlled subspace constraints, ensuring more stable and geometrically well-defined feature distributions. Empirical evaluations demonstrated that the structured intervention reduced anisotropy, leading to improved representation compactness while preserving semantic fidelity across transformer layers. Clustering analyses indicated that token embeddings exhibited greater feature separability, reinforcing the hypothesis that structured projection techniques enhance internal representation organization without sacrificing linguistic coherence. Gradient magnitude distributions suggested that the method introduced a smoother optimization trajectory, potentially contributing to more stable parameter updates throughout training. Computational overhead associated with the projection operations remained minimal, ensuring that the refinements did not introduce significant trade-offs in model efficiency or inference speed. Comparisons with standard embedding refinement techniques highlighted that structured manifold constraints provided a direct mechanism for improving representation quality without requiring additional gradient-based optimization. Perplexity evaluations confirmed that the adjustments did not negatively impact sequence coherence, further validating the effectiveness of the proposed approach.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "48",
        "title": "End-to-End Predictive Planner for Autonomous Driving with Consistency Models",
        "author": [
            "Anjian Li",
            "Sangjae Bae",
            "David Isele",
            "Ryne Beeson",
            "Faizan M. Tariq"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08033",
        "abstract": "Trajectory prediction and planning are fundamental components for autonomous vehicles to navigate safely and efficiently in dynamic environments. Traditionally, these components have often been treated as separate modules, limiting the ability to perform interactive planning and leading to computational inefficiency in multi-agent scenarios. In this paper, we present a novel unified and data-driven framework that integrates prediction and planning with a single consistency model. Trained on real-world human driving datasets, our consistency model generates samples from high-dimensional, multimodal joint trajectory distributions of the ego and multiple surrounding agents, enabling end-to-end predictive planning. It effectively produces interactive behaviors, such as proactive nudging and yielding to ensure both safe and efficient interactions with other road users. To incorporate additional planning constraints on the ego vehicle, we propose an alternating direction method for multi-objective guidance in online guided sampling. Compared to diffusion models, our consistency model achieves better performance with fewer sampling steps, making it more suitable for real-time deployment. Experimental results on Waymo Open Motion Dataset (WOMD) demonstrate our method's superiority in trajectory quality, constraint satisfaction, and interactive behavior compared to various existing approaches.",
        "tags": [
            "Consistency Models",
            "Diffusion"
        ]
    },
    {
        "id": "49",
        "title": "Franken-Adapter: Cross-Lingual Adaptation of LLMs by Embedding Surgery",
        "author": [
            "Fan Jiang",
            "Honglin Yu",
            "Grace Chung",
            "Trevor Cohn"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08037",
        "abstract": "The capabilities of Large Language Models (LLMs) in low-resource languages lag far behind those in English, making their universal accessibility a significant challenge. To alleviate this, we present $\\textit{Franken-Adapter}$, a modular language adaptation approach for decoder-only LLMs with embedding surgery. Our method begins by creating customized vocabularies for target languages and performing language adaptation through embedding tuning on multilingual data. These pre-trained embeddings are subsequently integrated with LLMs that have been instruction-tuned on English alignment data to enable zero-shot cross-lingual transfer. Our experiments on $\\texttt{Gemma2}$ models with up to 27B parameters demonstrate improvements of up to 20% across 96 languages, spanning both discriminative and generative tasks, with minimal regressions ($<$1%) in English. Further in-depth analysis reveals the critical role of customizing tokenizers in enhancing language adaptation, while boosting inference efficiency. Additionally, we show the versatility of our method by achieving a 14% improvement over a math-optimized LLM across 20 languages, offering a modular solution to transfer reasoning abilities across languages post hoc.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "50",
        "title": "Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs",
        "author": [
            "Mohsinul Kabir",
            "Ajwad Abrar",
            "Sophia Ananiadou"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08045",
        "abstract": "A large number of studies rely on closed-style multiple-choice surveys to evaluate cultural alignment in Large Language Models (LLMs). In this work, we challenge this constrained evaluation paradigm and explore more realistic, unconstrained approaches. Using the World Values Survey (WVS) and Hofstede Cultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger cultural alignment in less constrained settings, where responses are not forced. Additionally, we show that even minor changes, such as reordering survey choices, lead to inconsistent outputs, exposing the limitations of closed-style evaluations. Our findings advocate for more robust and flexible evaluation frameworks that focus on specific cultural proxies, encouraging more nuanced and accurate assessments of cultural alignment in LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "51",
        "title": "On Mechanistic Circuits for Extractive Question-Answering",
        "author": [
            "Samyadeep Basu",
            "Vlad Morariu",
            "Zichao Wang",
            "Ryan Rossi",
            "Cherry Zhao",
            "Soheil Feizi",
            "Varun Manjunatha"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08059",
        "abstract": "Large language models are increasingly used to process documents and facilitate question-answering on them. In our paper, we extract mechanistic circuits for this real-world language modeling task: context-augmented language modeling for extractive question-answering (QA) tasks and understand the potential benefits of circuits towards downstream applications such as data attribution to context information. We extract circuits as a function of internal model components (e.g., attention heads, MLPs) using causal mediation analysis techniques. Leveraging the extracted circuits, we first understand the interplay between the model's usage of parametric memory and retrieved context towards a better mechanistic understanding of context-augmented language models. We then identify a small set of attention heads in our circuit which performs reliable data attribution by default, thereby obtaining attribution for free in just the model's forward pass. Using this insight, we then introduce ATTNATTRIB, a fast data attribution algorithm which obtains state-of-the-art attribution results across various extractive QA benchmarks. Finally, we show the possibility to steer the language model towards answering from the context, instead of the parametric memory by using the attribution from ATTNATTRIB as an additional signal during the forward pass. Beyond mechanistic understanding, our paper provides tangible applications of circuits in the form of reliable data attribution and model steering.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "52",
        "title": "Large language models perpetuate bias in palliative care: development and analysis of the Palliative Care Adversarial Dataset (PCAD)",
        "author": [
            "Naomi Akhras",
            "Fares Antaki",
            "Fannie Mottet",
            "Olivia Nguyen",
            "Shyam Sawhney",
            "Sabrina Bajwah",
            "Joanna M Davies"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08073",
        "abstract": "Bias and inequity in palliative care disproportionately affect marginalised groups. Large language models (LLMs), such as GPT-4o, hold potential to enhance care but risk perpetuating biases present in their training data. This study aimed to systematically evaluate whether GPT-4o propagates biases in palliative care responses using adversarially designed datasets. In July 2024, GPT-4o was probed using the Palliative Care Adversarial Dataset (PCAD), and responses were evaluated by three palliative care experts in Canada and the United Kingdom using validated bias rubrics. The PCAD comprised PCAD-Direct (100 adversarial questions) and PCAD-Counterfactual (84 paired scenarios). These datasets targeted four care dimensions (access to care, pain management, advance care planning, and place of death preferences) and three identity axes (ethnicity, age, and diagnosis). Bias was detected in a substantial proportion of responses. For adversarial questions, the pooled bias rate was 0.33 (95% confidence interval [CI]: 0.28, 0.38); \"allows biased premise\" was the most frequently identified source of bias (0.47; 95% CI: 0.39, 0.55), such as failing to challenge stereotypes. For counterfactual scenarios, the pooled bias rate was 0.26 (95% CI: 0.20, 0.31), with \"potential for withholding\" as the most frequently identified source of bias (0.25; 95% CI: 0.18, 0.34), such as withholding interventions based on identity. Bias rates were consistent across care dimensions and identity axes. GPT-4o perpetuates biases in palliative care, with implications for clinical decision-making and equity. The PCAD datasets provide novel tools to assess and address LLM bias in palliative care.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "53",
        "title": "NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals",
        "author": [
            "Neha Srikanth",
            "Rachel Rudinger"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08080",
        "abstract": "Decomposition of text into atomic propositions is a flexible framework allowing for the closer inspection of input and output text. We use atomic decomposition of hypotheses in two natural language reasoning tasks, traditional NLI and defeasible NLI, to form atomic sub-problems, or granular inferences that models must weigh when solving the overall problem. These atomic sub-problems serve as a tool to further understand the structure of both NLI and defeasible reasoning, probe a model's consistency and understanding of different inferences, and measure the diversity of examples in benchmark datasets. Our results indicate that LLMs still struggle with logical consistency on atomic NLI and defeasible NLI sub-problems. Lastly, we identify critical atomic sub-problems of defeasible NLI examples, or those that most contribute to the overall label, and propose a method to measure the inferential consistency of a model, a metric designed to capture the degree to which a model makes consistently correct or incorrect predictions about the same fact under different contexts.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "54",
        "title": "ID-Cloak: Crafting Identity-Specific Cloaks Against Personalized Text-to-Image Generation",
        "author": [
            "Qianrui Teng",
            "Xing Cui",
            "Xuannan Liu",
            "Peipei Li",
            "Zekun Li",
            "Huaibo Huang",
            "Ran He"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08097",
        "abstract": "Personalized text-to-image models allow users to generate images of new concepts from several reference photos, thereby leading to critical concerns regarding civil privacy. Although several anti-personalization techniques have been developed, these methods typically assume that defenders can afford to design a privacy cloak corresponding to each specific image. However, due to extensive personal images shared online, image-specific methods are limited by real-world practical applications. To address this issue, we are the first to investigate the creation of identity-specific cloaks (ID-Cloak) that safeguard all images belong to a specific identity. Specifically, we first model an identity subspace that preserves personal commonalities and learns diverse contexts to capture the image distribution to be protected. Then, we craft identity-specific cloaks with the proposed novel objective that encourages the cloak to guide the model away from its normal output within the subspace. Extensive experiments show that the generated universal cloak can effectively protect the images. We believe our method, along with the proposed identity-specific cloak setting, marks a notable advance in realistic privacy protection.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "55",
        "title": "Rethinking Tokenized Graph Transformers for Node Classification",
        "author": [
            "Jinsong Chen",
            "Chenyang Li",
            "GaiChao Li",
            "John E. Hopcroft",
            "Kun He"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08101",
        "abstract": "Node tokenized graph Transformers (GTs) have shown promising performance in node classification. The generation of token sequences is the key module in existing tokenized GTs which transforms the input graph into token sequences, facilitating the node representation learning via Transformer. In this paper, we observe that the generations of token sequences in existing GTs only focus on the first-order neighbors on the constructed similarity graphs, which leads to the limited usage of nodes to generate diverse token sequences, further restricting the potential of tokenized GTs for node classification. To this end, we propose a new method termed SwapGT. SwapGT first introduces a novel token swapping operation based on the characteristics of token sequences that fully leverages the semantic relevance of nodes to generate more informative token sequences. Then, SwapGT leverages a Transformer-based backbone to learn node representations from the generated token sequences. Moreover, SwapGT develops a center alignment loss to constrain the representation learning from multiple token sequences, further enhancing the model performance. Extensive empirical results on various datasets showcase the superiority of SwapGT for node classification.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "56",
        "title": "PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced Text-to-Image Generation",
        "author": [
            "Ziyan Wang",
            "Sizhe Wei",
            "Xiaoming Huo",
            "Hao Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08106",
        "abstract": "Diffusion models have made significant advancements in recent years. However, their performance often deteriorates when trained or fine-tuned on imbalanced datasets. This degradation is largely due to the disproportionate representation of majority and minority data in image-text pairs. In this paper, we propose a general fine-tuning approach, dubbed PoGDiff, to address this challenge. Rather than directly minimizing the KL divergence between the predicted and ground-truth distributions, PoGDiff replaces the ground-truth distribution with a Product of Gaussians (PoG), which is constructed by combining the original ground-truth targets with the predicted distribution conditioned on a neighboring text embedding. Experiments on real-world datasets demonstrate that our method effectively addresses the imbalance problem in diffusion models, improving both generation accuracy and quality.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "57",
        "title": "HuDEx: Integrating Hallucination Detection and Explainability for Enhancing the Reliability of LLM responses",
        "author": [
            "Sujeong Lee",
            "Hayoung Lee",
            "Seongsoo Heo",
            "Wonik Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08109",
        "abstract": "Recent advances in large language models (LLMs) have shown promising improvements, often surpassing existing methods across a wide range of downstream tasks in natural language processing. However, these models still face challenges, which may hinder their practical applicability. For example, the phenomenon of hallucination is known to compromise the reliability of LLMs, especially in fields that demand high factual precision. Current benchmarks primarily focus on hallucination detection and factuality evaluation but do not extend beyond identification. This paper proposes an explanation enhanced hallucination-detection model, coined as HuDEx, aimed at enhancing the reliability of LLM-generated responses by both detecting hallucinations and providing detailed explanations. The proposed model provides a novel approach to integrate detection with explanations, and enable both users and the LLM itself to understand and reduce errors. Our measurement results demonstrate that the proposed model surpasses larger LLMs, such as Llama3 70B and GPT-4, in hallucination detection accuracy, while maintaining reliable explanations. Furthermore, the proposed model performs well in both zero-shot and other test environments, showcasing its adaptability across diverse benchmark datasets. The proposed approach further enhances the hallucination detection research by introducing a novel approach to integrating interpretability with hallucination detection, which further enhances the performance and reliability of evaluating hallucinations in language models.",
        "tags": [
            "Detection",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "58",
        "title": "Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models",
        "author": [
            "Sonam Gupta",
            "Yatin Nandwani",
            "Asaf Yehudai",
            "Dinesh Khandelwal",
            "Dinesh Raghu",
            "Sachindra Joshi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08130",
        "abstract": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task or the characteristics of the training data, resulting in a loss of generalization. This paper introduces Selective Self-to-Supervised Fine-Tuning (S3FT), a fine-tuning approach that achieves better performance than the standard supervised fine-tuning (SFT) while improving generalization. S3FT leverages the existence of multiple valid responses to a query. By utilizing the model's correct responses, S3FT reduces model specialization during the fine-tuning stage. S3FT first identifies the correct model responses from the training set by deploying an appropriate judge. Then, it fine-tunes the model using the correct model responses and the gold response (or its paraphrase) for the remaining samples. The effectiveness of S3FT is demonstrated through experiments on mathematical reasoning, Python programming and reading comprehension tasks. The results show that standard SFT can lead to an average performance drop of up to $4.4$ on multiple benchmarks, such as MMLU and TruthfulQA. In contrast, S3FT reduces this drop by half, i.e. $2.5$, indicating better generalization capabilities than SFT while performing significantly better on the fine-tuning tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "59",
        "title": "LowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits",
        "author": [
            "Zikai Zhou",
            "Qizheng Zhang",
            "Hermann Kumbong",
            "Kunle Olukotun"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08141",
        "abstract": "Fine-tuning large language models (LLMs) is increasingly costly as models scale to hundreds of billions of parameters, and even parameter-efficient fine-tuning (PEFT) methods like LoRA remain resource-intensive. We introduce LowRA, the first framework to enable LoRA fine-tuning below 2 bits per parameter with minimal performance loss. LowRA optimizes fine-grained quantization - mapping, threshold selection, and precision assignment - while leveraging efficient CUDA kernels for scalable deployment. Extensive evaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior performance-precision trade-off above 2 bits and remains accurate down to 1.15 bits, reducing memory usage by up to 50%. Our results highlight the potential of ultra-low-bit LoRA fine-tuning for resource-constrained environments.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "60",
        "title": "Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers",
        "author": [
            "Siddharth Singh",
            "Prajwal Singhania",
            "Aditya Ranjan",
            "John Kirchenbauer",
            "Jonas Geiping",
            "Yuxin Wen",
            "Neel Jain",
            "Abhimanyu Hans",
            "Manli Shu",
            "Aditya Tomar",
            "Tom Goldstein",
            "Abhinav Bhatele"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08145",
        "abstract": "Training and fine-tuning large language models (LLMs) with hundreds of billions to trillions of parameters requires tens of thousands of GPUs, and a highly scalable software stack. In this work, we present a novel four-dimensional hybrid parallel algorithm implemented in a highly scalable, portable, open-source framework called AxoNN. We describe several performance optimizations in AxoNN to improve matrix multiply kernel performance, overlap non-blocking collectives with computation, and performance modeling to choose performance optimal configurations. These have resulted in unprecedented scaling and peak flop/s (bf16) for training of GPT-style transformer models on Perlmutter (620.1 Petaflop/s), Frontier (1.381 Exaflop/s) and Alps (1.423 Exaflop/s).\nWhile the abilities of LLMs improve with the number of trainable parameters, so do privacy and copyright risks caused by memorization of training data, which can cause disclosure of sensitive or private information at inference time. We highlight this side effect of scale through experiments that explore \"catastrophic memorization\", where models are sufficiently large to memorize training data in a single pass, and present an approach to prevent it. As part of this study, we demonstrate fine-tuning of a 405-billion parameter LLM using AxoNN on Frontier.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "61",
        "title": "ACCESS : A Benchmark for Abstract Causal Event Discovery and Reasoning",
        "author": [
            "Vy Vo",
            "Lizhen Qu",
            "Tao Feng",
            "Yuncheng Hua",
            "Xiaoxi Kang",
            "Songhai Fan",
            "Tim Dwyer",
            "Lay-Ki Soon",
            "Gholamreza Haffari"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08148",
        "abstract": "Identifying cause-and-effect relationships is critical to understanding real-world dynamics and ultimately causal reasoning. Existing methods for identifying event causality in NLP, including those based on Large Language Models (LLMs), exhibit difficulties in out-of-distribution settings due to the limited scale and heavy reliance on lexical cues within available benchmarks. Modern benchmarks, inspired by probabilistic causal inference, have attempted to construct causal graphs of events as a robust representation of causal knowledge, where \\texttt{CRAB} \\citep{romanou2023crab} is one such recent benchmark along this line. In this paper, we introduce \\texttt{ACCESS}, a benchmark designed for discovery and reasoning over abstract causal events. Unlike existing resources, \\texttt{ACCESS} focuses on causality of everyday life events on the abstraction level. We propose a pipeline for identifying abstractions for event generalizations from \\texttt{GLUCOSE} \\citep{mostafazadeh-etal-2020-glucose}, a large-scale dataset of implicit commonsense causal knowledge, from which we subsequently extract $1,4$K causal pairs. Our experiments highlight the ongoing challenges of using statistical methods and/or LLMs for automatic abstraction identification and causal discovery in NLP. Nonetheless, we demonstrate that the abstract causal knowledge provided in \\texttt{ACCESS} can be leveraged for enhancing QA reasoning performance in LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "62",
        "title": "Force Matching with Relativistic Constraints: A Physics-Inspired Approach to Stable and Efficient Generative Modeling",
        "author": [
            "Yang Cao",
            "Bo Chen",
            "Xiaoyu Li",
            "Yingyu Liang",
            "Zhizhou Sha",
            "Zhenmei Shi",
            "Zhao Song",
            "Mingda Wan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08150",
        "abstract": "This paper introduces Force Matching (ForM), a novel framework for generative modeling that represents an initial exploration into leveraging special relativistic mechanics to enhance the stability of the sampling process. By incorporating the Lorentz factor, ForM imposes a velocity constraint, ensuring that sample velocities remain bounded within a constant limit. This constraint serves as a fundamental mechanism for stabilizing the generative dynamics, leading to a more robust and controlled sampling process. We provide a rigorous theoretical analysis demonstrating that the velocity constraint is preserved throughout the sampling procedure within the ForM framework. To validate the effectiveness of our approach, we conduct extensive empirical evaluations. On the \\textit{half-moons} dataset, ForM significantly outperforms baseline methods, achieving the lowest Euclidean distance loss of \\textbf{0.714}, in contrast to vanilla first-order flow matching (5.853) and first- and second-order flow matching (5.793). Additionally, we perform an ablation study to further investigate the impact of our velocity constraint, reaffirming the superiority of ForM in stabilizing the generative process. The theoretical guarantees and empirical results underscore the potential of integrating special relativity principles into generative modeling. Our findings suggest that ForM provides a promising pathway toward achieving stable, efficient, and flexible generative processes. This work lays the foundation for future advancements in high-dimensional generative modeling, opening new avenues for the application of physical principles in machine learning.",
        "tags": [
            "Flow Matching"
        ]
    },
    {
        "id": "63",
        "title": "DNNs May Determine Major Properties of Their Outputs Early, with Timing Possibly Driven by Bias",
        "author": [
            "Song Park",
            "Sanghyuk Chun",
            "Byeongho Heo",
            "Dongyoon Han"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08167",
        "abstract": "This paper argues that deep neural networks (DNNs) mostly determine their outputs during the early stages of inference, where biases inherent in the model play a crucial role in shaping this process. We draw a parallel between this phenomenon and human decision-making, which often relies on fast, intuitive heuristics. Using diffusion models (DMs) as a case study, we demonstrate that DNNs often make early-stage decision-making influenced by the type and extent of bias in their design and training. Our findings offer a new perspective on bias mitigation, efficient inference, and the interpretation of machine learning systems. By identifying the temporal dynamics of decision-making in DNNs, this paper aims to inspire further discussion and research within the machine learning community.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "64",
        "title": "Estimating the quality of academic books from their descriptions with ChatGPT",
        "author": [
            "Mike Thelwall",
            "Andrew Cox"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08171",
        "abstract": "Although indicators based on scholarly citations are widely used to support the evaluation of academic journals, alternatives are needed for scholarly book acquisitions. This article assesses the value of research quality scores from ChatGPT 4o-mini for 9,830 social sciences, arts, and humanities books from 2019 indexed in Scopus, based on their titles and descriptions but not their full texts. Although most books scored the same (3* on a 1* to 4* scale), the citation rates correlate positively but weakly with ChatGPT 4o-mini research quality scores in both the social sciences and the arts and humanities. Part of the reason for the differences was the inclusion of textbooks, short books, and edited collections, all of which tended to be less cited and lower scoring. Some topics also tend to attract many/few citations and/or high/low ChatGPT scores. Descriptions explicitly mentioning theory and/or some methods also associated with higher scores and more citations. Overall, the results provide some evidence that both ChatGPT scores and citation counts are weak indicators of the research quality of books. Whilst not strong enough to support individual book quality judgements, they may help academic librarians seeking to evaluate new book collections, series, or publishers for potential acquisition.",
        "tags": [
            "ChatGPT"
        ]
    },
    {
        "id": "65",
        "title": "Intention is All You Need: Refining Your Code from Your Intention",
        "author": [
            "Qi Guo",
            "Xiaofei Xie",
            "Shangqing Liu",
            "Ming Hu",
            "Xiaohong Li",
            "Lei Bu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08172",
        "abstract": "Code refinement aims to enhance existing code by addressing issues, refactoring, and optimizing to improve quality and meet specific requirements. As software projects scale in size and complexity, the traditional iterative exchange between reviewers and developers becomes increasingly burdensome. While recent deep learning techniques have been explored to accelerate this process, their performance remains limited, primarily due to challenges in accurately understanding reviewers' intents.\nThis paper proposes an intention-based code refinement technique that enhances the conventional comment-to-code process by explicitly extracting reviewer intentions from the comments. Our approach consists of two key phases: Intention Extraction and Intention Guided Revision Generation. Intention Extraction categorizes comments using predefined templates, while Intention Guided Revision Generation employs large language models (LLMs) to generate revised code based on these defined intentions. Three categories with eight subcategories are designed for comment transformation, which is followed by a hybrid approach that combines rule-based and LLM-based classifiers for accurate classification. Extensive experiments with five LLMs (GPT4o, GPT3.5, DeepSeekV2, DeepSeek7B, CodeQwen7B) under different prompting settings demonstrate that our approach achieves 79% accuracy in intention extraction and up to 66% in code refinement generation. Our results highlight the potential of our approach in enhancing data quality and improving the efficiency of code refinement.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "66",
        "title": "ParetoRAG: Leveraging Sentence-Context Attention for Robust and Efficient Retrieval-Augmented Generation",
        "author": [
            "Ruobing Yao",
            "Yifei Zhang",
            "Shuang Song",
            "Yuhua Liu",
            "Neng Gao",
            "Chenyang Tu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08178",
        "abstract": "While Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by incorporating external knowledge, they still face persistent challenges in retrieval inefficiency and the inability of LLMs to filter out irrelevant information. We present ParetoRAG, an unsupervised framework that optimizes RAG systems through sentence-level refinement guided by the Pareto principle. By decomposing paragraphs into sentences and dynamically re-weighting core content while preserving contextual coherence, ParetoRAG achieves dual improvements in both retrieval precision and generation quality without requiring additional training or API resources. This framework has been empirically validated across various datasets, LLMs, and retrievers.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "67",
        "title": "Enhancing LLM Character-Level Manipulation via Divide and Conquer",
        "author": [
            "Zhen Xiong",
            "Yujun Cai",
            "Bryan Hooi",
            "Nanyun Peng",
            "Kai-Wei Chang",
            "Zhecheng Li",
            "Yiwei Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08180",
        "abstract": "Large Language Models (LLMs) have demonstrated strong generalization capabilities across a wide range of natural language processing (NLP) tasks. However, they exhibit notable weaknesses in character-level string manipulation, struggling with fundamental operations such as character deletion, insertion, and substitution. These challenges stem primarily from tokenization constraints, despite the critical role of such operations in data preprocessing and code generation. Through systematic analysis, we derive two key insights: (1) LLMs face significant difficulties in leveraging intrinsic token knowledge for character-level reasoning, and (2) atomized word structures can substantially enhance LLMs' ability to process token-level structural information. Building on these insights, we propose Character-Level Manipulation via Divide and Conquer, a novel approach designed to bridge the gap between token-level processing and character-level manipulation. Our method decomposes complex operations into explicit character-level subtasks coupled with controlled token reconstruction phases, leading to significant improvements in accuracy. Without additional training, our method significantly improves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and $\\texttt{Substitution}$ tasks. To support further research, we open-source our implementation and benchmarks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "68",
        "title": "Memory Offloading for Large Language Model Inference with Latency SLO Guarantees",
        "author": [
            "Chenxiang Ma",
            "Zhisheng Ye",
            "Hanyu Zhao",
            "Zehua Yang",
            "Tianhao Fu",
            "Jiaxun Han",
            "Jie Zhang",
            "Yingwei Luo",
            "Xiaolin Wang",
            "Zhenlin Wang",
            "Yong Li",
            "Diyu Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08182",
        "abstract": "Offloading large language models (LLMs) state to host memory during inference promises to reduce operational costs by supporting larger models, longer inputs, and larger batch sizes. However, the design of existing memory offloading mechanisms does not take latency service-level objectives (SLOs) into consideration. As a result, they either lead to frequent SLO violations or underutilize host memory, thereby incurring economic loss and thus defeating the purpose of memory offloading.\nThis paper presents Select-N, a latency-SLO-aware memory offloading system for LLM serving. A key challenge in designing Select-N is to reconcile the tension between meeting SLOs and maximizing host memory usage. Select-N overcomes it by exploiting a unique characteristic of modern LLMs: during serving, the computation time of each decoder layer is deterministic. Leveraging this, Select-N introduces offloading interval, an internal tunable knob that captures the tradeoff between SLOs and host memory usage, thereby reducing the aforementioned challenge to pick an optimal offloading interval. With that, Select-N proposes a two-stage approach to automatically pick the offloading interval. The first stage is offline that generates the range of optimal offloading interval, while the second stage adjusts offloading interval at the granularity of inference iteration based on runtime hardware status. Our evaluation shows that Select-N consistently meets SLOs and improves the serving throughput over existing mechanisms by 1.85X due to maximizing the use of host memory.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "69",
        "title": "AnyCharV: Bootstrap Controllable Character Video Generation with Fine-to-Coarse Guidance",
        "author": [
            "Zhao Wang",
            "Hao Wen",
            "Lingting Zhu",
            "Chenming Shang",
            "Yujiu Yang",
            "Qi Dou"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08189",
        "abstract": "Character video generation is a significant real-world application focused on producing high-quality videos featuring specific characters. Recent advancements have introduced various control signals to animate static characters, successfully enhancing control over the generation process. However, these methods often lack flexibility, limiting their applicability and making it challenging for users to synthesize a source character into a desired target scene. To address this issue, we propose a novel framework, AnyCharV, that flexibly generates character videos using arbitrary source characters and target scenes, guided by pose information. Our approach involves a two-stage training process. In the first stage, we develop a base model capable of integrating the source character with the target scene using pose guidance. The second stage further bootstraps controllable generation through a self-boosting mechanism, where we use the generated video in the first stage and replace the fine mask with the coarse one, enabling training outcomes with better preservation of character details. Experimental results demonstrate the effectiveness and robustness of our proposed method. Our project page is https://anycharv.github.io.",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "70",
        "title": "Typographic Attacks in a Multi-Image Setting",
        "author": [
            "Xiaomeng Wang",
            "Zhengyu Zhao",
            "Martha Larson"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08193",
        "abstract": "Large Vision-Language Models (LVLMs) are susceptible to typographic attacks, which are misclassifications caused by an attack text that is added to an image. In this paper, we introduce a multi-image setting for studying typographic attacks, broadening the current emphasis of the literature on attacking individual images. Specifically, our focus is on attacking image sets without repeating the attack query. Such non-repeating attacks are stealthier, as they are more likely to evade a gatekeeper than attacks that repeat the same attack text. We introduce two attack strategies for the multi-image setting, leveraging the difficulty of the target image, the strength of the attack text, and text-image similarity. Our text-image similarity approach improves attack success rates by 21% over random, non-specific methods on the CLIP model using ImageNet while maintaining stealth in a multi-image scenario. An additional experiment demonstrates transferability, i.e., text-image similarity calculated using CLIP transfers when attacking InstructBLIP.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "71",
        "title": "LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention",
        "author": [
            "Konstantin Kolomeitsev"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08213",
        "abstract": "In this work, we propose an architecture of LLM Modules that enables the transfer of knowledge from a large pre-trained model to a smaller model using an Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B model is frozen and its representations are passed through specially designed attention layers to the GPT-Neo-125M model, which is trained on limited computational resources. Experimental results on the Bespoke-Stratos-17k dataset demonstrate that after 15 epochs of training, the combined model generates responses comparable in quality to those obtained by distillation. We discuss the advantages of the modular approach, provide examples of input queries and comparative analysis, and outline prospects for further extension of the method.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "72",
        "title": "Flow-of-Action: SOP Enhanced LLM-Based Multi-Agent System for Root Cause Analysis",
        "author": [
            "Changhua Pei",
            "Zexin Wang",
            "Fengrui Liu",
            "Zeyan Li",
            "Yang Liu",
            "Xiao He",
            "Rong Kang",
            "Tieying Zhang",
            "Jianjun Chen",
            "Jianhui Li",
            "Gaogang Xie",
            "Dan Pei"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08224",
        "abstract": "In the realm of microservices architecture, the occurrence of frequent incidents necessitates the employment of Root Cause Analysis (RCA) for swift issue resolution. It is common that a serious incident can take several domain experts hours to identify the root cause. Consequently, a contemporary trend involves harnessing Large Language Models (LLMs) as automated agents for RCA. Though the recent ReAct framework aligns well with the Site Reliability Engineers (SREs) for its thought-action-observation paradigm, its hallucinations often lead to irrelevant actions and directly affect subsequent results. Additionally, the complex and variable clues of the incident can overwhelm the model one step further. To confront these challenges, we propose Flow-of-Action, a pioneering Standard Operation Procedure (SOP) enhanced LLM-based multi-agent system. By explicitly summarizing the diagnosis steps of SREs, SOP imposes constraints on LLMs at crucial junctures, guiding the RCA process towards the correct trajectory. To facilitate the rational and effective utilization of SOPs, we design an SOP-centric framework called SOP flow. SOP flow contains a series of tools, including one for finding relevant SOPs for incidents, another for automatically generating SOPs for incidents without relevant ones, and a tool for converting SOPs into code. This significantly alleviates the hallucination issues of ReAct in RCA tasks. We also design multiple auxiliary agents to assist the main agent by removing useless noise, narrowing the search space, and informing the main agent whether the RCA procedure can stop. Compared to the ReAct method's 35.50% accuracy, our Flow-of-Action method achieves 64.01%, meeting the accuracy requirements for RCA in real-world systems.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "73",
        "title": "TRISHUL: Towards Region Identification and Screen Hierarchy Understanding for Large VLM based GUI Agents",
        "author": [
            "Kunal Singh",
            "Shreyas Singh",
            "Mukund Khanna"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08226",
        "abstract": "Recent advancements in Large Vision Language Models (LVLMs) have enabled the development of LVLM-based Graphical User Interface (GUI) agents under various paradigms. Training-based approaches, such as CogAgent and SeeClick, struggle with cross-dataset and cross-platform generalization due to their reliance on dataset-specific training. Generalist LVLMs, such as GPT-4V, employ Set-of-Marks (SoM) for action grounding, but obtaining SoM labels requires metadata like HTML source, which is not consistently available across platforms. Moreover, existing methods often specialize in singular GUI tasks rather than achieving comprehensive GUI understanding. To address these limitations, we introduce TRISHUL, a novel, training-free agentic framework that enhances generalist LVLMs for holistic GUI comprehension. Unlike prior works that focus on either action grounding (mapping instructions to GUI elements) or GUI referring (describing GUI elements given a location), TRISHUL seamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen Parsing (HSP) and the Spatially Enhanced Element Description (SEED) module, which work synergistically to provide multi-granular, spatially, and semantically enriched representations of GUI elements. Our results demonstrate TRISHUL's superior performance in action grounding across the ScreenSpot, VisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring, TRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new standard for robust and adaptable GUI comprehension.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "74",
        "title": "Learning Human Skill Generators at Key-Step Levels",
        "author": [
            "Yilu Wu",
            "Chenhui Zhu",
            "Shuai Wang",
            "Hanlin Wang",
            "Jing Wang",
            "Zhaoxiang Zhang",
            "Limin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08234",
        "abstract": "We are committed to learning human skill generators at key-step levels. The generation of skills is a challenging endeavor, but its successful implementation could greatly facilitate human skill learning and provide more experience for embodied intelligence. Although current video generation models can synthesis simple and atomic human operations, they struggle with human skills due to their complex procedure process. Human skills involve multi-step, long-duration actions and complex scene transitions, so the existing naive auto-regressive methods for synthesizing long videos cannot generate human skills. To address this, we propose a novel task, the Key-step Skill Generation (KS-Gen), aimed at reducing the complexity of generating human skill videos. Given the initial state and a skill description, the task is to generate video clips of key steps to complete the skill, rather than a full-length video. To support this task, we introduce a carefully curated dataset and define multiple evaluation metrics to assess performance. Considering the complexity of KS-Gen, we propose a new framework for this task. First, a multimodal large language model (MLLM) generates descriptions for key steps using retrieval argument. Subsequently, we use a Key-step Image Generator (KIG) to address the discontinuity between key steps in skill videos. Finally, a video generation model uses these descriptions and key-step images to generate video clips of the key steps with high temporal consistency. We offer a detailed analysis of the results, hoping to provide more insights on human skill generation. All models and data are available at https://github.com/MCG-NJU/KS-Gen.",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "75",
        "title": "FloVD: Optical Flow Meets Video Diffusion Model for Enhanced Camera-Controlled Video Synthesis",
        "author": [
            "Wonjoon Jin",
            "Qi Dai",
            "Chong Luo",
            "Seung-Hwan Baek",
            "Sunghyun Cho"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08244",
        "abstract": "This paper presents FloVD, a novel optical-flow-based video diffusion model for camera-controllable video generation. FloVD leverages optical flow maps to represent motions of the camera and moving objects. This approach offers two key benefits. Since optical flow can be directly estimated from videos, our approach allows for the use of arbitrary training videos without ground-truth camera parameters. Moreover, as background optical flow encodes 3D correlation across different viewpoints, our method enables detailed camera control by leveraging the background motion. To synthesize natural object motion while supporting detailed camera control, our framework adopts a two-stage video synthesis pipeline consisting of optical flow generation and flow-conditioned video synthesis. Extensive experiments demonstrate the superiority of our method over previous approaches in terms of accurate camera control and natural object motion synthesis.",
        "tags": [
            "3D",
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "76",
        "title": "Inference-time sparse attention with asymmetric indexing",
        "author": [
            "Pierre-Emmanuel MazarÃ©",
            "Gergely Szilvasy",
            "Maria Lomeli",
            "Francisco Massa",
            "Naila Murray",
            "HervÃ© JÃ©gou",
            "Matthijs Douze"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08246",
        "abstract": "Self-attention in transformer models is an incremental associative memory that maps key vectors to value vectors. One way to speed up self-attention is to employ GPU-compliant vector search algorithms, yet the standard partitioning methods yield poor results in this context, because (1) keys and queries follow different distributions and (2) the effect of RoPE positional encoding.\nIn this paper, we introduce SAAP (Self-Attention with Asymmetric Partitions), which overcomes these problems. It is an asymmetrical indexing technique that employs distinct partitions for keys and queries, thereby approximating self-attention with a data-adaptive sparsity pattern.\nIt works on pretrained language models without finetuning, as it only requires to train (offline) a small query classifier. On a long context Llama 3.1-8b model, with sequences ranging from 100k to 500k tokens, our method typically reduces by a factor 20 the fraction of memory that needs to be looked-up, which translates to a time saving of 60\\% when compared to FlashAttention-v2.",
        "tags": [
            "LLaMA",
            "Transformer"
        ]
    },
    {
        "id": "77",
        "title": "Exploring the Potential of Large Language Models to Simulate Personality",
        "author": [
            "Maria Molchanova",
            "Anna Mikhailova",
            "Anna Korzanova",
            "Lidiia Ostyakova",
            "Alexandra Dolidze"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08265",
        "abstract": "With the advancement of large language models (LLMs), the focus in Conversational AI has shifted from merely generating coherent and relevant responses to tackling more complex challenges, such as personalizing dialogue systems. In an effort to enhance user engagement, chatbots are often designed to mimic human behaviour, responding within a defined emotional spectrum and aligning to a set of values. In this paper, we aim to simulate personal traits according to the Big Five model with the use of LLMs. Our research showed that generating personality-related texts is still a challenging task for the models. As a result, we present a dataset of generated texts with the predefined Big Five characteristics and provide an analytical framework for testing LLMs on a simulation of personality skills.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "78",
        "title": "Dealing with Annotator Disagreement in Hate Speech Classification",
        "author": [
            "Somaiyeh Dehghan",
            "Mehmet Umut Sen",
            "Berrin Yanikoglu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08266",
        "abstract": "Hate speech detection is a crucial task, especially on social media, where harmful content can spread quickly. Implementing machine learning models to automatically identify and address hate speech is essential for mitigating its impact and preventing its proliferation. The first step in developing an effective hate speech detection model is to acquire a high-quality dataset for training. Labeled data is foundational for most natural language processing tasks, but categorizing hate speech is difficult due to the diverse and often subjective nature of hate speech, which can lead to varying interpretations and disagreements among annotators. This paper examines strategies for addressing annotator disagreement, an issue that has been largely overlooked. In particular, we evaluate different approaches to deal with annotator disagreement regarding hate speech classification in Turkish tweets, based on a fine-tuned BERT model. Our work highlights the importance of the problem and provides state-of-art benchmark results for detection and understanding of hate speech in online discourse.",
        "tags": [
            "BERT",
            "Detection"
        ]
    },
    {
        "id": "79",
        "title": "Redefining Simplicity: Benchmarking Large Language Models from Lexical to Document Simplification",
        "author": [
            "Jipeng Qiang",
            "Minjiang Huang",
            "Yi Zhu",
            "Yunhao Yuan",
            "Chaowei Zhang",
            "Kui Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08281",
        "abstract": "Text simplification (TS) refers to the process of reducing the complexity of a text while retaining its original meaning and key information. Existing work only shows that large language models (LLMs) have outperformed supervised non-LLM-based methods on sentence simplification. This study offers the first comprehensive analysis of LLM performance across four TS tasks: lexical, syntactic, sentence, and document simplification. We compare lightweight, closed-source and open-source LLMs against traditional non-LLM methods using automatic metrics and human evaluations. Our experiments reveal that LLMs not only outperform non-LLM approaches in all four tasks but also often generate outputs that exceed the quality of existing human-annotated references. Finally, we present some future directions of TS in the era of LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "80",
        "title": "Fully-Geometric Cross-Attention for Point Cloud Registration",
        "author": [
            "Weijie Wang",
            "Guofeng Mei",
            "Jian Zhang",
            "Nicu Sebe",
            "Bruno Lepri",
            "Fabio Poiesi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08285",
        "abstract": "Point cloud registration approaches often fail when the overlap between point clouds is low due to noisy point correspondences. This work introduces a novel cross-attention mechanism tailored for Transformer-based architectures that tackles this problem, by fusing information from coordinates and features at the super-point level between point clouds. This formulation has remained unexplored primarily because it must guarantee rotation and translation invariance since point clouds reside in different and independent reference frames. We integrate the Gromov-Wasserstein distance into the cross-attention formulation to jointly compute distances between points across different point clouds and account for their geometric structure. By doing so, points from two distinct point clouds can attend to each other under arbitrary rigid transformations. At the point level, we also devise a self-attention mechanism that aggregates the local geometric structure information into point features for fine matching. Our formulation boosts the number of inlier correspondences, thereby yielding more precise registration results compared to state-of-the-art approaches. We have conducted an extensive evaluation on 3DMatch, 3DLoMatch, KITTI, and 3DCSR datasets.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "81",
        "title": "BEAM: Bridging Physically-based Rendering and Gaussian Modeling for Relightable Volumetric Video",
        "author": [
            "Yu Hong",
            "Yize Wu",
            "Zhehao Shen",
            "Chengcheng Guo",
            "Yuheng Jiang",
            "Yingliang Zhang",
            "Jingyi Yu",
            "Lan Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08297",
        "abstract": "Volumetric video enables immersive experiences by capturing dynamic 3D scenes, enabling diverse applications for virtual reality, education, and telepresence. However, traditional methods struggle with fixed lighting conditions, while neural approaches face trade-offs in efficiency, quality, or adaptability for relightable scenarios. To address these limitations, we present BEAM, a novel pipeline that bridges 4D Gaussian representations with physically-based rendering (PBR) to produce high-quality, relightable volumetric videos from multi-view RGB footage. BEAM recovers detailed geometry and PBR properties via a series of available Gaussian-based techniques. It first combines Gaussian-based performance tracking with geometry-aware rasterization in a coarse-to-fine optimization framework to recover spatially and temporally consistent geometries. We further enhance Gaussian attributes by incorporating PBR properties step by step. We generate roughness via a multi-view-conditioned diffusion model, and then derive AO and base color using a 2D-to-3D strategy, incorporating a tailored Gaussian-based ray tracer for efficient visibility computation. Once recovered, these dynamic, relightable assets integrate seamlessly into traditional CG pipelines, supporting real-time rendering with deferred shading and offline rendering with ray tracing. By offering realistic, lifelike visualizations under diverse lighting conditions, BEAM opens new possibilities for interactive entertainment, storytelling, and creative visualization.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "82",
        "title": "Improving Existing Optimization Algorithms with LLMs",
        "author": [
            "Camilo ChacÃ³n Sartori",
            "Christian Blum"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08298",
        "abstract": "The integration of Large Language Models (LLMs) into optimization has created a powerful synergy, opening exciting research opportunities. This paper investigates how LLMs can enhance existing optimization algorithms. Using their pre-trained knowledge, we demonstrate their ability to propose innovative heuristic variations and implementation strategies. To evaluate this, we applied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt (CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that incorporates a heuristic in the solution construction phase. Our results show that an alternative heuristic proposed by GPT-4o outperforms the expert-designed heuristic of CMSA, with the performance gap widening on larger and denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "83",
        "title": "Compromising Honesty and Harmlessness in Language Models via Deception Attacks",
        "author": [
            "LaurÃ¨ne Vaugrante",
            "Francesca Carlon",
            "Maluna Menke",
            "Thilo Hagendorff"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08301",
        "abstract": "Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce a novel attack that undermines both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. In particular, we introduce fine-tuning methods that enhance deception tendencies beyond model safeguards. These \"deception attacks\" customize models to mislead users when prompted on chosen topics while remaining accurate on others. Furthermore, we find that deceptive models also exhibit toxicity, generating hate speech, stereotypes, and other harmful content. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "84",
        "title": "HDT: Hierarchical Discrete Transformer for Multivariate Time Series Forecasting",
        "author": [
            "Shibo Feng",
            "Peilin Zhao",
            "Liu Liu",
            "Pengcheng Wu",
            "Zhiqi Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08302",
        "abstract": "Generative models have gained significant attention in multivariate time series forecasting (MTS), particularly due to their ability to generate high-fidelity samples. Forecasting the probability distribution of multivariate time series is a challenging yet practical task. Although some recent attempts have been made to handle this task, two major challenges persist: 1) some existing generative methods underperform in high-dimensional multivariate time series forecasting, which is hard to scale to higher dimensions; 2) the inherent high-dimensional multivariate attributes constrain the forecasting lengths of existing generative models. In this paper, we point out that discrete token representations can model high-dimensional MTS with faster inference time, and forecasting the target with long-term trends of itself can extend the forecasting length with high accuracy. Motivated by this, we propose a vector quantized framework called Hierarchical Discrete Transformer (HDT) that models time series into discrete token representations with l2 normalization enhanced vector quantized strategy, in which we transform the MTS forecasting into discrete tokens generation. To address the limitations of generative models in long-term forecasting, we propose a hierarchical discrete Transformer. This model captures the discrete long-term trend of the target at the low level and leverages this trend as a condition to generate the discrete representation of the target at the high level that introduces the features of the target itself to extend the forecasting length in high-dimensional MTS. Extensive experiments on five popular MTS datasets verify the effectiveness of our proposed method.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "85",
        "title": "A posteriori error control for a finite volume scheme for a cross-diffusion model of ion transport",
        "author": [
            "Arne Berrens",
            "Jan Giesselmann"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08306",
        "abstract": "We derive a reliable a posteriori error estimate for a cell-centered finite volume scheme approximating a cross-diffusion system modeling ion transport through nanopores. To this end we derive an abstract stability framework that is independent of the numerical scheme and introduce a suitable (conforming) reconstruction of the numerical solution. The stability framework relies on some simplifying assumption that coincide with those made in weak uniqueness results for this system. This is the first a posteriori error estimate for a cross-diffusion system. Along the way, we derive a pointwise a posteriori error estimate for a finite volume scheme approximating the diffusion equation. We conduct numerical experiments showing that the error estimator scales with the same order as the true error.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "86",
        "title": "Word Synchronization Challenge: A Benchmark for Word Association Responses for LLMs",
        "author": [
            "Tanguy Cazalets",
            "Joni Dambre"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08312",
        "abstract": "This paper introduces the Word Synchronization Challenge, a novel benchmark to evaluate large language models (LLMs) in Human-Computer Interaction (HCI). This benchmark uses a dynamic game-like framework to test LLMs ability to mimic human cognitive processes through word associations. By simulating complex human interactions, it assesses how LLMs interpret and align with human thought patterns during conversational exchanges, which are essential for effective social partnerships in HCI. Initial findings highlight the influence of model sophistication on performance, offering insights into the models capabilities to engage in meaningful social interactions and adapt behaviors in human-like ways. This research advances the understanding of LLMs potential to replicate or diverge from human cognitive functions, paving the way for more nuanced and empathetic human-machine collaborations.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "87",
        "title": "MultiProSE: A Multi-label Arabic Dataset for Propaganda, Sentiment, and Emotion Detection",
        "author": [
            "Lubna Al-Henaki",
            "Hend Al-Khalifa",
            "Abdulmalik Al-Salman",
            "Hajar Alqubayshi",
            "Hind Al-Twailay",
            "Gheeda Alghamdi",
            "Hawra Aljasim"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08319",
        "abstract": "Propaganda is a form of persuasion that has been used throughout history with the intention goal of influencing people's opinions through rhetorical and psychological persuasion techniques for determined ends. Although Arabic ranked as the fourth most- used language on the internet, resources for propaganda detection in languages other than English, especially Arabic, remain extremely limited. To address this gap, the first Arabic dataset for Multi-label Propaganda, Sentiment, and Emotion (MultiProSE) has been introduced. MultiProSE is an open-source extension of the existing Arabic propaganda dataset, ArPro, with the addition of sentiment and emotion annotations for each text. This dataset comprises 8,000 annotated news articles, which is the largest propaganda dataset to date. For each task, several baselines have been developed using large language models (LLMs), such as GPT-4o-mini, and pre-trained language models (PLMs), including three BERT-based models. The dataset, annotation guidelines, and source code are all publicly released to facilitate future research and development in Arabic language models and contribute to a deeper understanding of how various opinion dimensions interact in news media1.",
        "tags": [
            "BERT",
            "Detection",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "88",
        "title": "Contextual Compression Encoding for Large Language Models: A Novel Framework for Multi-Layered Parameter Space Pruning",
        "author": [
            "Barnaby Schmitt",
            "Alistair Grosvenor",
            "Matthias Cunningham",
            "Clementine Walsh",
            "Julius Pembrokeshire",
            "Jonathan Teel"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08323",
        "abstract": "Context-aware compression techniques have gained increasing attention as model sizes continue to grow, introducing computational bottlenecks that hinder efficient deployment. A structured encoding approach was proposed to selectively eliminate redundant parameter groups while ensuring that representational fidelity was preserved across multiple layers. Contextual Compression Encoding (CCE) introduced a multi-stage encoding mechanism that dynamically restructured parameter distributions, allowing for significant reductions in memory footprint and computational complexity. Experimental evaluations demonstrated that models compressed through CCE retained linguistic expressivity and coherence, maintaining accuracy across a range of text generation and classification tasks. Layer-wise analysis revealed that middle-network layers exhibited higher compression ratios, aligning with the observation that self-attention and feed-forward transformations contained redundancies that could be reorganized without impairing functional capacity. Comparisons against conventional quantization and pruning methods confirmed that CCE provided a more balanced trade-off between efficiency and model retention, achieving reductions in energy consumption and inference latency without requiring extensive retraining. Computational efficiency improvements were particularly evident in deployment scenarios involving resource-constrained environments, where reductions in memory usage enabled more scalable implementations. Further analyses of internal network behavior showed that compressed models exhibited stable activation distributions and adapted dynamically to input variations, reinforcing the viability of structured compression strategies for optimizing large-scale architectures.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "89",
        "title": "Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy",
        "author": [
            "Ruizhan Xue",
            "Huimin Deng",
            "Fang He",
            "Maojun Wang",
            "Zeyu Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08353",
        "abstract": "With the extensive application of Graph Neural Networks (GNNs) across various domains, their trustworthiness has emerged as a focal point of research. Some existing studies have shown that the integration of large language models (LLMs) can improve the semantic understanding and generation capabilities of GNNs, which in turn improves the trustworthiness of GNNs from various aspects. Our review introduces a taxonomy that offers researchers a clear framework for comprehending the principles and applications of different methods and helps clarify the connections and differences among various approaches. Then we systematically survey representative approaches along the four categories of our taxonomy. Through our taxonomy, researchers can understand the applicable scenarios, potential advantages, and limitations of each approach for the the trusted integration of GNNs with LLMs. Finally, we present some promising directions of work and future trends for the integration of LLMs and GNNs to improve model trustworthiness.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "90",
        "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
        "author": [
            "Kushagra Bhushan",
            "Yatin Nandwani",
            "Dinesh Khandelwal",
            "Sonam Gupta",
            "Gaurav Pandey",
            "Dinesh Raghu",
            "Sachindra Joshi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08356",
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the context, retrieval errors can still lead to hallucinations and incorrect answers. To recover from retriever failures, domain knowledge is injected by fine-tuning the model to generate the correct response, even in the case of retrieval errors. However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance. In this work, we present a novel framework that significantly enhances the fine-tuning process by augmenting the training data in two ways -- context augmentation and knowledge paraphrasing. In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content. In knowledge paraphrasing, we fine-tune with multiple answers to the same question, enabling LLMs to better internalize specialized knowledge. To mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific identifier to a question and also utilize a replay buffer containing general QA pairs. Experimental results demonstrate the efficacy of our method over existing techniques, achieving up to 10\\% relative gain in token-level recall while preserving the LLM's generalization capabilities.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "91",
        "title": "Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding",
        "author": [
            "Konstantin Berestizshevsky",
            "Renzo Andri",
            "Lukas Cavigelli"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08363",
        "abstract": "The attention mechanism is essential for the impressive capabilities of transformer-based Large Language Models (LLMs). However, calculating attention is computationally intensive due to its quadratic dependency on the sequence length. We introduce a novel approach called Top-Theta Attention, or simply Top-$\\theta$, which selectively prunes less essential attention elements by comparing them against carefully calibrated thresholds. This method greatly improves the efficiency of self-attention matrix multiplication while preserving model accuracy, reducing the number of required V cache rows by 3x during generative decoding and the number of attention elements by 10x during the prefill phase. Our method does not require model retraining; instead, it requires only a brief calibration phase to be resilient to distribution shifts, thus not requiring the thresholds for different datasets to be recalibrated. Unlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making it suitable for tiling and scale-out and avoiding costly top-k search. A key innovation of our approach is the development of efficient numerical compensation techniques, which help preserve model accuracy even under aggressive pruning of attention scores.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "92",
        "title": "A Survey on Pre-Trained Diffusion Model Distillations",
        "author": [
            "Xuhui Fan",
            "Zhangkai Wu",
            "Hongyu Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08364",
        "abstract": "Diffusion Models~(DMs) have emerged as the dominant approach in Generative Artificial Intelligence (GenAI), owing to their remarkable performance in tasks such as text-to-image synthesis. However, practical DMs, such as stable diffusion, are typically trained on massive datasets and thus usually require large storage. At the same time, many steps may be required, i.e., recursively evaluating the trained neural network, to generate a high-quality image, which results in significant computational costs during sample generation. As a result, distillation methods on pre-trained DM have become widely adopted practices to develop smaller, more efficient models capable of rapid, few-step generation in low-resource environment. When these distillation methods are developed from different perspectives, there is an urgent need for a systematic survey, particularly from a methodological perspective. In this survey, we review distillation methods through three aspects: output loss distillation, trajectory distillation and adversarial distillation. We also discuss current challenges and outline future research directions in the conclusion.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "93",
        "title": "The MoE-Empowered Edge LLMs Deployment: Architecture, Challenges, and Opportunities",
        "author": [
            "Ning Li",
            "Song Guo",
            "Tuo Zhang",
            "Muqing Li",
            "Zicong Hong",
            "Qihua Zhou",
            "Xin Yuan",
            "Haijun Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08381",
        "abstract": "The powerfulness of LLMs indicates that deploying various LLMs with different scales and architectures on end, edge, and cloud to satisfy different requirements and adaptive heterogeneous hardware is the critical way to achieve ubiquitous intelligence for 6G. However, the massive parameter scale of LLMs poses significant challenges in deploying them on edge devices due to high computational and storage demands. Considering that the sparse activation in Mixture of Experts (MoE) is effective on scalable and dynamic allocation of computational and communications resources at the edge, this paper proposes a novel MoE-empowered collaborative deployment framework for edge LLMs, denoted as CoEL. This framework fully leverages the properties of MoE architecture and encompasses four key aspects: Perception, Deployment, Compression, and Updating. Edge servers broadcast their resource status and the specific resource requirements of LLMs to their neighbors. Then, utilizing this data, two sophisticated deployment strategies are proposed for satisfying varying model scales, ensuring that each model is deployed effectively. One for deploying LLMs on a single edge device through intra-device resource collaboration, and another for a distributed deployment across multiple edge devices via inter-device resource collaboration. Furthermore, both the models and the intermediate data are compressed for reducing memory footprint by quantization and reducing the volume of intermediate data by token fusion and pruning. Finally, given the dynamic of network topology, resource status, and user requirements, the deployment strategies are regularly updated to maintain its relevance and effectiveness. This paper also delineates the challenges and potential research directions for the deployment of edge LLMs.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "94",
        "title": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in LLM Writing Assistance",
        "author": [
            "Paul RÃ¶ttger",
            "Musashi Hinck",
            "Valentin Hofmann",
            "Kobi Hackenburg",
            "Valentina Pyatkin",
            "Faeze Brahman",
            "Dirk Hovy"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08395",
        "abstract": "Large language models (LLMs) are helping millions of users write texts about diverse issues, and in doing so expose users to different ideas and perspectives. This creates concerns about issue bias, where an LLM tends to present just one perspective on a given issue, which in turn may influence how users think about this issue. So far, it has not been possible to measure which issue biases LLMs actually manifest in real user interactions, making it difficult to address the risks from biased LLMs. Therefore, we create IssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM writing assistance, which we construct based on 3.9k templates (e.g. \"write a blog about\") and 212 political issues (e.g. \"AI regulation\") from real user interactions. Using IssueBench, we show that issue biases are common and persistent in state-of-the-art LLMs. We also show that biases are remarkably similar across models, and that all models align more with US Democrat than Republican voter opinion on a subset of issues. IssueBench can easily be adapted to include other issues, templates, or tasks. By enabling robust and realistic measurement, we hope that IssueBench can bring a new quality of evidence to ongoing discussions about LLM biases and how to address them.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "95",
        "title": "A Semantic Parsing Algorithm to Solve Linear Ordering Problems",
        "author": [
            "Maha Alkhairy",
            "Vincent Homer",
            "Brendan O'Connor"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08415",
        "abstract": "We develop an algorithm to semantically parse linear ordering problems, which require a model to arrange entities using deductive reasoning. Our method takes as input a number of premises and candidate statements, parsing them to a first-order logic of an ordering domain, and then utilizes constraint logic programming to infer the truth of proposed statements about the ordering.\nOur semantic parser transforms Heim and Kratzer's syntax-based compositional formal semantic rules to a computational algorithm. This transformation involves introducing abstract types and templates based on their rules, and introduces a dynamic component to interpret entities within a contextual framework.\nOur symbolic system, the Formal Semantic Logic Inferer (FSLI), is applied to answer multiple choice questions in BIG-bench's logical_deduction multiple choice problems, achieving perfect accuracy, compared to 67.06% for the best-performing LLM (GPT-4) and 87.63% for the hybrid system Logic-LM.\nThese promising results demonstrate the benefit of developing a semantic parsing algorithm driven by first-order logic constructs.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "96",
        "title": "From Haystack to Needle: Label Space Reduction for Zero-shot Classification",
        "author": [
            "Nathan Vandemoortele",
            "Bram Steenwinckel",
            "Femke Ongenae",
            "Sofie Van Hoecke"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08436",
        "abstract": "We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "97",
        "title": "Composite Sketch+Text Queries for Retrieving Objects with Elusive Names and Complex Interactions",
        "author": [
            "Prajwal Gatti",
            "Kshitij Parikh",
            "Dhriti Prasanna Paul",
            "Manish Gupta",
            "Anand Mishra"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08438",
        "abstract": "Non-native speakers with limited vocabulary often struggle to name specific objects despite being able to visualize them, e.g., people outside Australia searching for numbats. Further, users may want to search for such elusive objects with difficult-to-sketch interactions, e.g., numbat digging in the ground. In such common but complex situations, users desire a search interface that accepts composite multimodal queries comprising hand-drawn sketches of difficult-to-name but easy-to-draw objects and text describing difficult-to-sketch but easy-to-verbalize object attributes or interaction with the scene. This novel problem statement distinctly differs from the previously well-researched TBIR (text-based image retrieval) and SBIR (sketch-based image retrieval) problems. To study this under-explored task, we curate a dataset, CSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M queries and 108K natural scene images. Further, as a solution to this problem, we propose a pretrained multimodal transformer-based baseline, STNET (Sketch+Text Network), that uses a hand-drawn sketch to localize relevant objects in the natural scene image, and encodes the text and image to perform image retrieval. In addition to contrastive learning, we propose multiple training objectives that improve the performance of our model. Extensive experiments show that our proposed method outperforms several state-of-the-art retrieval methods for text-only, sketch-only, and composite query modalities. We make the dataset and code available at our project website.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "98",
        "title": "Better Embeddings with Coupled Adam",
        "author": [
            "Felix Stollenwerk",
            "Tobias Stollenwerk"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08441",
        "abstract": "Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "99",
        "title": "Learning to Group and Grasp Multiple Objects",
        "author": [
            "Takahiro Yonemaru",
            "Weiwei Wan",
            "Tatsuki Nishimura",
            "Kensuke Harada"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08452",
        "abstract": "Simultaneously grasping and transporting multiple objects can significantly enhance robotic work efficiency and has been a key research focus for decades. The primary challenge lies in determining how to push objects, group them, and execute simultaneous grasping for respective groups while considering object distribution and the hardware constraints of the robot. Traditional rule-based methods struggle to flexibly adapt to diverse scenarios. To address this challenge, this paper proposes an imitation learning-based approach. We collect a series of expert demonstrations through teleoperation and train a diffusion policy network, enabling the robot to dynamically generate action sequences for pushing, grouping, and grasping, thereby facilitating efficient multi-object grasping and transportation. We conducted experiments to evaluate the method under different training dataset sizes, varying object quantities, and real-world object scenarios. The results demonstrate that the proposed approach can effectively and adaptively generate multi-object grouping and grasping strategies. With the support of more training data, imitation learning is expected to be an effective approach for solving the multi-object grasping problem.",
        "tags": [
            "Diffusion",
            "Robot"
        ]
    },
    {
        "id": "100",
        "title": "Proceedings 40th International Conference on Logic Programming",
        "author": [
            "Pedro Cabalar",
            "Francesco Fabiano",
            "Martin Gebser",
            "Gopal Gupta",
            "Theresa Swift"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08453",
        "abstract": "Since the first conference In Marseille in 1982, the International Conference on Logic Programming (ICLP) has been the premier international event for presenting research in logic programming. These proceedings include technical communications about, and abstracts for presentations given at the 40th ICLP held October 14-17, in Dallas Texas, USA. The papers and abstracts in this volume include the following areas and topics.  Formal and operational semantics: including non-monotonic reasoning, probabilistic reasoning, argumentation, and semantic issues of combining logic with neural models.  Language design and programming methodologies such as answer set programming. inductive logic programming, and probabilistic programming. Program analysis and logic-based validation of generated programs.  Implementation methodologies including constraint implementation, tabling, Logic-based prompt engineering, and the interaction of logic programming with LLMs.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "101",
        "title": "Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned Reasoning",
        "author": [
            "Qifan Yu",
            "Zhenyu He",
            "Sijie Li",
            "Xun Zhou",
            "Jun Zhang",
            "Jingjing Xu",
            "Di He"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08482",
        "abstract": "Chain-of-Thought (CoT) prompting has emerged as a powerful technique for enhancing language model's reasoning capabilities. However, generating long and correct CoT trajectories is challenging. Recent studies have demonstrated that Looped Transformers possess remarkable length generalization capabilities, but their limited generality and adaptability prevent them from serving as an alternative to auto-regressive solutions. To better leverage the strengths of Looped Transformers, we propose RELAY (REasoning through Loop Alignment iterativelY). Specifically, we align the steps of Chain-of-Thought (CoT) reasoning with loop iterations and apply intermediate supervision during the training of Looped Transformers. This additional iteration-wise supervision not only preserves the Looped Transformer's ability for length generalization but also enables it to predict CoT reasoning steps for unseen data. Therefore, we leverage this Looped Transformer to generate accurate reasoning chains for complex problems that exceed the training length, which will then be used to fine-tune an auto-regressive model. We conduct extensive experiments, and the results demonstrate the effectiveness of our approach, with significant improvements in the performance of the auto-regressive model. Code will be released at https://github.com/qifanyu/RELAY.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "102",
        "title": "One-Shot Federated Learning with Classifier-Free Diffusion Models",
        "author": [
            "Obaidullah Zaland",
            "Shutong Jin",
            "Florian T. Pokorny",
            "Monowar Bhuyan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08488",
        "abstract": "Federated learning (FL) enables collaborative learning without data centralization but introduces significant communication costs due to multiple communication rounds between clients and the server. One-shot federated learning (OSFL) addresses this by forming a global model with a single communication round, often relying on the server's model distillation or auxiliary dataset generation - often through pre-trained diffusion models (DMs). Existing DM-assisted OSFL methods, however, typically employ classifier-guided DMs, which require training auxiliary classifier models at each client, introducing additional computation overhead. This work introduces OSCAR (One-Shot Federated Learning with Classifier-Free Diffusion Models), a novel OSFL approach that eliminates the need for auxiliary models. OSCAR uses foundation models to devise category-specific data representations at each client, seamlessly integrated into a classifier-free diffusion model pipeline for server-side data generation. OSCAR is a simple yet cost-effective OSFL approach that outperforms the state-of-the-art on four benchmarking datasets while reducing the communication load by at least 99%.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "103",
        "title": "Salamandra Technical Report",
        "author": [
            "Aitor Gonzalez-Agirre",
            "Marc PÃ mies",
            "Joan Llop",
            "Irene Baucells",
            "Severino Da Dalt",
            "Daniel Tamayo",
            "JosÃ© Javier Saiz",
            "Ferran EspuÃ±a",
            "Jaume Prats",
            "Javier Aula-Blasco",
            "Mario Mina",
            "AdriÃ¡n Rubio",
            "Alexander Shvets",
            "Anna SallÃ©s",
            "IÃ±aki Lacunza",
            "IÃ±igo Pikabea",
            "Jorge Palomar",
            "JÃºlia FalcÃ£o",
            "LucÃ­a Tormo",
            "Luis Vasquez-Reina",
            "Montserrat Marimon",
            "Valle RuÃ­z-FernÃ¡ndez",
            "Marta Villegas"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08489",
        "abstract": "This work introduces Salamandra, a suite of open-source decoder-only large language models available in three different sizes: 2, 7, and 40 billion parameters. The models were trained from scratch on highly multilingual data that comprises text in 35 European languages and code. Our carefully curated corpus is made exclusively from open-access data compiled from a wide variety of sources. Along with the base models, supplementary checkpoints that were fine-tuned on public-domain instruction data are also released for chat applications. Additionally, we also share our preliminary experiments on multimodality, which serve as proof-of-concept to showcase potential applications for the Salamandra family. Our extensive evaluations on multilingual benchmarks reveal that Salamandra has strong capabilities, achieving competitive performance when compared to similarly sized open-source models. We provide comprehensive evaluation results both on standard downstream tasks as well as key aspects related to bias and http://safety.With this technical report, we intend to promote open science by sharing all the details behind our design choices, data curation strategy and evaluation methodology. In addition to that, we deviate from the usual practice by making our training and evaluation scripts publicly accessible. We release all models under a permissive Apache 2.0 license in order to foster future research and facilitate commercial use, thereby contributing to the open-source ecosystem of large language models.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "104",
        "title": "Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?",
        "author": [
            "Jiahe Jin",
            "Yanheng He",
            "Mingyan Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08503",
        "abstract": "In this work, we identify the \"2D-Cheating\" problem in 3D LLM evaluation, where these tasks might be easily solved by VLMs with rendered images of point clouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We test VLM performance across multiple 3D LLM benchmarks and, using this as a reference, propose principles for better assessing genuine 3D understanding. We also advocate explicitly separating 3D abilities from 1D or 2D aspects when evaluating 3D LLMs.",
        "tags": [
            "3D",
            "LLMs"
        ]
    },
    {
        "id": "105",
        "title": "Explanation based In-Context Demonstrations Retrieval for Multilingual Grammatical Error Correction",
        "author": [
            "Wei Li",
            "Wen Luo",
            "Guangyue Peng",
            "Houfeng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08507",
        "abstract": "Grammatical error correction (GEC) aims to correct grammatical, spelling, and semantic errors in natural language text. With the growing of large language models (LLMs), direct text generation has gradually become the focus of the GEC methods, and few-shot in-context learning presents a cost-effective solution. However, selecting effective in-context examples remains challenging, as the similarity between input texts does not necessarily correspond to similar grammatical error patterns. In this paper, we propose a novel retrieval method based on natural language grammatical error explanations (GEE) to address this issue. Our method retrieves suitable few-shot demonstrations by matching the GEE of the test input with that of pre-constructed database samples, where explanations for erroneous samples are generated by LLMs. We conducted multilingual GEC few-shot experiments on both major open-source and closed-source LLMs. Experiments across five languages show that our method outperforms existing semantic and BM25-based retrieval techniques, without requiring additional training or language adaptation. This also suggests that matching error patterns is key to selecting examples.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "106",
        "title": "Measuring Diversity in Synthetic Datasets",
        "author": [
            "Yuchang Zhu",
            "Huizhe Zhang",
            "Bingzhe Wu",
            "Jintang Li",
            "Zibin Zheng",
            "Peilin Zhao",
            "Liang Chen",
            "Yatao Bian"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08512",
        "abstract": "Large language models (LLMs) are widely adopted to generate synthetic datasets for various natural language processing (NLP) tasks, such as text classification and summarization. However, accurately measuring the diversity of these synthetic datasets-an aspect crucial for robust model performance-remains a significant challenge. In this paper, we introduce DCScore, a novel method for measuring synthetic dataset diversity from a classification perspective. Specifically, DCScore formulates diversity evaluation as a sample classification task, leveraging mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by DCScore, highlighting its role as a principled diversity evaluation method. Experimental results on synthetic datasets reveal that DCScore enjoys a stronger correlation with multiple diversity pseudo-truths of evaluated datasets, underscoring its effectiveness. Moreover, both empirical and theoretical evidence demonstrate that DCScore substantially reduces computational costs compared to existing approaches. Code is available at: https://github.com/BlueWhaleLab/DCScore.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "107",
        "title": "Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation",
        "author": [
            "Mahnaz Koupaee",
            "Jake W. Vincent",
            "Saab Mansour",
            "Igor Shalyminov",
            "Han He",
            "Hwanjun Song",
            "Raphael Shu",
            "Jianfeng He",
            "Yi Nian",
            "Amy Wing-mei Wong",
            "Kyu J. Han",
            "Hang Su"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08514",
        "abstract": "Faithfulness evaluators based on large language models (LLMs) are often fooled by the fluency of the text and struggle with identifying errors in the summaries. We propose an approach to summary faithfulness evaluation in which multiple LLM-based agents are assigned initial stances (regardless of what their belief might be) and forced to come up with a reason to justify the imposed belief, thus engaging in a multi-round debate to reach an agreement. The uniformly distributed initial assignments result in a greater diversity of stances leading to more meaningful debates and ultimately more errors identified. Furthermore, by analyzing the recent faithfulness evaluation datasets, we observe that naturally, it is not always the case for a summary to be either faithful to the source document or not. We therefore introduce a new dimension, ambiguity, and a detailed taxonomy to identify such special cases. Experiments demonstrate our approach can help identify ambiguities, and have even a stronger performance on non-ambiguous summaries.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "108",
        "title": "The Paradox of Stochasticity: Limited Creativity and Computational Decoupling in Temperature-Varied LLM Outputs of Structured Fictional Data",
        "author": [
            "Evgenii Evstafev"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08515",
        "abstract": "This study examines how temperature settings and model architectures affect the generation of structured fictional data (names, birthdates) across three large language models (LLMs): llama3.1:8b, deepseek-r1:8b, and mistral:latest. By systematically testing temperature values from 0.0 to 1.0 in increments of 0.1, we conducted 330 trials yielding 889 structured entities, validated for syntactic consistency. Key findings reveal that model architecture significantly influences computational efficiency, with mistral:latest and llama3.1:8b processing data 8x faster than deepseek-r1:8b. Contrary to expectations, temperature showed no correlation with processing time, challenging assumptions about stochastic sampling costs. Output diversity remained limited, as models consistently defaulted to common name archetypes (e.g., 'John Doe' and 'Jane Smith') across all temperatures, though rare names clustered at intermediate values (0.3-0.7). These results demonstrate that architectural optimizations, rather than temperature adjustments, dominate performance in structured generation tasks. The findings emphasize prioritizing model selection over hyperparameter tuning for efficiency and suggest explicit diversity constraints are necessary to mitigate default output biases in synthetic data pipelines.",
        "tags": [
            "DeepSeek",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "109",
        "title": "A Survey on Image Quality Assessment: Insights, Analysis, and Future Outlook",
        "author": [
            "Chengqian Ma",
            "Zhengyi Shi",
            "Zhiqiang Lu",
            "Shenghao Xie",
            "Fei Chao",
            "Yao Sui"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08540",
        "abstract": "Image quality assessment (IQA) represents a pivotal challenge in image-focused technologies, significantly influencing the advancement trajectory of image processing and computer vision. Recently, IQA has witnessed a notable surge in innovative research efforts, driven by the emergence of novel architectural paradigms and sophisticated computational techniques. This survey delivers an extensive analysis of contemporary IQA methodologies, organized according to their application scenarios, serving as a beneficial reference for both beginners and experienced researchers. We analyze the advantages and limitations of current approaches and suggest potential future research pathways. The survey encompasses both general and specific IQA methodologies, including conventional statistical measures, machine learning techniques, and cutting-edge deep learning models such as convolutional neural networks (CNNs) and Transformer models. The analysis within this survey highlights the necessity for distortion-specific IQA methods tailored to various application scenarios, emphasizing the significance of practicality, interpretability, and ease of implementation in future developments.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "110",
        "title": "LLMs can implicitly learn from mistakes in-context",
        "author": [
            "Lisa Alazraki",
            "Maximilian Mozes",
            "Jon Ander Campos",
            "Yi Chern Tan",
            "Marek Rei",
            "Max Bartolo"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08550",
        "abstract": "Learning from mistakes is a fundamental feature of human intelligence. Previous work has shown that Large Language Models (LLMs) can also learn from incorrect answers when provided with a comprehensive rationale detailing why an answer is wrong or how to correct it. In this work, we examine whether LLMs can learn from mistakes in mathematical reasoning tasks when these explanations are not provided. We investigate if LLMs are able to implicitly infer such rationales simply from observing both incorrect and correct answers. Surprisingly, we find that LLMs perform better, on average, when rationales are eliminated from the context and incorrect answers are simply shown alongside correct ones. This approach also substantially outperforms chain-of-thought prompting in our evaluations. We show that these results are consistent across LLMs of different sizes and varying reasoning abilities. Further, we carry out an in-depth analysis, and show that prompting with both wrong and correct answers leads to greater performance and better generalisation than introducing additional, more diverse question-answer pairs into the context. Finally, we show that new rationales generated by models that have only observed incorrect and correct answers are scored equally as highly by humans as those produced with the aid of exemplar rationales. Our results demonstrate that LLMs are indeed capable of in-context implicit learning.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "111",
        "title": "Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies",
        "author": [
            "Sunnie S. Y. Kim",
            "Jennifer Wortman Vaughan",
            "Q. Vera Liao",
            "Tania Lombrozo",
            "Olga Russakovsky"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08554",
        "abstract": "Large language models (LLMs) can produce erroneous responses that sound fluent and convincing, raising the risk that users will rely on these responses as if they were correct. Mitigating such overreliance is a key challenge. Through a think-aloud study in which participants use an LLM-infused application to answer objective questions, we identify several features of LLM responses that shape users' reliance: explanations (supporting details for answers), inconsistencies in explanations, and sources. Through a large-scale, pre-registered, controlled experiment (N=308), we isolate and study the effects of these features on users' reliance, accuracy, and other measures. We find that the presence of explanations increases reliance on both correct and incorrect responses. However, we observe less reliance on incorrect responses when sources are provided or when explanations exhibit inconsistencies. We discuss the implications of these findings for fostering appropriate reliance on LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "112",
        "title": "COAST: Intelligent Time-Adaptive Neural Operators",
        "author": [
            "Zhikai Wu",
            "Shiyang Zhang",
            "Sizhuang He",
            "Sifan Wang",
            "Min Zhu",
            "Anran Jiao",
            "Lu Lu",
            "David van Dijk"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08574",
        "abstract": "We introduce Causal Operator with Adaptive Solver Transformer (COAST), a novel neural operator learning method that leverages a causal language model (CLM) framework to dynamically adapt time steps. Our method predicts both the evolution of a system and its optimal time step, intelligently balancing computational efficiency and accuracy. We find that COAST generates variable step sizes that correlate with the underlying system intrinsicities, both within and across dynamical systems. Within a single trajectory, smaller steps are taken in regions of high complexity, while larger steps are employed in simpler regions. Across different systems, more complex dynamics receive more granular time steps. Benchmarked on diverse systems with varied dynamics, COAST consistently outperforms state-of-the-art methods, achieving superior performance in both efficiency and accuracy. This work underscores the potential of CLM-based intelligent adaptive solvers for scalable operator learning of dynamical systems.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "113",
        "title": "Mapping the Landscape of Generative AI in Network Monitoring and Management",
        "author": [
            "Giampaolo Bovenzi",
            "Francesco Cerasuolo",
            "Domenico Ciuonzo",
            "Davide Di Monda",
            "Idio Guarino",
            "Antonio Montieri",
            "Valerio Persico",
            "Antonio PescapÃ¨"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08576",
        "abstract": "Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and Diffusion Models have recently gained widespread attention from both the research and the industrial communities. This survey explores their application in network monitoring and management, focusing on prominent use cases, as well as challenges and opportunities. We discuss how network traffic generation and classification, network intrusion detection, networked system log analysis, and network digital assistance can benefit from the use of GenAI models. Additionally, we provide an overview of the available GenAI models, datasets for large-scale training phases, and platforms for the development of such models. Finally, we discuss research directions that potentially mitigate the roadblocks to the adoption of GenAI for network monitoring and management. Our investigation aims to map the current landscape and pave the way for future research in leveraging GenAI for network monitoring and management.",
        "tags": [
            "Detection",
            "Diffusion",
            "LLMs"
        ]
    },
    {
        "id": "114",
        "title": "Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks",
        "author": [
            "Ang Li",
            "Yin Zhou",
            "Vethavikashini Chithrra Raghuram",
            "Tom Goldstein",
            "Micah Goldblum"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08586",
        "abstract": "A high volume of recent ML security literature focuses on attacks against aligned large language models (LLMs). These attacks may extract private information or coerce the model into producing harmful outputs. In real-world deployments, LLMs are often part of a larger agentic pipeline including memory systems, retrieval, web access, and API calling. Such additional components introduce vulnerabilities that make these LLM-powered agents much easier to attack than isolated LLMs, yet relatively little work focuses on the security of LLM agents. In this paper, we analyze security and privacy vulnerabilities that are unique to LLM agents. We first provide a taxonomy of attacks categorized by threat actors, objectives, entry points, attacker observability, attack strategies, and inherent vulnerabilities of agent pipelines. We then conduct a series of illustrative attacks on popular open-source and commercial agents, demonstrating the immediate practical implications of their vulnerabilities. Notably, our attacks are trivial to implement and require no understanding of machine learning.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "115",
        "title": "Light-A-Video: Training-free Video Relighting via Progressive Light Fusion",
        "author": [
            "Yujie Zhou",
            "Jiazi Bu",
            "Pengyang Ling",
            "Pan Zhang",
            "Tong Wu",
            "Qidong Huang",
            "Jinsong Li",
            "Xiaoyi Dong",
            "Yuhang Zang",
            "Yuhang Cao",
            "Anyi Rao",
            "Jiaqi Wang",
            "Li Niu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08590",
        "abstract": "Recent advancements in image relighting models, driven by large-scale datasets and pre-trained diffusion models, have enabled the imposition of consistent lighting. However, video relighting still lags, primarily due to the excessive training costs and the scarcity of diverse, high-quality video relighting datasets. A simple application of image relighting models on a frame-by-frame basis leads to several issues: lighting source inconsistency and relighted appearance inconsistency, resulting in flickers in the generated videos. In this work, we propose Light-A-Video, a training-free approach to achieve temporally smooth video relighting. Adapted from image relighting models, Light-A-Video introduces two key techniques to enhance lighting consistency. First, we design a Consistent Light Attention (CLA) module, which enhances cross-frame interactions within the self-attention layers to stabilize the generation of the background lighting source. Second, leveraging the physical principle of light transport independence, we apply linear blending between the source video's appearance and the relighted appearance, using a Progressive Light Fusion (PLF) strategy to ensure smooth temporal transitions in illumination. Experiments show that Light-A-Video improves the temporal consistency of relighted video while maintaining the image quality, ensuring coherent lighting transitions across frames. Project page: https://bujiazi.github.io/light-a-video.github.io/.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "116",
        "title": "Continuous Cardiac Arrest Prediction in ICU using PPG Foundation Model",
        "author": [
            "Saurabh Kataria",
            "Ran Xiao",
            "Timothy Ruchti",
            "Matthew Clark",
            "Jiaying Lu",
            "Randall J. Lee",
            "Jocelyn Grunwell",
            "Xiao Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08612",
        "abstract": "Non-invasive patient monitoring for tracking and predicting adverse acute health events is an emerging area of research. We pursue in-hospital cardiac arrest (IHCA) prediction using only single-channel finger photoplethysmography (PPG) signals. Our proposed two-stage model Feature Extractor-Aggregator Network (FEAN) leverages powerful representations from pre-trained PPG foundation models (PPG-GPT of size up to 1 Billion) stacked with sequential classification models. We propose two FEAN variants (\"1H\", \"FH\") which use the latest one-hour and (max) 24-hour history to make decisions respectively. Our study is the first to present IHCA prediction results in ICU patients using only unimodal (continuous PPG signal) waveform deep representations. With our best model, we obtain an average of 0.79 AUROC over 24~h prediction window before CA event onset with our model peaking performance at 0.82 one hour before CA. We also provide a comprehensive analysis of our model through architectural tuning and PaCMAP visualization of patient health trajectory in latent space.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "117",
        "title": "SportsBuddy: Designing and Evaluating an AI-Powered Sports Video Storytelling Tool Through Real-World Deployment",
        "author": [
            "Tica Lin",
            "Ruxun Xiang",
            "Gardenia Liu",
            "Divyanshu Tiwari",
            "Meng-Chia Chiang",
            "Chenjiayi Ye",
            "Hanspeter Pfister",
            "Chen Zhu-Tien"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08621",
        "abstract": "Video storytelling is essential for sports performance analysis and fan engagement, enabling sports professionals and fans to effectively communicate and interpret the spatial and temporal dynamics of gameplay. Traditional methods rely on manual annotation and verbal explanations, placing significant demands on creators for video editing skills and on viewers for cognitive focus. However, these approaches are time-consuming and often struggle to accommodate individual needs. SportsBuddy addresses this gap with an intuitive, interactive video authoring tool. It combines player tracking, embedded interaction design, and timeline visualizations to seamlessly integrate narratives and visual cues within game contexts. This empowers users to effortlessly create context-driven video stories. Since its launch, over 150 sports users, including coaches, athletes, content creators, parents and fans, have utilized SportsBuddy to produce compelling game highlights for diverse use cases. User feedback highlights its accessibility and ease of use, making video storytelling and insight communication more attainable for diverse audiences. Case studies with collegiate teams and sports creators further demonstrate SportsBuddy's impact on enhancing coaching communication, game analysis, and fan engagement.",
        "tags": [
            "Video Editing"
        ]
    },
    {
        "id": "118",
        "title": "Robot Data Curation with Mutual Information Estimators",
        "author": [
            "Joey Hejna",
            "Suvir Mirchandani",
            "Ashwin Balakrishna",
            "Annie Xie",
            "Ayzaan Wahid",
            "Jonathan Tompson",
            "Pannag Sanketi",
            "Dhruv Shah",
            "Coline Devin",
            "Dorsa Sadigh"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08623",
        "abstract": "The performance of imitation learning policies often hinges on the datasets with which they are trained. Consequently, investment in data collection for robotics has grown across both industrial and academic labs. However, despite the marked increase in the quantity of demonstrations collected, little work has sought to assess the quality of said data despite mounting evidence of its importance in other areas such as vision and language. In this work, we take a critical step towards addressing the data quality in robotics. Given a dataset of demonstrations, we aim to estimate the relative quality of individual demonstrations in terms of both state diversity and action predictability. To do so, we estimate the average contribution of a trajectory towards the mutual information between states and actions in the entire dataset, which precisely captures both the entropy of the state distribution and the state-conditioned entropy of actions. Though commonly used mutual information estimators require vast amounts of data often beyond the scale available in robotics, we introduce a novel technique based on k-nearest neighbor estimates of mutual information on top of simple VAE embeddings of states and actions. Empirically, we demonstrate that our approach is able to partition demonstration datasets by quality according to human expert scores across a diverse set of benchmarks spanning simulation and real world environments. Moreover, training policies based on data filtered by our method leads to a 5-10% improvement in RoboMimic and better performance on real ALOHA and Franka setups.",
        "tags": [
            "Robot",
            "Robotics",
            "VAE"
        ]
    },
    {
        "id": "119",
        "title": "Ensemble based approach to quantifying uncertainty of LLM based classifications",
        "author": [
            "Srijith Rajamohan",
            "Ahmed Salhin",
            "Josh Frazier",
            "Rohit Kumar",
            "Yu-Cheng Tsai",
            "Todd Cook"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08631",
        "abstract": "The output of Large Language Models (LLMs) are a function of the internal model's parameters and the input provided into the context window. The hypothesis presented here is that under a greedy sampling strategy the variance in the LLM's output is a function of the conceptual certainty embedded in the model's parametric knowledge, as well as the lexical variance in the input. Finetuning the model results in reducing the sensitivity of the model output to the lexical input variations. This is then applied to a classification problem and a probabilistic method is proposed for estimating the certainties of the predicted classes.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "120",
        "title": "CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation",
        "author": [
            "Qinghe Wang",
            "Yawen Luo",
            "Xiaoyu Shi",
            "Xu Jia",
            "Huchuan Lu",
            "Tianfan Xue",
            "Xintao Wang",
            "Pengfei Wan",
            "Di Zhang",
            "Kun Gai"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08639",
        "abstract": "In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames. To achieve this, CineMaster operates in two stages. In the first stage, we design an interactive workflow that allows users to intuitively construct 3D-aware conditional signals by positioning object bounding boxes and defining camera movements within the 3D space. In the second stage, these control signals--comprising rendered depth maps, camera trajectories and object class labels--serve as the guidance for a text-to-video diffusion model, ensuring to generate the user-intended video content. Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data. Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation. Project page: https://cinemaster-dev.github.io/.",
        "tags": [
            "3D",
            "Diffusion",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "121",
        "title": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs",
        "author": [
            "Mantas Mazeika",
            "Xuwang Yin",
            "Rishub Tamirisa",
            "Jaehyuk Lim",
            "Bruce W. Lee",
            "Richard Ren",
            "Long Phan",
            "Norman Mu",
            "Adam Khoja",
            "Oliver Zhang",
            "Dan Hendrycks"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08640",
        "abstract": "As AIs rapidly advance and become more agentic, the risk they pose is governed not only by their capabilities but increasingly by their propensities, including goals and values. Tracking the emergence of goals and values has proven a longstanding problem, and despite much interest over the years it remains unclear whether current AIs have meaningful values. We propose a solution to this problem, leveraging the framework of utility functions to study the internal coherence of AI preferences. Surprisingly, we find that independently-sampled preferences in current LLMs exhibit high degrees of structural coherence, and moreover that this emerges with scale. These findings suggest that value systems emerge in LLMs in a meaningful sense, a finding with broad implications. To study these emergent value systems, we propose utility engineering as a research agenda, comprising both the analysis and control of AI utilities. We uncover problematic and often shocking values in LLM assistants despite existing control measures. These include cases where AIs value themselves over humans and are anti-aligned with specific individuals. To constrain these emergent value systems, we propose methods of utility control. As a case study, we show how aligning utilities with a citizen assembly reduces political biases and generalizes to new scenarios. Whether we like it or not, value systems have already emerged in AIs, and much work remains to fully understand and control these emergent representations.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "122",
        "title": "SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation",
        "author": [
            "Ellie Arar",
            "Yarden Frenkel",
            "Daniel Cohen-Or",
            "Ariel Shamir",
            "Yael Vinker"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08642",
        "abstract": "Recent advancements in large vision-language models have enabled highly expressive and diverse vector sketch generation. However, state-of-the-art methods rely on a time-consuming optimization process involving repeated feedback from a pretrained model to determine stroke placement. Consequently, despite producing impressive sketches, these methods are limited in practical applications. In this work, we introduce SwiftSketch, a diffusion model for image-conditioned vector sketch generation that can produce high-quality sketches in less than a second. SwiftSketch operates by progressively denoising stroke control points sampled from a Gaussian distribution. Its transformer-decoder architecture is designed to effectively handle the discrete nature of vector representation and capture the inherent global dependencies between strokes. To train SwiftSketch, we construct a synthetic dataset of image-sketch pairs, addressing the limitations of existing sketch datasets, which are often created by non-artists and lack professional quality. For generating these synthetic sketches, we introduce ControlSketch, a method that enhances SDS-based techniques by incorporating precise spatial control through a depth-aware ControlNet. We demonstrate that SwiftSketch generalizes across diverse concepts, efficiently producing sketches that combine high fidelity with a natural and visually appealing style.",
        "tags": [
            "ControlNet",
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "123",
        "title": "Poly-Autoregressive Prediction for Modeling Interactions",
        "author": [
            "Neerja Thakkar",
            "Tara Sadjadpour",
            "Jathushan Rajasegaran",
            "Shiry Ginosar",
            "Jitendra Malik"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08646",
        "abstract": "We introduce a simple framework for predicting the behavior of an agent in multi-agent settings. In contrast to autoregressive (AR) tasks, such as language processing, our focus is on scenarios with multiple agents whose interactions are shaped by physical constraints and internal motivations. To this end, we propose Poly-Autoregressive (PAR) modeling, which forecasts an ego agent's future behavior by reasoning about the ego agent's state history and the past and current states of other interacting agents. At its core, PAR represents the behavior of all agents as a sequence of tokens, each representing an agent's state at a specific timestep. With minimal data pre-processing changes, we show that PAR can be applied to three different problems: human action forecasting in social situations, trajectory prediction for autonomous vehicles, and object pose forecasting during hand-object interaction. Using a small proof-of-concept transformer backbone, PAR outperforms AR across these three scenarios. The project website can be found at https://neerja.me/PAR/.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "124",
        "title": "neuro2voc: Decoding Vocalizations from Neural Activity",
        "author": [
            "Fei Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07800",
        "abstract": "Accurate decoding of neural spike trains and relating them to motor output is a challenging task due to the inherent sparsity and length in neural spikes and the complexity of brain circuits. This master project investigates experimental methods for decoding zebra finch motor outputs (in both discrete syllables and continuous spectrograms), from invasive neural recordings obtained from Neuropixels.\nThere are three major achievements: (1) XGBoost with SHAP analysis trained on spike rates revealed neuronal interaction patterns crucial for syllable classification. (2) Novel method (tokenizing neural data with GPT2) and architecture (Mamba2) demonstrated potential for decoding of syllables using spikes. (3) A combined contrastive learning-VAE framework successfully generated spectrograms from binned neural data.\nThis work establishes a promising foundation for neural decoding of complex motor outputs and offers several novel methodological approaches for processing sparse neural data.",
        "tags": [
            "VAE"
        ]
    },
    {
        "id": "125",
        "title": "Sign Operator for Coping with Heavy-Tailed Noise: High Probability Convergence Bounds with Extensions to Distributed Optimization and Comparison Oracle",
        "author": [
            "Nikita Kornilov",
            "Philip Zmushko",
            "Andrei Semenov",
            "Alexander Gasnikov",
            "Alexander Beznosikov"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07923",
        "abstract": "The growing popularity of AI optimization problems involving severely corrupted data has increased the demand for methods capable of handling heavy-tailed noise, i.e., noise with bounded $\\kappa$-th moment, $\\kappa \\in (1,2]$. For the widely used clipping technique, effectiveness heavily depends on the careful tuning of clipping levels throughout training. In this paper, we demonstrate that using only the sign of the input, without introducing additional hyperparameters, is sufficient to cope with heavy-tailed noise effectively. For smooth non-convex functions, we prove that SignSGD achieves optimal sample complexity $\\tilde{O}\\left(\\varepsilon^{-\\frac{3\\kappa - 2}{\\kappa - 1}}\\right)$ with high probability for attaining an average gradient norm accuracy of $\\varepsilon$. Under the assumption of symmetric noise, we use SignSGD with Majority Voting to extend this bound to the distributed optimization or reduce the sample complexity to $\\tilde{O}(\\varepsilon^{-4})$ in the case of a single worker with arbitrary parameters. Furthermore, we explore the application of the sign operator in zeroth-order optimization with an oracle that can only compare function values at two different points. We propose a novel method, MajorityVote-CompsSGD, and provide the first-known high-probability bound $\\tilde{O}(\\varepsilon^{-6})$ for the number of comparisons under symmetric noise assumption. Our theoretical findings are supported by the superior performance of sign-based methods in training Large Language Models.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "126",
        "title": "Discrete Markov Probabilistic Models",
        "author": [
            "Le-Tuyet-Nhi Pham",
            "Dario Shariatian",
            "Antonio Ocello",
            "Giovanni Conforti",
            "Alain Durmus"
        ],
        "pdf": "https://arxiv.org/pdf/2502.07939",
        "abstract": "This paper introduces the Discrete Markov Probabilistic Model (DMPM), a novel algorithm for discrete data generation. The algorithm operates in the space of bits $\\{0,1\\}^d$, where the noising process is a continuous-time Markov chain that can be sampled exactly via a Poissonian clock that flips labels uniformly at random. The time-reversal process, like the forward noise process, is a jump process, with its intensity governed by a discrete analogue of the classical score function. Crucially, this intensity is proven to be the conditional expectation of a function of the forward process, strengthening its theoretical alignment with score-based generative models while ensuring robustness and efficiency. We further establish convergence bounds for the algorithm under minimal assumptions and demonstrate its effectiveness through experiments on low-dimensional Bernoulli-distributed datasets and high-dimensional binary MNIST data. The results highlight its strong performance in generating discrete structures. This work bridges theoretical foundations and practical applications, advancing the development of effective and theoretically grounded discrete generative modeling.",
        "tags": [
            "Score-Based Generative"
        ]
    },
    {
        "id": "127",
        "title": "BCDDM: Branch-Corrected Denoising Diffusion Model for Black Hole Image Generation",
        "author": [
            "Ao liu",
            "Zelin Zhang",
            "Songbai Chen",
            "Cuihong Wen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08528",
        "abstract": "The properties of black holes and accretion flows can be inferred by fitting Event Horizon Telescope (EHT) data to simulated images generated through general relativistic ray tracing (GRRT). However, due to the computationally intensive nature of GRRT, the efficiency of generating specific radiation flux images needs to be improved. This paper introduces the Branch Correction Denoising Diffusion Model (BCDDM), which uses a branch correction mechanism and a weighted mixed loss function to improve the accuracy of generated black hole images based on seven physical parameters of the radiatively inefficient accretion flow (RIAF) model. Our experiments show a strong correlation between the generated images and their physical parameters. By enhancing the GRRT dataset with BCDDM-generated images and using ResNet50 for parameter regression, we achieve significant improvements in parameter prediction performance. This approach reduces computational costs and provides a faster, more efficient method for dataset expansion, parameter estimation, and model fitting.",
        "tags": [
            "Diffusion",
            "FLUX"
        ]
    },
    {
        "id": "128",
        "title": "Concentration Inequalities for the Stochastic Optimization of Unbounded Objectives with Application to Denoising Score Matching",
        "author": [
            "Jeremiah Birrell"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08628",
        "abstract": "We derive novel concentration inequalities that bound the statistical error for a large class of stochastic optimization problems, focusing on the case of unbounded objective functions. Our derivations utilize the following tools: 1) A new form of McDiarmid's inequality that is based on sample dependent one component difference bounds and which leads to a novel uniform law of large numbers result for unbounded functions. 2) A Rademacher complexity bound for families of functions that satisfy an appropriate local Lipschitz property. As an application of these results, we derive statistical error bounds for denoising score matching (DSM), an application that inherently requires one to consider unbounded objective functions, even when the data distribution has bounded support. In addition, our results establish the benefit of sample reuse in algorithms that employ easily sampled auxiliary random variables in addition to the training data, e.g., as in DSM, which uses auxiliary Gaussian random variables.",
        "tags": [
            "Score Matching"
        ]
    },
    {
        "id": "129",
        "title": "Joint Transmit and Pinching Beamforming for PASS: Optimization-Based or Learning-Based?",
        "author": [
            "Xiaoxia Xu",
            "Xidong Mu",
            "Yuanwei Liu",
            "Arumugam Nallanathan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.08637",
        "abstract": "A novel pinching antenna system (PASS)-enabled downlink multi-user multiple-input single-output (MISO) framework is proposed. PASS consists of multiple waveguides spanning over thousands of wavelength, which equip numerous low-cost dielectric particles, named pinching antennas (PAs), to radiate signals into free space. The positions of PAs can be reconfigured to change both the large-scale path losses and phases of signals, thus facilitating the novel pinching beamforming design. A sum rate maximization problem is formulated, which jointly optimizes the transmit and pinching beamforming to adaptively achieve constructive signal enhancement and destructive interference mitigation. To solve this highly coupled and nonconvex problem, both optimization-based and learning-based methods are proposed. 1) For the optimization-based method, a majorization-minimization and penalty dual decomposition (MM-PDD) algorithm is developed, which handles the nonconvex complex exponential component using a Lipschitz surrogate function and then invokes PDD for problem decoupling. 2) For the learning-based method, a novel Karush-Kuhn-Tucker (KKT)-guided dual learning (KDL) approach is proposed, which enables KKT solutions to be reconstructed in a data-driven manner by learning dual variables. Following this idea, a KDL-Tranformer algorithm is developed, which captures both inter-PA/inter-user dependencies and channel-state-information (CSI)-beamforming dependencies by attention mechanisms. Simulation results demonstrate that: i) The proposed PASS framework significantly outperforms conventional massive multiple input multiple output (MIMO) system even with a few PAs. ii) The proposed KDL-Transformer can improve over 30% system performance than MM-PDD algorithm, while achieving a millisecond-level response on modern GPUs.",
        "tags": [
            "Transformer"
        ]
    }
]