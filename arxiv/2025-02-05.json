[
    {
        "id": "1",
        "title": "Fine-tuning LLaMA 2 interference: a comparative study of language implementations for optimal efficiency",
        "author": [
            "Sazzad Hossain",
            "Touhidul Alam Seyam",
            "Avijit Chowdhury",
            "Munis Xamidov",
            "Rajib Ghose",
            "Abhijit Pathak"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01651",
        "abstract": "This paper presents a comparative study aimed at optimizing Llama2 inference, a critical aspect of machine learning and natural language processing (NLP). We evaluate various programming languages and frameworks, including TensorFlow, PyTorch, Python, Mojo, C++, and Java, analyzing their performance in terms of speed, memory consumption, and ease of implementation through extensive benchmarking. Strengths and limitations of each approach are highlighted, along with proposed optimization strategies for parallel processing and hardware utilization. Furthermore, we investigate the Mojo SDK, a novel framework designed for large language model (LLM) inference on Apple Silicon, benchmarking its performance against implementations in C, C++, Rust, Zig, Go, and Julia. Our experiments, conducted on an Apple M1 Max, demonstrate Mojo SDK's competitive performance, ease of use, and seamless Python compatibility, positioning it as a strong alternative for LLM inference on Apple Silicon. We also discuss broader implications for LLM deployment on resource-constrained hardware and identify potential directions for future research.",
        "tags": [
            "LLaMA"
        ]
    },
    {
        "id": "2",
        "title": "Hybrid Group Relative Policy Optimization: A Multi-Sample Approach to Enhancing Policy Optimization",
        "author": [
            "Soham Sane"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01652",
        "abstract": "Hybrid Group Relative Policy Optimization (Hybrid GRPO) is a reinforcement learning framework that extends Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO) by incorporating empirical multi-sample action evaluation while preserving the stability of value function-based learning. Unlike DeepSeek GRPO, which eliminates the value function in favor of purely empirical reward estimation, Hybrid GRPO introduces a structured advantage computation method that balances empirical action sampling with bootstrapped value estimation. This approach enhances sample efficiency, improves learning stability, and mitigates variance amplification observed in purely empirical methods. A detailed mathematical comparison between PPO, DeepSeek GRPO, and Hybrid GRPO is presented, highlighting key differences in advantage estimation and policy updates. Experimental validation in a controlled reinforcement learning environment demonstrates that Hybrid GRPO achieves superior convergence speed, more stable policy updates, and improved sample efficiency compared to existing methods. Several extensions to Hybrid GRPO are explored, including entropy-regularized sampling, hierarchical multi-step sub-sampling, adaptive reward normalization, and value-based action selection. Beyond reinforcement learning in simulated environments, Hybrid GRPO provides a scalable framework for bridging the gap between large language models (LLMs) and real-world agent-based decision-making. By integrating structured empirical sampling with reinforcement learning stability mechanisms, Hybrid GRPO has potential applications in autonomous robotics, financial modeling, and AI-driven control systems. These findings suggest that Hybrid GRPO serves as a robust and adaptable reinforcement learning methodology, paving the way for further advancements in policy optimization.",
        "tags": [
            "DeepSeek",
            "LLMs",
            "Large Language Models",
            "Robotics"
        ]
    },
    {
        "id": "3",
        "title": "Improving Rule-based Reasoning in LLMs via Neurosymbolic Representations",
        "author": [
            "Varun Dhanraj",
            "Chris Eliasmith"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01657",
        "abstract": "Large language models (LLMs) continue to face challenges in reliably solving reasoning tasks, particularly tasks that involve precise rule following, as often found in mathematical reasoning tasks. This paper introduces a novel neurosymbolic method that improves LLM reasoning by encoding hidden states into neurosymbolic vectors, allowing for problem-solving within a neurosymbolic vector space. The results are decoded and combined with the original hidden state, boosting the model's performance on numerical reasoning tasks. By offloading computation through neurosymbolic representations, this method improves efficiency, reliability, and interpretability. Our experimental results demonstrate an average of $82.86\\%$ lower cross entropy loss and $24.50$ times more problems correctly solved on a suite of mathematical reasoning problems compared to chain-of-thought prompting and supervised fine-tuning (LoRA), while at the same time not hindering the performance of the LLM on other tasks.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "4",
        "title": "Large Language Models' Accuracy in Emulating Human Experts' Evaluation of Public Sentiments about Heated Tobacco Products on Social Media",
        "author": [
            "Kwanho Kim",
            "Soojong Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01658",
        "abstract": "Sentiment analysis of alternative tobacco products on social media is important for tobacco control research. Large Language Models (LLMs) can help streamline the labor-intensive human sentiment analysis process. This study examined the accuracy of LLMs in replicating human sentiment evaluation of social media messages about heated tobacco products (HTPs).\nThe research used GPT-3.5 and GPT-4 Turbo to classify 500 Facebook and 500 Twitter messages, including anti-HTPs, pro-HTPs, and neutral messages. The models evaluated each message up to 20 times, and their majority label was compared to human evaluators.\nResults showed that GPT-3.5 accurately replicated human sentiment 61.2% of the time for Facebook messages and 57.0% for Twitter messages. GPT-4 Turbo performed better, with 81.7% accuracy for Facebook and 77.0% for Twitter. Using three response instances, GPT-4 Turbo achieved 99% of the accuracy of twenty instances. GPT-4 Turbo also had higher accuracy for anti- and pro-HTPs messages compared to neutral ones. Misclassifications by GPT-3.5 often involved anti- or pro-HTPs messages being labeled as neutral or irrelevant, while GPT-4 Turbo showed improvements across all categories.\nIn conclusion, LLMs can be used for sentiment analysis of HTP-related social media messages, with GPT-4 Turbo reaching around 80% accuracy compared to human experts. However, there's a risk of misrepresenting overall sentiment due to differences in accuracy across sentiment categories.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "5",
        "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
        "author": [
            "Nathaniel Tomczak",
            "Sanmukh Kuppannagari"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01659",
        "abstract": "Transformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the input's context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve \"true sparsity\" are lacking.\nIn this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "6",
        "title": "Employee Turnover Prediction: A Cross-component Attention Transformer with Consideration of Competitor Influence and Contagious Effect",
        "author": [
            "Hao Liu",
            "Yong Ge"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01660",
        "abstract": "Employee turnover refers to an individual's termination of employment from the current organization. It is one of the most persistent challenges for firms, especially those ones in Information Technology (IT) industry that confront high turnover rates. Effective prediction of potential employee turnovers benefits multiple stakeholders such as firms and online recruiters. Prior studies have focused on either the turnover prediction within a single firm or the aggregated employee movement among firms. How to predict the individual employees' turnovers among multiple firms has gained little attention in literature, and thus remains a great research challenge. In this study, we propose a novel deep learning approach based on job embeddedness theory to predict the turnovers of individual employees across different firms. Through extensive experimental evaluations using a real-world dataset, our developed method demonstrates superior performance over several state-of-the-art benchmark methods. Additionally, we estimate the cost saving for recruiters by using our turnover prediction solution and interpret the attributions of various driving factors to employee's turnover to showcase its practical business value.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "7",
        "title": "Speculative Ensemble: Fast Large Language Model Ensemble via Speculation",
        "author": [
            "Jiale Fu",
            "Yuchu Jiang",
            "Junkai Chen",
            "Jiaming Fan",
            "Xin Geng",
            "Xu Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01662",
        "abstract": "Ensemble methods enhance Large Language Models (LLMs) by combining multiple models but suffer from high computational costs. In this paper, we introduce Speculative Ensemble, a novel framework that accelerates LLM ensembles without sacrificing performance, inspired by Speculative Decoding-where a small proposal model generates tokens sequentially, and a larger target model verifies them in parallel. Our approach builds on two key insights: (1) the verification distribution can be the ensemble distribution of both the proposal and target models, and (2) alternating each model as the proposer and verifier can further enhance efficiency. We generalize this method to ensembles with n models and theoretically prove that SE is never slower than a standard ensemble, typically achieving faster speed. Extensive experiments demonstrate speed improvements of 1.11x-2.23x over standard ensemble techniques without compromising generation quality. Our code is available at https://github.com/Kamichanw/Speculative-Ensemble/",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "8",
        "title": "Explainable AI for Sentiment Analysis of Human Metapneumovirus (HMPV) Using XLNet",
        "author": [
            "Md. Shahriar Hossain Apu",
            "Md Saiful Islam",
            "Tanjim Taharat Aurpa"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01663",
        "abstract": "In 2024, the outbreak of Human Metapneumovirus (HMPV) in China, which later spread to the UK and other countries, raised significant public concern. While HMPV typically causes mild symptoms, its effects on vulnerable individuals prompted health authorities to emphasize preventive measures. This paper explores how sentiment analysis can enhance our understanding of public reactions to HMPV by analyzing social media data. We apply transformer models, particularly XLNet, achieving 93.50% accuracy in sentiment classification. Additionally, we use explainable AI (XAI) through SHAP to improve model transparency.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "9",
        "title": "Leveraging Stable Diffusion for Monocular Depth Estimation via Image Semantic Encoding",
        "author": [
            "Jingming Xia",
            "Guanqun Cao",
            "Guang Ma",
            "Yiben Luo",
            "Qinzhao Li",
            "John Oyekan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01666",
        "abstract": "Monocular depth estimation involves predicting depth from a single RGB image and plays a crucial role in applications such as autonomous driving, robotic navigation, 3D reconstruction, etc. Recent advancements in learning-based methods have significantly improved depth estimation performance. Generative models, particularly Stable Diffusion, have shown remarkable potential in recovering fine details and reconstructing missing regions through large-scale training on diverse datasets. However, models like CLIP, which rely on textual embeddings, face limitations in complex outdoor environments where rich context information is needed. These limitations reduce their effectiveness in such challenging scenarios. Here, we propose a novel image-based semantic embedding that extracts contextual information directly from visual features, significantly improving depth prediction in complex environments. Evaluated on the KITTI and Waymo datasets, our method achieves performance comparable to state-of-the-art models while addressing the shortcomings of CLIP embeddings in handling outdoor scenes. By leveraging visual semantics directly, our method demonstrates enhanced robustness and adaptability in depth estimation tasks, showcasing its potential for application to other visual perception tasks.",
        "tags": [
            "3D",
            "CLIP",
            "Depth Estimation",
            "Diffusion"
        ]
    },
    {
        "id": "10",
        "title": "Refining Alignment Framework for Diffusion Models with Intermediate-Step Preference Ranking",
        "author": [
            "Jie Ren",
            "Yuhang Zhang",
            "Dongrui Liu",
            "Xiaopeng Zhang",
            "Qi Tian"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01667",
        "abstract": "Direct preference optimization (DPO) has shown success in aligning diffusion models with human preference. Previous approaches typically assume a consistent preference label between final generations and noisy samples at intermediate steps, and directly apply DPO to these noisy samples for fine-tuning. However, we theoretically identify inherent issues in this assumption and its impacts on the effectiveness of preference alignment. We first demonstrate the inherent issues from two perspectives: gradient direction and preference order, and then propose a Tailored Preference Optimization (TailorPO) framework for aligning diffusion models with human preference, underpinned by some theoretical insights. Our approach directly ranks intermediate noisy samples based on their step-wise reward, and effectively resolves the gradient direction issues through a simple yet efficient design. Additionally, we incorporate the gradient guidance of diffusion models into preference alignment to further enhance the optimization effectiveness. Experimental results demonstrate that our method significantly improves the model's ability to generate aesthetically pleasing and human-preferred images.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "11",
        "title": "Multilingual State Space Models for Structured Question Answering in Indic Languages",
        "author": [
            "Arpita Vats",
            "Rahul Raja",
            "Mrinal Mathur",
            "Vinija Jain",
            "Aman Chadha"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01673",
        "abstract": "The diversity and complexity of Indic languages present unique challenges for natural language processing (NLP) tasks, particularly in the domain of question answering (QA).To address these challenges, this paper explores the application of State Space Models (SSMs),to build efficient and contextually aware QA systems tailored for Indic languages. SSMs are particularly suited for this task due to their ability to model long-term and short-term dependencies in sequential data, making them well-equipped to handle the rich morphology, complex syntax, and contextual intricacies characteristic of Indian languages. We evaluated multiple SSM architectures across diverse datasets representing various Indic languages and conducted a comparative analysis of their performance. Our results demonstrate that these models effectively capture linguistic subtleties, leading to significant improvements in question interpretation, context alignment, and answer generation. This work represents the first application of SSMs to question answering tasks in Indic languages, establishing a foundational benchmark for future research in this domain. We propose enhancements to existing SSM frameworks, optimizing their applicability to low-resource settings and multilingual scenarios prevalent in Indic languages.",
        "tags": [
            "SSMs",
            "State Space Models"
        ]
    },
    {
        "id": "12",
        "title": "Semantic Communication based on Generative AI: A New Approach to Image Compression and Edge Optimization",
        "author": [
            "Francesco Pezone"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01675",
        "abstract": "As digital technologies advance, communication networks face challenges in handling the vast data generated by intelligent devices. Autonomous vehicles, smart sensors, and IoT systems necessitate new paradigms. This thesis addresses these challenges by integrating semantic communication and generative models for optimized image compression and edge network resource allocation. Unlike bit-centric systems, semantic communication prioritizes transmitting meaningful data specifically selected to convey the meaning rather than obtain a faithful representation of the original data. The communication infrastructure can benefit to significant improvements in bandwidth efficiency and latency reduction. Central to this work is the design of semantic-preserving image compression using Generative Adversarial Networks and Denoising Diffusion Probabilistic Models. These models compress images by encoding only semantically relevant features, allowing for high-quality reconstruction with minimal transmission. Additionally, a Goal-Oriented edge network optimization framework is introduced, leveraging the Information Bottleneck principle and stochastic optimization to dynamically allocate resources and enhance efficiency. By integrating semantic communication into edge networks, this approach balances computational efficiency and communication effectiveness, making it suitable for real-time applications. The thesis compares semantic-aware models with conventional image compression techniques using classical and semantic evaluation metrics. Results demonstrate the potential of combining generative AI and semantic communication to create more efficient semantic-goal-oriented communication networks that meet the demands of modern data-driven applications.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "13",
        "title": "Benchmark on Peer Review Toxic Detection: A Challenging Task with a New Dataset",
        "author": [
            "Man Luo",
            "Bradley Peterson",
            "Rafael Gan",
            "Hari Ramalingame",
            "Navya Gangrade",
            "Ariadne Dimarogona",
            "Imon Banerjee",
            "Phillip Howard"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01676",
        "abstract": "Peer review is crucial for advancing and improving science through constructive criticism. However, toxic feedback can discourage authors and hinder scientific progress. This work explores an important but underexplored area: detecting toxicity in peer reviews. We first define toxicity in peer reviews across four distinct categories and curate a dataset of peer reviews from the OpenReview platform, annotated by human experts according to these definitions. Leveraging this dataset, we benchmark a variety of models, including a dedicated toxicity detection model, a sentiment analysis model, several open-source large language models (LLMs), and two closed-source LLMs. Our experiments explore the impact of different prompt granularities, from coarse to fine-grained instructions, on model performance. Notably, state-of-the-art LLMs like GPT-4 exhibit low alignment with human judgments under simple prompts but achieve improved alignment with detailed instructions. Moreover, the model's confidence score is a good indicator of better alignment with human judgments. For example, GPT-4 achieves a Cohen's Kappa score of 0.56 with human judgments, which increases to 0.63 when using only predictions with a confidence score higher than 95%. Overall, our dataset and benchmarks underscore the need for continued research to enhance toxicity detection capabilities of LLMs. By addressing this issue, our work aims to contribute to a healthy and responsible environment for constructive academic discourse and scientific collaboration.",
        "tags": [
            "Detection",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "14",
        "title": "LIBRA: Measuring Bias of Large Language Model from a Local Context",
        "author": [
            "Bo Pang",
            "Tingrui Qiao",
            "Caroline Walker",
            "Chris Cunningham",
            "Yun Sing Koh"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01679",
        "abstract": "Large Language Models (LLMs) have significantly advanced natural language processing applications, yet their widespread use raises concerns regarding inherent biases that may reduce utility or harm for particular social groups. Despite the advancement in addressing LLM bias, existing research has two major limitations. First, existing LLM bias evaluation focuses on the U.S. cultural context, making it challenging to reveal stereotypical biases of LLMs toward other cultures, leading to unfair development and use of LLMs. Second, current bias evaluation often assumes models are familiar with the target social groups. When LLMs encounter words beyond their knowledge boundaries that are unfamiliar in their training data, they produce irrelevant results in the local context due to hallucinations and overconfidence, which are not necessarily indicative of inherent bias. This research addresses these limitations with a Local Integrated Bias Recognition and Assessment Framework (LIBRA) for measuring bias using datasets sourced from local corpora without crowdsourcing. Implementing this framework, we develop a dataset comprising over 360,000 test cases in the New Zealand context. Furthermore, we propose the Enhanced Idealized CAT Score (EiCAT), integrating the iCAT score with a beyond knowledge boundary score (bbs) and a distribution divergence-based bias measurement to tackle the challenge of LLMs encountering words beyond knowledge boundaries. Our results show that the BERT family, GPT-2, and Llama-3 models seldom understand local words in different contexts. While Llama-3 exhibits larger bias, it responds better to different cultural contexts. The code and dataset are available at: https://github.com/ipangbo/LIBRA.",
        "tags": [
            "BERT",
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "15",
        "title": "DeepGate4: Efficient and Effective Representation Learning for Circuit Design at Scale",
        "author": [
            "Ziyang Zheng",
            "Shan Huang",
            "Jianyuan Zhong",
            "Zhengyuan Shi",
            "Guohao Dai",
            "Ningyi Xu",
            "Qiang Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01681",
        "abstract": "Circuit representation learning has become pivotal in electronic design automation, enabling critical tasks such as testability analysis, logic reasoning, power estimation, and SAT solving. However, existing models face significant challenges in scaling to large circuits due to limitations like over-squashing in graph neural networks and the quadratic complexity of transformer-based models. To address these issues, we introduce DeepGate4, a scalable and efficient graph transformer specifically designed for large-scale circuits. DeepGate4 incorporates several key innovations: (1) an update strategy tailored for circuit graphs, which reduce memory complexity to sub-linear and is adaptable to any graph transformer; (2) a GAT-based sparse transformer with global and local structural encodings for AIGs; and (3) an inference acceleration CUDA kernel that fully exploit the unique sparsity patterns of AIGs. Our extensive experiments on the ITC99 and EPFL benchmarks show that DeepGate4 significantly surpasses state-of-the-art methods, achieving 15.5% and 31.1% performance improvements over the next-best models. Furthermore, the Fused-DeepGate4 variant reduces runtime by 35.1% and memory usage by 46.8%, making it highly efficient for large-scale circuit analysis. These results demonstrate the potential of DeepGate4 to handle complex EDA tasks while offering superior scalability and efficiency.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "16",
        "title": "LLM-Powered Benchmark Factory: Reliable, Generic, and Efficient",
        "author": [
            "Peiwen Yuan",
            "Shaoxiong Feng",
            "Yiwei Li",
            "Xinglin Wang",
            "Yueqi Zhang",
            "Jiayi Shi",
            "Chuyi Tan",
            "Boyuan Pan",
            "Yao Hu",
            "Kan Li"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01683",
        "abstract": "The rapid advancement of large language models (LLMs) has led to a surge in both model supply and application demands. To facilitate effective matching between them, reliable, generic and efficient benchmark generators are widely needed. However, human annotators are constrained by inefficiency, and current LLM benchmark generators not only lack generalizability but also struggle with limited reliability, as they lack a comprehensive evaluation framework for validation and optimization. To fill this gap, we first propose an automated and unbiased evaluation framework, structured around four dimensions and ten criteria. Under this framework, we carefully analyze the advantages and weaknesses of directly prompting LLMs as generic benchmark generators. To enhance the reliability, we introduce a series of methods to address the identified weaknesses and integrate them as BenchMaker. Experiments across multiple LLMs and tasks confirm that BenchMaker achieves superior or comparable performance to human-annotated benchmarks on all metrics, highlighting its generalizability and reliability. More importantly, it delivers highly consistent evaluation results across 12 LLMs (0.967 Pearson correlation against MMLU-Pro), while taking only $0.005 and 0.38 minutes per sample.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "17",
        "title": "HuViDPO:Enhancing Video Generation through Direct Preference Optimization for Human-Centric Alignment",
        "author": [
            "Lifan Jiang",
            "Boxi Wu",
            "Jiahui Zhang",
            "Xiaotong Guan",
            "Shuang Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01690",
        "abstract": "With the rapid development of AIGC technology, significant progress has been made in diffusion model-based technologies for text-to-image (T2I) and text-to-video (T2V). In recent years, a few studies have introduced the strategy of Direct Preference Optimization (DPO) into T2I tasks, significantly enhancing human preferences in generated images. However, existing T2V generation methods lack a well-formed pipeline with exact loss function to guide the alignment of generated videos with human preferences using DPO strategies. Additionally, challenges such as the scarcity of paired video preference data hinder effective model training. At the same time, the lack of training datasets poses a risk of insufficient flexibility and poor video generation quality in the generated videos. Based on those problems, our work proposes three targeted solutions in sequence. 1) Our work is the first to introduce the DPO strategy into the T2V tasks. By deriving a carefully structured loss function, we utilize human feedback to align video generation with human preferences. We refer to this new method as HuViDPO. 2) Our work constructs small-scale human preference datasets for each action category and fine-tune this model, improving the aesthetic quality of the generated videos while reducing training costs. 3) We adopt a First-Frame-Conditioned strategy, leveraging the rich in formation from the first frame to guide the generation of subsequent frames, enhancing flexibility in video generation. At the same time, we employ a SparseCausal Attention mechanism to enhance the quality of the generated http://videos.More details and examples can be accessed on our website: https://tankowa.github.io/HuViDPO. http://github.io/.",
        "tags": [
            "Diffusion",
            "Text-to-Image",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "18",
        "title": "Metastable Dynamics of Chain-of-Thought Reasoning: Provable Benefits of Search, RL and Distillation",
        "author": [
            "Juno Kim",
            "Denny Wu",
            "Jason Lee",
            "Taiji Suzuki"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01694",
        "abstract": "A key paradigm to improve the reasoning capabilities of large language models (LLMs) is to allocate more inference-time compute to search against a verifier or reward model. This process can then be utilized to refine the pretrained model or distill its reasoning patterns into more efficient models. In this paper, we study inference-time compute by viewing chain-of-thought (CoT) generation as a metastable Markov process: easy reasoning steps (e.g., algebraic manipulations) form densely connected clusters, while hard reasoning steps (e.g., applying a relevant theorem) create sparse, low-probability edges between clusters, leading to phase transitions at longer timescales. Under this framework, we prove that implementing a search protocol that rewards sparse edges improves CoT by decreasing the expected number of steps to reach different clusters. In contrast, we establish a limit on reasoning capability when the model is restricted to local information of the pretrained graph. We also show that the information gained by search can be utilized to obtain a better reasoning model: (1) the pretrained model can be directly finetuned to favor sparse edges via policy gradient methods, and moreover (2) a compressed metastable representation of the reasoning dynamics can be distilled into a smaller, more efficient model.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RL"
        ]
    },
    {
        "id": "19",
        "title": "BARE: Combining Base and Instruction-Tuned Language Models for Better Synthetic Data Generation",
        "author": [
            "Alan Zhu",
            "Parth Asawa",
            "Jared Quincy Davis",
            "Lingjiao Chen",
            "Ion Stoica",
            "Joseph E. Gonzalez",
            "Matei Zaharia"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01697",
        "abstract": "As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. A common assumption about synthetic data is that sampling from instruct-tuned models is sufficient; however, these models struggle to produce diverse outputs-a key requirement for generalization. Despite various prompting methods, in this work we show that achieving meaningful diversity from instruct-tuned models remains challenging. In contrast, we find base models without post-training exhibit greater diversity, but are less capable at instruction following and hence of lower quality. Leveraging this insight, we propose Base-Refine (BARE), a synthetic data generation method that combines the diversity of base models with the quality of instruct-tuned models through a two-stage process. With minimal few-shot examples and curation, BARE generates diverse and high-quality datasets, improving downstream task performance. We show that fine-tuning with as few as 1,000 BARE-generated samples can reach performance comparable to the best similarly sized models on LiveCodeBench tasks. Furthermore, fine-tuning with BARE-generated data achieves a 101% improvement over instruct-only data on GSM8K and a 18.4% improvement over SOTA methods on RAFT.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "20",
        "title": "Al-Khwarizmi: Discovering Physical Laws with Foundation Models",
        "author": [
            "Christopher E. Mower",
            "Haitham Bou-Ammar"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01702",
        "abstract": "Inferring physical laws from data is a central challenge in science and engineering, including but not limited to healthcare, physical sciences, biosciences, social sciences, sustainability, climate, and robotics. Deep networks offer high-accuracy results but lack interpretability, prompting interest in models built from simple components. The Sparse Identification of Nonlinear Dynamics (SINDy) method has become the go-to approach for building such modular and interpretable models. SINDy leverages sparse regression with L1 regularization to identify key terms from a library of candidate functions. However, SINDy's choice of candidate library and optimization method requires significant technical expertise, limiting its widespread applicability. This work introduces Al-Khwarizmi, a novel agentic framework for physical law discovery from data, which integrates foundational models with SINDy. Leveraging LLMs, VLMs, and Retrieval-Augmented Generation (RAG), our approach automates physical law discovery, incorporating prior knowledge and iteratively refining candidate solutions via reflection. Al-Khwarizmi operates in two steps: it summarizes system observations-comprising textual descriptions, raw data, and plots-followed by a secondary step that generates candidate feature libraries and optimizer configurations to identify hidden physics laws correctly. Evaluating our algorithm on over 198 models, we demonstrate state-of-the-art performance compared to alternatives, reaching a 20 percent increase against the best-performing alternative.",
        "tags": [
            "LLMs",
            "RAG",
            "Robotics"
        ]
    },
    {
        "id": "21",
        "title": "QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning",
        "author": [
            "Moses Ananta",
            "Muhammad Farid Adilazuarda",
            "Zayd Muhammad Kawakibi Zuhri",
            "Ayu Purwarianti",
            "Alham Fikri Aji"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01703",
        "abstract": "Fine-tuning large language models (LLMs) is often constrained by the computational costs of processing massive datasets. We propose \\textbf{QLESS} (Quantized Low-rank Gradient Similarity Search), which integrates gradient quantization with the LESS framework to enable memory-efficient data valuation and selection. QLESS employs a two-step compression process: first, it obtains low-dimensional gradient representations through LoRA-based random projection; then, it quantizes these gradients to low-bitwidth representations. Experiments on multiple LLM architectures (LLaMA, Mistral, Qwen) and benchmarks (MMLU, BBH, TyDiQA) show that QLESS achieves comparable data selection performance to LESS while reducing memory usage by up to 16x. Even 1-bit gradient quantization preserves data valuation quality. These findings underscore QLESS as a practical, scalable approach to identifying informative examples within strict memory constraints.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models",
            "LoRA",
            "Qwen"
        ]
    },
    {
        "id": "22",
        "title": "Progressive Binarization with Semi-Structured Pruning for LLMs",
        "author": [
            "Xianglong Yan",
            "Tianao Zhang",
            "Zhiteng Li",
            "Yulun Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01705",
        "abstract": "Large language models (LLMs) have achieved remarkable success in natural language processing tasks, but their high computational and memory demands pose challenges for deployment on resource-constrained devices. Binarization, as an efficient compression method that reduces model weights to just 1 bit, significantly lowers both computational and memory requirements. Despite this, the binarized LLM still contains redundancy, which can be further compressed. Semi-structured pruning provides a promising approach to achieve this, which offers a better trade-off between model performance and hardware efficiency. However, simply combining binarization with semi-structured pruning can lead to a significant performance drop. To address this issue, we propose a Progressive Binarization with Semi-Structured Pruning (PBS$^2$P) method for LLM compression. We first propose a Stepwise semi-structured Pruning with Binarization Optimization (SPBO). Our optimization strategy significantly reduces the total error caused by pruning and binarization, even below that of the no-pruning scenario. Furthermore, we design a Coarse-to-Fine Search (CFS) method to select pruning elements more effectively. Extensive experiments demonstrate that PBS$^2$P achieves superior accuracy across various LLM families and evaluation metrics, noticeably outperforming state-of-the-art (SOTA) binary PTQ methods. The code and models will be available at https://github.com/XIANGLONGYAN/PBS2P.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "23",
        "title": "CLIP-DQA: Blindly Evaluating Dehazed Images from Global and Local Perspectives Using CLIP",
        "author": [
            "Yirui Zeng",
            "Jun Fu",
            "Hadi Amirpour",
            "Huasheng Wang",
            "Guanghui Yue",
            "Hantao Liu",
            "Ying Chen",
            "Wei Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01707",
        "abstract": "Blind dehazed image quality assessment (BDQA), which aims to accurately predict the visual quality of dehazed images without any reference information, is essential for the evaluation, comparison, and optimization of image dehazing algorithms. Existing learning-based BDQA methods have achieved remarkable success, while the small scale of DQA datasets limits their performance. To address this issue, in this paper, we propose to adapt Contrastive Language-Image Pre-Training (CLIP), pre-trained on large-scale image-text pairs, to the BDQA task. Specifically, inspired by the fact that the human visual system understands images based on hierarchical features, we take global and local information of the dehazed image as the input of CLIP. To accurately map the input hierarchical information of dehazed images into the quality score, we tune both the vision branch and language branch of CLIP with prompt learning. Experimental results on two authentic DQA datasets demonstrate that our proposed approach, named CLIP-DQA, achieves more accurate quality predictions over existing BDQA methods. The code is available at https://github.com/JunFu1995/CLIP-DQA.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "24",
        "title": "Adapter-Based Multi-Agent AVSR Extension for Pre-Trained ASR Models",
        "author": [
            "Christopher Simic",
            "Korbinian Riedhammer",
            "Tobias Bocklet"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01709",
        "abstract": "We present an approach to Audio-Visual Speech Recognition that builds on a pre-trained Whisper model. To infuse visual information into this audio-only model, we extend it with an AV fusion module and LoRa adapters, one of the most up-to-date adapter approaches. One advantage of adapter-based approaches, is that only a relatively small number of parameters are trained, while the basic model remains unchanged. Common AVSR approaches train single models to handle several noise categories and noise levels simultaneously. Taking advantage of the lightweight nature of adapter approaches, we train noise-scenario-specific adapter-sets, each covering individual noise-categories or a specific noise-level range. The most suitable adapter-set is selected by previously classifying the noise-scenario. This enables our models to achieve an optimum coverage across different noise-categories and noise-levels, while training only a minimum number of parameters.\nCompared to a full fine-tuning approach with SOTA performance our models achieve almost comparable results over the majority of the tested noise-categories and noise-levels, with up to 88.5% less trainable parameters. Our approach can be extended by further noise-specific adapter-sets to cover additional noise scenarios. It is also possible to utilize the underlying powerful ASR model when no visual information is available, as it remains unchanged.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "25",
        "title": "Process-Supervised Reinforcement Learning for Code Generation",
        "author": [
            "Yufan Ye",
            "Ting Zhang",
            "Wenbin Jiang",
            "Hua Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01715",
        "abstract": "Existing reinforcement learning strategies based on outcome supervision have proven effective in enhancing the performance of large language models(LLMs) for code generation. While reinforcement learning based on process supervision has shown great promise in handling multi-step reasoning tasks, its effectiveness in code generation remains largely underexplored and underjustified. The primary obstacle stems from the resource-intensive nature of constructing high-quality process-supervised data, which demands substantial human expertise and computational resources. In response to this challenge, we propose a \"statement mutation/refactoring-compile and execution verification\" strategy: mutating and refactoring code line-by-line through a teacher model, and utilizing compiler execution results to automatically label each line, resulting in line-by-line process-supervised data, which is pivotal for training a process-supervised reward model. The trained reward model is then integrated into the PRLCoder framework, followed by experimental validation on several benchmarks. Experimental results demonstrate that process-supervised reinforcement learning significantly surpasses methods relying solely on outcome supervision. Notably, in tackling complex code generation tasks, process-supervised reinforcement learning shows a clear advantage, ensuring both the integrity of the code generation process and the correctness of the generation results.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "26",
        "title": "Choose Your Model Size: Any Compression by a Single Gradient Descent",
        "author": [
            "Martin Genzel",
            "Patrick Putzky",
            "Pengfei Zhao",
            "Sebastian Schulze",
            "Mattes Mollenhauer",
            "Robert Seidel",
            "Stefan Dietzel",
            "Thomas Wollmann"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01717",
        "abstract": "The adoption of Foundation Models in resource-constrained environments remains challenging due to their large size and inference costs. A promising way to overcome these limitations is post-training compression, which aims to balance reduced model size against performance degradation. This work presents Any Compression via Iterative Pruning (ACIP), a novel algorithmic approach to determine a compression-performance trade-off from a single stochastic gradient descent run. To ensure parameter efficiency, we use an SVD-reparametrization of linear layers and iteratively prune their singular values with a sparsity-inducing penalty. The resulting pruning order gives rise to a global parameter ranking that allows us to materialize models of any target size. Importantly, the compressed models exhibit strong predictive downstream performance without the need for costly fine-tuning. We evaluate ACIP on a large selection of open-weight LLMs and tasks, and demonstrate state-of-the-art results compared to existing factorisation-based compression methods. We also show that ACIP seamlessly complements common quantization-based compression techniques.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "27",
        "title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis",
        "author": [
            "Huaye Zeng",
            "Dongfu Jiang",
            "Haozhe Wang",
            "Ping Nie",
            "Xiaotong Chen",
            "Wenhu Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01718",
        "abstract": "Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25\\% and MBPP-plus by 6\\% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models.",
        "tags": [
            "DeepSeek",
            "LLaMA",
            "RL"
        ]
    },
    {
        "id": "28",
        "title": "MJ-VIDEO: Fine-Grained Benchmarking and Rewarding Video Preferences in Video Generation",
        "author": [
            "Haibo Tong",
            "Zhaoyang Wang",
            "Zhaorun Chen",
            "Haonian Ji",
            "Shi Qiu",
            "Siwei Han",
            "Kexin Geng",
            "Zhongkai Xue",
            "Yiyang Zhou",
            "Peng Xia",
            "Mingyu Ding",
            "Rafael Rafailov",
            "Chelsea Finn",
            "Huaxiu Yao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01719",
        "abstract": "Recent advancements in video generation have significantly improved the ability to synthesize videos from text instructions. However, existing models still struggle with key challenges such as instruction misalignment, content hallucination, safety concerns, and bias. Addressing these limitations, we introduce MJ-BENCH-VIDEO, a large-scale video preference benchmark designed to evaluate video generation across five critical aspects: Alignment, Safety, Fineness, Coherence & Consistency, and Bias & Fairness. This benchmark incorporates 28 fine-grained criteria to provide a comprehensive evaluation of video preference. Building upon this dataset, we propose MJ-VIDEO, a Mixture-of-Experts (MoE)-based video reward model designed to deliver fine-grained reward. MJ-VIDEO can dynamically select relevant experts to accurately judge the preference based on the input text-video pair. This architecture enables more precise and adaptable preference judgments. Through extensive benchmarking on MJ-BENCH-VIDEO, we analyze the limitations of existing video reward models and demonstrate the superior performance of MJ-VIDEO in video preference assessment, achieving 17.58% and 15.87% improvements in overall and fine-grained preference judgments, respectively. Additionally, introducing MJ-VIDEO for preference tuning in video generation enhances the alignment performance.",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "29",
        "title": "Generating Multi-Image Synthetic Data for Text-to-Image Customization",
        "author": [
            "Nupur Kumari",
            "Xi Yin",
            "Jun-Yan Zhu",
            "Ishan Misra",
            "Samaneh Azadi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01720",
        "abstract": "Customization of text-to-image models enables users to insert custom concepts and generate the concepts in unseen settings. Existing methods either rely on costly test-time optimization or train encoders on single-image training datasets without multi-image supervision, leading to worse image quality. We propose a simple approach that addresses both limitations. We first leverage existing text-to-image models and 3D datasets to create a high-quality Synthetic Customization Dataset (SynCD) consisting of multiple images of the same object in different lighting, backgrounds, and poses. We then propose a new encoder architecture based on shared attention mechanisms that better incorporate fine-grained visual details from input images. Finally, we propose a new inference technique that mitigates overexposure issues during inference by normalizing the text and image guidance vectors. Through extensive experiments, we show that our model, trained on the synthetic dataset with the proposed encoder and inference algorithm, outperforms existing tuning-free methods on standard customization benchmarks.",
        "tags": [
            "3D",
            "Text-to-Image"
        ]
    },
    {
        "id": "30",
        "title": "Evaluation of Large Language Models via Coupled Token Generation",
        "author": [
            "Nina Corvelo Benz",
            "Stratis Tsirtsis",
            "Eleni Straitouri",
            "Ivi Chatzi",
            "Ander Artola Velasco",
            "Suhas Thejaswi",
            "Manuel Gomez-Rodriguez"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01754",
        "abstract": "State of the art large language models rely on randomization to respond to a prompt. As an immediate consequence, a model may respond differently to the same prompt if asked multiple times. In this work, we argue that the evaluation and ranking of large language models should control for the randomization underpinning their functioning. Our starting point is the development of a causal model for coupled autoregressive generation, which allows different large language models to sample responses with the same source of randomness. Building upon our causal model, we first show that, on evaluations based on benchmark datasets, coupled autoregressive generation leads to the same conclusions as vanilla autoregressive generation but using provably fewer samples. However, we further show that, on evaluations based on (human) pairwise comparisons, coupled and vanilla autoregressive generation can surprisingly lead to different rankings when comparing more than two models, even with an infinite amount of samples. This suggests that the apparent advantage of a model over others in existing evaluation protocols may not be genuine but rather confounded by the randomness inherent to the generation process. To illustrate and complement our theoretical results, we conduct experiments with several large language models from the Llama family. We find that, across multiple knowledge areas from the popular MMLU benchmark dataset, coupled autoregressive generation requires up to 40% fewer samples to reach the same conclusions as vanilla autoregressive generation. Further, using data from the LMSYS Chatbot Arena platform, we find that the win-rates derived from pairwise comparisons by a strong large language model to prompts differ under coupled and vanilla autoregressive generation.",
        "tags": [
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "31",
        "title": "Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA",
        "author": [
            "Shuangyi Chen",
            "Yuanxin Guo",
            "Yue Ju",
            "Harik Dalal",
            "Ashish Khisti"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01755",
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) optimize federated training by reducing computational and communication costs. We propose RoLoRA, a federated framework using alternating optimization to fine-tune LoRA adapters. Our approach emphasizes the importance of learning up and down projection matrices to enhance expressiveness and robustness. We use both theoretical analysis and extensive experiments to demonstrate the advantages of RoLoRA over prior approaches that either generate imperfect model updates or limit expressiveness of the model. We present theoretical analysis on a simplified linear model to demonstrate the importance of learning both down-projection and up-projection matrices in LoRA. We provide extensive experimental evaluations on a toy neural network on MNIST as well as large language models including RoBERTa-Large, Llama-2-7B on diverse tasks to demonstrate the advantages of RoLoRA over other methods.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models",
            "LoRA",
            "Low-Rank Adaptation"
        ]
    },
    {
        "id": "32",
        "title": "Training Users Against Human and GPT-4 Generated Social Engineering Attacks",
        "author": [
            "Tyler Malloy",
            "Maria Jose Ferreira",
            "Fei Fang",
            "Cleotilde Gonzalez"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01764",
        "abstract": "In real-world decision making, outcomes are often delayed, meaning individuals must make multiple decisions before receiving any feedback. Moreover, feedback can be presented in different ways: it may summarize the overall results of multiple decisions (aggregated feedback) or report the outcome of individual decisions after some delay (clustered feedback). Despite its importance, the timing and presentation of delayed feedback has received little attention in cognitive modeling of decision-making, which typically focuses on immediate feedback. To address this, we conducted an experiment to compare the effect of delayed vs. immediate feedback and aggregated vs. clustered feedback. We also propose a Hierarchical Instance-Based Learning (HIBL) model that captures how people make decisions in delayed feedback settings. HIBL uses a super-model that chooses between sub-models to perform the decision-making task until an outcome is observed. Simulations show that HIBL best predicts human behavior and specific patterns, demonstrating the flexibility of IBL models.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "33",
        "title": "Hamming Attention Distillation: Binarizing Keys and Queries for Efficient Long-Context Transformers",
        "author": [
            "Mark Horton",
            "Tergel Molom-Ochir",
            "Peter Liu",
            "Bhavna Gopal",
            "Chiyue Wei",
            "Cong Guo",
            "Brady Taylor",
            "Deliang Fan",
            "Shan X. Wang",
            "Hai Li",
            "Yiran Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01770",
        "abstract": "Pre-trained transformer models with extended context windows are notoriously expensive to run at scale, often limiting real-world deployment due to their high computational and memory requirements. In this paper, we introduce Hamming Attention Distillation (HAD), a novel framework that binarizes keys and queries in the attention mechanism to achieve significant efficiency gains. By converting keys and queries into {-1, +1} vectors and replacing dot-product operations with efficient Hamming distance computations, our method drastically reduces computational overhead. Additionally, we incorporate attention matrix sparsification to prune low-impact activations, which further reduces the cost of processing long-context sequences. \\par Despite these aggressive compression strategies, our distilled approach preserves a high degree of representational power, leading to substantially improved accuracy compared to prior transformer binarization methods. We evaluate HAD on a range of tasks and models, including the GLUE benchmark, ImageNet, and QuALITY, demonstrating state-of-the-art performance among binarized Transformers while drastically reducing the computational costs of long-context inference. \\par We implement HAD in custom hardware simulations, demonstrating superior performance characteristics compared to a custom hardware implementation of standard attention. HAD achieves just $\\mathbf{1.78}\\%$ performance losses on GLUE compared to $9.08\\%$ in state-of-the-art binarization work, and $\\mathbf{2.5}\\%$ performance losses on ImageNet compared to $12.14\\%$, all while targeting custom hardware with a $\\mathbf{79}\\%$ area reduction and $\\mathbf{87}\\%$ power reduction compared to its standard attention counterpart.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "34",
        "title": "Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity",
        "author": [
            "Haocheng Xi",
            "Shuo Yang",
            "Yilong Zhao",
            "Chenfeng Xu",
            "Muyang Li",
            "Xiuyu Li",
            "Yujun Lin",
            "Han Cai",
            "Jintao Zhang",
            "Dacheng Li",
            "Jianfei Chen",
            "Ion Stoica",
            "Kurt Keutzer",
            "Song Han"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01776",
        "abstract": "Diffusion Transformers (DiTs) dominate video generation but their high computational cost severely limits real-world applicability, usually requiring tens of minutes to generate a few seconds of video even on high-performance GPUs. This inefficiency primarily arises from the quadratic computational complexity of 3D Full Attention with respect to the context length. In this paper, we propose a training-free framework termed Sparse VideoGen (SVG) that leverages the inherent sparsity in 3D Full Attention to boost inference efficiency. We reveal that the attention heads can be dynamically classified into two groups depending on distinct sparse patterns: (1) Spatial Head, where only spatially-related tokens within each frame dominate the attention output, and (2) Temporal Head, where only temporally-related tokens across different frames dominate. Based on this insight, SVG proposes an online profiling strategy to capture the dynamic sparse patterns and predicts the type of attention head. Combined with a novel hardware-efficient tensor layout transformation and customized kernel implementations, SVG achieves up to 2.28x and 2.33x end-to-end speedup on CogVideoX-v1.5 and HunyuanVideo, respectively, while preserving generation quality.",
        "tags": [
            "3D",
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "35",
        "title": "GNN-DT: Graph Neural Network Enhanced Decision Transformer for Efficient Optimization in Dynamic Environments",
        "author": [
            "Stavros Orfanoudakis",
            "Nanda Kishor Panda",
            "Peter Palensky",
            "Pedro P. Vergara"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01778",
        "abstract": "Reinforcement Learning (RL) methods used for solving real-world optimization problems often involve dynamic state-action spaces, larger scale, and sparse rewards, leading to significant challenges in convergence, scalability, and efficient exploration of the solution space. This study introduces GNN-DT, a novel Decision Transformer (DT) architecture that integrates Graph Neural Network (GNN) embedders with a novel residual connection between input and output tokens crucial for handling dynamic environments. By learning from previously collected trajectories, GNN-DT reduces dependence on accurate simulators and tackles the sparse rewards limitations of online RL algorithms. We evaluate GNN-DT on the complex electric vehicle (EV) charging optimization problem and prove that its performance is superior and requires significantly fewer training trajectories, thus improving sample efficiency compared to existing DT baselines. Furthermore, GNN-DT exhibits robust generalization to unseen environments and larger action spaces, addressing a critical gap in prior DT-based approaches",
        "tags": [
            "RL",
            "Transformer"
        ]
    },
    {
        "id": "36",
        "title": "VILP: Imitation Learning with Latent Video Planning",
        "author": [
            "Zhengtong Xu",
            "Qiang Qiu",
            "Yu She"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01784",
        "abstract": "In the era of generative AI, integrating video generation models into robotics opens new possibilities for the general-purpose robot agent. This paper introduces imitation learning with latent video planning (VILP). We propose a latent video diffusion model to generate predictive robot videos that adhere to temporal consistency to a good degree. Our method is able to generate highly time-aligned videos from multiple views, which is crucial for robot policy learning. Our video generation model is highly time-efficient. For example, it can generate videos from two distinct perspectives, each consisting of six frames with a resolution of 96x160 pixels, at a rate of 5 Hz. In the experiments, we demonstrate that VILP outperforms the existing video generation robot policy across several metrics: training costs, inference speed, temporal consistency of generated videos, and the performance of the policy. We also compared our method with other imitation learning methods. Our findings indicate that VILP can rely less on extensive high-quality task-specific robot action data while still maintaining robust performance. In addition, VILP possesses robust capabilities in representing multi-modal action distributions. Our paper provides a practical example of how to effectively integrate video generation models into robot policies, potentially offering insights for related fields and directions. For more details, please refer to our open-source repository https://github.com/ZhengtongXu/VILP.",
        "tags": [
            "Diffusion",
            "Robot",
            "Robotics",
            "Video Generation"
        ]
    },
    {
        "id": "37",
        "title": "An Agentic AI Workflow for Detecting Cognitive Concerns in Real-world Data",
        "author": [
            "Jiazi Tian",
            "Liqin Wang",
            "Pedram Fard",
            "Valdery Moura Junior",
            "Deborah Blacker",
            "Jennifer S. Haas",
            "Chirag Patel",
            "Shawn N. Murphy",
            "Lidia M.V.R. Moura",
            "Hossein Estiri"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01789",
        "abstract": "Early identification of cognitive concerns is critical but often hindered by subtle symptom presentation. This study developed and validated a fully automated, multi-agent AI workflow using LLaMA 3 8B to identify cognitive concerns in 3,338 clinical notes from Mass General Brigham. The agentic workflow, leveraging task-specific agents that dynamically collaborate to extract meaningful insights from clinical notes, was compared to an expert-driven benchmark. Both workflows achieved high classification performance, with F1-scores of 0.90 and 0.91, respectively. The agentic workflow demonstrated improved specificity (1.00) and achieved prompt refinement in fewer iterations. Although both workflows showed reduced performance on validation data, the agentic workflow maintained perfect specificity. These findings highlight the potential of fully automated multi-agent AI workflows to achieve expert-level accuracy with greater efficiency, offering a scalable and cost-effective solution for detecting cognitive concerns in clinical settings.",
        "tags": [
            "LLaMA"
        ]
    },
    {
        "id": "38",
        "title": "Harmful Terms and Where to Find Them: Measuring and Modeling Unfavorable Financial Terms and Conditions in Shopping Websites at Scale",
        "author": [
            "Elisa Tsai",
            "Neal Mangaokar",
            "Boyuan Zheng",
            "Haizhong Zheng",
            "Atul Prakash"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01798",
        "abstract": "Terms and conditions for online shopping websites often contain terms that can have significant financial consequences for customers. Despite their impact, there is currently no comprehensive understanding of the types and potential risks associated with unfavorable financial terms. Furthermore, there are no publicly available detection systems or datasets to systematically identify or mitigate these terms. In this paper, we take the first steps toward solving this problem with three key contributions.\n\\textit{First}, we introduce \\textit{TermMiner}, an automated data collection and topic modeling pipeline to understand the landscape of unfavorable financial terms. \\textit{Second}, we create \\textit{ShopTC-100K}, a dataset of terms and conditions from shopping websites in the Tranco top 100K list, comprising 1.8 million terms from 8,251 websites. Consequently, we develop a taxonomy of 22 types from 4 categories of unfavorable financial terms -- spanning purchase, post-purchase, account termination, and legal aspects. \\textit{Third}, we build \\textit{TermLens}, an automated detector that uses Large Language Models (LLMs) to identify unfavorable financial terms.\nFine-tuned on an annotated dataset, \\textit{TermLens} achieves an F1 score of 94.6\\% and a false positive rate of 2.3\\% using GPT-4o. When applied to shopping websites from the Tranco top 100K, we find that 42.06\\% of these sites contain at least one unfavorable financial term, with such terms being more prevalent on less popular websites. Case studies further highlight the financial risks and customer dissatisfaction associated with unfavorable financial terms, as well as the limitations of existing ecosystem defenses.",
        "tags": [
            "Detection",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "39",
        "title": "MemPal: Leveraging Multimodal AI and LLMs for Voice-Activated Object Retrieval in Homes of Older Adults",
        "author": [
            "Natasha Maniar",
            "Samantha W.T. Chan",
            "Wazeer Zulfikar",
            "Scott Ren",
            "Christine Xu",
            "Pattie Maes"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01801",
        "abstract": "Older adults have increasing difficulty with retrospective memory, hindering their abilities to perform daily activities and posing stress on caregivers to ensure their wellbeing. Recent developments in Artificial Intelligence (AI) and large context-aware multimodal models offer an opportunity to create memory support systems that assist older adults with common issues like object finding. This paper discusses the development of an AI-based, wearable memory assistant, MemPal, that helps older adults with a common problem, finding lost objects at home, and presents results from tests of the system in older adults' own homes. Using visual context from a wearable camera, the multimodal LLM system creates a real-time automated text diary of the person's activities for memory support purposes, offering object retrieval assistance using a voice-based interface. The system is designed to support additional use cases like context-based proactive safety reminders and recall of past actions. We report on a quantitative and qualitative study with N=15 older adults within their own homes that showed improved performance of object finding with audio-based assistance compared to no aid and positive overall user perceptions on the designed system. We discuss further applications of MemPal's design as a multi-purpose memory aid and future design guidelines to adapt memory assistants to older adults' unique needs.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "40",
        "title": "Discovering Chunks in Neural Embeddings for Interpretability",
        "author": [
            "Shuchen Wu",
            "Stephan Alaniz",
            "Eric Schulz",
            "Zeynep Akata"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01803",
        "abstract": "Understanding neural networks is challenging due to their high-dimensional, interacting components. Inspired by human cognition, which processes complex sensory data by chunking it into recurring entities, we propose leveraging this principle to interpret artificial neural population activities. Biological and artificial intelligence share the challenge of learning from structured, naturalistic data, and we hypothesize that the cognitive mechanism of chunking can provide insights into artificial systems. We first demonstrate this concept in recurrent neural networks (RNNs) trained on artificial sequences with imposed regularities, observing that their hidden states reflect these patterns, which can be extracted as a dictionary of chunks that influence network responses. Extending this to large language models (LLMs) like LLaMA, we identify similar recurring embedding states corresponding to concepts in the input, with perturbations to these states activating or inhibiting the associated concepts. By exploring methods to extract dictionaries of identifiable chunks across neural embeddings of varying complexity, our findings introduce a new framework for interpreting neural networks, framing their population activity as structured reflections of the data they process.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "41",
        "title": "Toward Neurosymbolic Program Comprehension",
        "author": [
            "Alejandro Velasco",
            "Aya Garryyeva",
            "David N. Palacio",
            "Antonio Mastropaolo",
            "Denys Poshyvanyk"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01806",
        "abstract": "Recent advancements in Large Language Models (LLMs) have paved the way for Large Code Models (LCMs), enabling automation in complex software engineering tasks, such as code generation, software testing, and program comprehension, among others. Tools like GitHub Copilot and ChatGPT have shown substantial benefits in supporting developers across various practices. However, the ambition to scale these models to trillion-parameter sizes, exemplified by GPT-4, poses significant challenges that limit the usage of Artificial Intelligence (AI)-based systems powered by large Deep Learning (DL) models. These include rising computational demands for training and deployment and issues related to trustworthiness, bias, and interpretability. Such factors can make managing these models impractical for many organizations, while their \"black-box'' nature undermines key aspects, including transparency and accountability. In this paper, we question the prevailing assumption that increasing model parameters is always the optimal path forward, provided there is sufficient new data to learn additional patterns. In particular, we advocate for a Neurosymbolic research direction that combines the strengths of existing DL techniques (e.g., LLMs) with traditional symbolic methods--renowned for their reliability, speed, and determinism. To this end, we outline the core features and present preliminary results for our envisioned approach, aimed at establishing the first Neurosymbolic Program Comprehension (NsPC) framework to aid in identifying defective code components.",
        "tags": [
            "ChatGPT",
            "GPT",
            "LCMs",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "42",
        "title": "SelfCheckAgent: Zero-Resource Hallucination Detection in Generative Large Language Models",
        "author": [
            "Diyana Muhammed",
            "Gollam Rabby",
            "SÃ¶ren Auer"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01812",
        "abstract": "Detecting hallucinations in Large Language Models (LLMs) remains a critical challenge for their reliable deployment in real-world applications. To address this, we introduce SelfCheckAgent, a novel framework integrating three different agents: the Symbolic Agent, the Specialized Detection Agent, and the Contextual Consistency Agent. These agents provide a robust multi-dimensional approach to hallucination detection. Notable results include the Contextual Consistency Agent leveraging Llama 3.1 with Chain-of-Thought (CoT) to achieve outstanding performance on the WikiBio dataset, with NonFactual hallucination detection scoring 93.64%, Factual 70.26%, and Ranking 78.48% respectively. On the AIME dataset, GPT-4o with CoT excels in NonFactual detection with 94.89% but reveals trade-offs in Factual with 30.58% and Ranking with 30.68%, underscoring the complexity of hallucination detection in the complex mathematical domains. The framework also incorporates a triangulation strategy, which increases the strengths of the SelfCheckAgent, yielding significant improvements in real-world hallucination identification. The comparative analysis demonstrates SelfCheckAgent's applicability across diverse domains, positioning it as a crucial advancement for trustworthy LLMs. These findings highlight the potentiality of consistency-driven methodologies in detecting hallucinations in LLMs.",
        "tags": [
            "Detection",
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "43",
        "title": "Low Resource Video Super-resolution using Memory and Residual Deformable Convolutions",
        "author": [
            "Kavitha Viswanathan",
            "Shashwat Pathak",
            "Piyush Bharambe",
            "Harsh Choudhary",
            "Amit Sethi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01816",
        "abstract": "Transformer-based video super-resolution (VSR) models have set new benchmarks in recent years, but their substantial computational demands make most of them unsuitable for deployment on resource-constrained devices. Achieving a balance between model complexity and output quality remains a formidable challenge in VSR. Although lightweight models have been introduced to address this issue, they often struggle to deliver state-of-the-art performance. We propose a novel lightweight, parameter-efficient deep residual deformable convolution network for VSR. Unlike prior methods, our model enhances feature utilization through residual connections and employs deformable convolution for precise frame alignment, addressing motion dynamics effectively. Furthermore, we introduce a single memory tensor to capture information accrued from the past frames and improve motion estimation across frames. This design enables an efficient balance between computational cost and reconstruction quality. With just 2.3 million parameters, our model achieves state-of-the-art SSIM of 0.9175 on the REDS4 dataset, surpassing existing lightweight and many heavy models in both accuracy and resource efficiency. Architectural insights from our model pave the way for real-time VSR on streaming data.",
        "tags": [
            "Super Resolution",
            "Transformer"
        ]
    },
    {
        "id": "44",
        "title": "Score as Action: Fine-Tuning Diffusion Generative Models by Continuous-time Reinforcement Learning",
        "author": [
            "Hanyang Zhao",
            "Haoxian Chen",
            "Ji Zhang",
            "David D. Yao",
            "Wenpin Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01819",
        "abstract": "Reinforcement learning from human feedback (RLHF), which aligns a diffusion model with input prompt, has become a crucial step in building reliable generative AI models. Most works in this area use a discrete-time formulation, which is prone to induced errors, and often not applicable to models with higher-order/black-box solvers. The objective of this study is to develop a disciplined approach to fine-tune diffusion models using continuous-time RL, formulated as a stochastic control problem with a reward function that aligns the end result (terminal state) with input prompt. The key idea is to treat score matching as controls or actions, and thereby making connections to policy optimization and regularization in continuous-time RL. To carry out this idea, we lay out a new policy optimization framework for continuous-time RL, and illustrate its potential in enhancing the value networks design space via leveraging the structural property of diffusion models. We validate the advantages of our method by experiments in downstream tasks of fine-tuning large-scale Text2Image models of Stable Diffusion v1.5.",
        "tags": [
            "Diffusion",
            "RL",
            "Score Matching"
        ]
    },
    {
        "id": "45",
        "title": "Scalable 3D Gaussian Splatting-Based RF Signal Spatial Propagation Modeling",
        "author": [
            "Kang Yang",
            "Gaofeng Dong",
            "Sijie Ji",
            "Wan Du",
            "Mani Srivastava"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01826",
        "abstract": "Effective network planning and sensing in wireless networks require resource-intensive site surveys for data collection. An alternative is Radio-Frequency (RF) signal spatial propagation modeling, which computes received signals given transceiver positions in a scene (e.g.s a conference room). We identify a fundamental trade-off between scalability and fidelity in the state-of-the-art method. To address this issue, we explore leveraging 3D Gaussian Splatting (3DGS), an advanced technique for the image synthesis of 3D scenes in real-time from arbitrary camera poses. By integrating domain-specific insights, we design three components for adapting 3DGS to the RF domain, including Gaussian-based RF scene representation, gradient-guided RF attribute learning, and RF-customized CUDA for ray tracing. Building on them, we develop RFSPM, an end-to-end framework for scalable RF signal Spatial Propagation Modeling. We evaluate RFSPM in four field studies and two applications across RFID, BLE, LoRa, and 5G, covering diverse frequencies, antennas, signals, and scenes. The results show that RFSPM matches the fidelity of the state-of-the-art method while reducing data requirements, training GPU-hours, and inference latency by up to 9.8\\,$\\times$, 18.6\\,$\\times$, and 84.4\\,$\\times$, respectively.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "LoRA"
        ]
    },
    {
        "id": "46",
        "title": "Efficient Denial of Service Attack Detection in IoT using Kolmogorov-Arnold Networks",
        "author": [
            "Oleksandr Kuznetsov"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01835",
        "abstract": "The proliferation of Internet of Things (IoT) devices has created a pressing need for efficient security solutions, particularly against Denial of Service (DoS) attacks. While existing detection approaches demonstrate high accuracy, they often require substantial computational resources, making them impractical for IoT deployment. This paper introduces a novel lightweight approach to DoS attack detection based on Kolmogorov-Arnold Networks (KANs). By leveraging spline-based transformations instead of traditional weight matrices, our solution achieves state-of-the-art detection performance while maintaining minimal resource requirements. Experimental evaluation on the CICIDS2017 dataset demonstrates 99.0% detection accuracy with only 0.19 MB memory footprint and 2.00 ms inference time per sample. Compared to existing solutions, KAN reduces memory requirements by up to 98% while maintaining competitive detection rates. The model's linear computational complexity ensures efficient scaling with input size, making it particularly suitable for large-scale IoT deployments. We provide comprehensive performance comparisons with recent approaches and demonstrate effectiveness across various DoS attack patterns. Our solution addresses the critical challenge of implementing sophisticated attack detection on resource-constrained devices, offering a practical approach to enhancing IoT security without compromising computational efficiency.",
        "tags": [
            "Detection",
            "KAN",
            "Kolmogorov-Arnold Networks"
        ]
    },
    {
        "id": "47",
        "title": "Texture Image Synthesis Using Spatial GAN Based on Vision Transformers",
        "author": [
            "Elahe Salari",
            "Zohreh Azimifar"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01842",
        "abstract": "Texture synthesis is a fundamental task in computer vision, whose goal is to generate visually realistic and structurally coherent textures for a wide range of applications, from graphics to scientific simulations. While traditional methods like tiling and patch-based techniques often struggle with complex textures, recent advancements in deep learning have transformed this field. In this paper, we propose ViT-SGAN, a new hybrid model that fuses Vision Transformers (ViTs) with a Spatial Generative Adversarial Network (SGAN) to address the limitations of previous methods. By incorporating specialized texture descriptors such as mean-variance (mu, sigma) and textons into the self-attention mechanism of ViTs, our model achieves superior texture synthesis. This approach enhances the model's capacity to capture complex spatial dependencies, leading to improved texture quality that is superior to state-of-the-art models, especially for regular and irregular textures. Comparison experiments with metrics such as FID, IS, SSIM, and LPIPS demonstrate the substantial improvement of ViT-SGAN, which underlines its efficiency in generating diverse realistic textures.",
        "tags": [
            "GAN",
            "ViT"
        ]
    },
    {
        "id": "48",
        "title": "UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping",
        "author": [
            "Aashish Rai",
            "Dilin Wang",
            "Mihir Jain",
            "Nikolaos Sarafianos",
            "Arthur Chen",
            "Srinath Sridhar",
            "Aayush Prakash"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01846",
        "abstract": "3D Gaussian Splatting (3DGS) has demonstrated superior quality in modeling 3D objects and scenes. However, generating 3DGS remains challenging due to their discrete, unstructured, and permutation-invariant nature. In this work, we present a simple yet effective method to overcome these challenges. We utilize spherical mapping to transform 3DGS into a structured 2D representation, termed UVGS. UVGS can be viewed as multi-channel images, with feature dimensions as a concatenation of Gaussian attributes such as position, scale, color, opacity, and rotation. We further find that these heterogeneous features can be compressed into a lower-dimensional (e.g., 3-channel) shared feature space using a carefully designed multi-branch network. The compressed UVGS can be treated as typical RGB images. Remarkably, we discover that typical VAEs trained with latent diffusion models can directly generalize to this new representation without additional training. Our novel representation makes it effortless to leverage foundational 2D models, such as diffusion models, to directly model 3DGS. Additionally, one can simply increase the 2D UV resolution to accommodate more Gaussians, making UVGS a scalable solution compared to typical 3D backbones. This approach immediately unlocks various novel generation applications of 3DGS by inherently utilizing the already developed superior 2D generation capabilities. In our experiments, we demonstrate various unconditional, conditional generation, and inpainting applications of 3DGS based on diffusion models, which were previously non-trivial.",
        "tags": [
            "3D",
            "Diffusion",
            "Gaussian Splatting",
            "Inpainting"
        ]
    },
    {
        "id": "49",
        "title": "Robust virtual element methods for 3D stress-assisted diffusion problems",
        "author": [
            "Andres E. Rubiano"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01851",
        "abstract": "This paper presents an initial exploration of stress-assisted diffusion problems in three dimensions within the framework of the virtual element method (VEM). Hilbert spaces enriched with parameter-weighted norms, the extended BabuÅ¡ka-Brezzi-Braess theory for perturbed saddle-point problems, and Banach fixed-point theory play a crucial role in performing a robust analysis of the fully coupled non-linear system. The proposed virtual element formulations are provided with appropriate projection, interpolation, and stabilisation operators that ensures the well-posedness of the discrete problem. Numerical simulations are conducted to show the accuracy, performance, and applicability of the method.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "50",
        "title": "Security and Quality in LLM-Generated Code: A Multi-Language, Multi-Model Analysis",
        "author": [
            "Mohammed Kharma",
            "Soohyeon Choi",
            "Mohammed AlKhanafseh",
            "David Mohaisen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01853",
        "abstract": "Artificial Intelligence (AI)-driven code generation tools are increasingly used throughout the software development lifecycle to accelerate coding tasks. However, the security of AI-generated code using Large Language Models (LLMs) remains underexplored, with studies revealing various risks and weaknesses. This paper analyzes the security of code generated by LLMs across different programming languages. We introduce a dataset of 200 tasks grouped into six categories to evaluate the performance of LLMs in generating secure and maintainable code. Our research shows that while LLMs can automate code creation, their security effectiveness varies by language. Many models fail to utilize modern security features in recent compiler and toolkit updates, such as Java 17. Moreover, outdated methods are still commonly used, particularly in C++. This highlights the need for advancing LLMs to enhance security and quality while incorporating emerging best practices in programming languages.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "51",
        "title": "SE Arena: Benchmarking Software Engineering Chatbots with Iterative Interactions",
        "author": [
            "Zhimin Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01860",
        "abstract": "Foundation models (FMs), particularly large language models (LLMs), have shown significant promise in various software engineering (SE) tasks, including code generation, debugging, and requirement refinement. Despite these advances, existing evaluation frameworks are insufficient for assessing model performance in iterative, context-rich workflows characteristic of SE activities. To address this limitation, we introduce SE Arena, an interactive platform designed to evaluate SE-focused chatbots. SE Arena provides a transparent, open-source leaderboard, supports multi-round conversational workflows, and enables end-to-end model comparisons. Moreover, SE Arena incorporates a new feature called RepoChat, which automatically injects repository-related context (e.g., issues, commits, pull requests) into the conversation, further aligning evaluations with real-world development processes. This paper outlines the design and capabilities of SE Arena, emphasizing its potential to advance the evaluation and practical application of FMs in software engineering.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "52",
        "title": "Latent Lexical Projection in Large Language Models: A Novel Approach to Implicit Representation Refinement",
        "author": [
            "Ziad Shaker",
            "Brendan Ashdown",
            "Hugo Fitzalan",
            "Alistair Heathcote",
            "Jocasta Huntington"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01882",
        "abstract": "Generating semantically coherent text requires a robust internal representation of linguistic structures, which traditional embedding techniques often fail to capture adequately. A novel approach, Latent Lexical Projection (LLP), is introduced to refine lexical representations through a structured transformation into a latent space, thereby enhancing the alignment between input embeddings and their contextual meanings. The method integrates an optimized projection mechanism within an existing language model architecture, enabling more accurate token selection while maintaining syntactic integrity. Evaluations across multiple benchmarks indicate a reduction in perplexity and an increase in BLEU scores, suggesting improvements in predictive accuracy and fluency. The analysis of lexical diversity reveals a more varied vocabulary in generated text, addressing common issues of redundancy and repetitive phrase structures. Further assessments of entropy distributions demonstrate a decline in uncertainty during decoding, reflecting enhanced confidence in word selection. Additionally, long-range dependency retention exhibits measurable gains, with increased classification accuracy at extended token distances. Computational efficiency remains within manageable constraints, despite the added projection mechanism, highlighting the practicality of LLP for integration into existing architectures.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "53",
        "title": "Conceptual Metaphor Theory as a Prompting Paradigm for Large Language Models",
        "author": [
            "Oliver Kramer"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01901",
        "abstract": "We introduce Conceptual Metaphor Theory (CMT) as a framework for enhancing large language models (LLMs) through cognitive prompting in complex reasoning tasks. CMT leverages metaphorical mappings to structure abstract reasoning, improving models' ability to process and explain intricate concepts. By incorporating CMT-based prompts, we guide LLMs toward more structured and human-like reasoning patterns. To evaluate this approach, we compare four native models (Llama3.2, Phi3, Gemma2, and Mistral) against their CMT-augmented counterparts on benchmark tasks spanning domain-specific reasoning, creative insight, and metaphor interpretation. Responses were automatically evaluated using the Llama3.3 70B model. Experimental results indicate that CMT prompting significantly enhances reasoning accuracy, clarity, and metaphorical coherence, outperforming baseline models across all evaluated tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "54",
        "title": "Rethinking Homogeneity of Vision and Text Tokens in Large Vision-and-Language Models",
        "author": [
            "Chia-Wen Kuo",
            "Sijie Zhu",
            "Fan Chen",
            "Xiaohui Shen",
            "Longyin Wen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01906",
        "abstract": "Large vision-and-language models (LVLMs) typically treat visual and textual embeddings as homogeneous inputs to a large language model (LLM). However, these inputs are inherently different: visual inputs are multi-dimensional and contextually rich, often pre-encoded by models like CLIP, while textual inputs lack this structure. In this paper, we propose Decomposed Attention (D-Attn), a novel method that processes visual and textual embeddings differently by decomposing the 1-D causal self-attention in LVLMs. After the attention decomposition, D-Attn diagonalizes visual-to-visual self-attention, reducing computation from $\\mathcal{O}(|V|^2)$ to $\\mathcal{O}(|V|)$ for $|V|$ visual embeddings without compromising performance. Moreover, D-Attn debiases positional encodings in textual-to-visual cross-attention, further enhancing visual understanding. Finally, we introduce an $\\alpha$-weighting strategy to merge visual and textual information, maximally preserving the pre-trained LLM's capabilities with minimal modifications. Extensive experiments and rigorous analyses validate the effectiveness of D-Attn, demonstrating significant improvements on multiple image benchmarks while significantly reducing computational costs. Code, data, and models will be publicly available.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "55",
        "title": "Unlocking Efficient Large Inference Models: One-Bit Unrolling Tips the Scales",
        "author": [
            "Arian Eamaz",
            "Farhang Yeganegi",
            "Mojtaba Soltanalian"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01908",
        "abstract": "Recent advancements in Large Language Model (LLM) compression, such as BitNet and BitNet b1.58, have marked significant strides in reducing the computational demands of LLMs through innovative one-bit quantization techniques. We extend this frontier by looking at Large Inference Models (LIMs) that have become indispensable across various applications. However, their scale and complexity often come at a significant computational cost. We introduce a novel approach that leverages one-bit algorithm unrolling, effectively integrating information from the physical world in the model architecture. Our method achieves a bit-per-link rate significantly lower than the 1.58 bits reported in prior work, thanks to the natural sparsity that emerges in our network architectures. We numerically demonstrate that the proposed one-bit algorithm unrolling scheme can improve both training and test outcomes by effortlessly increasing the number of layers while substantially compressing the network. Additionally, we provide theoretical results on the generalization gap, convergence rate, stability, and sensitivity of our proposed one-bit algorithm unrolling.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "56",
        "title": "Composite Gaussian Processes Flows for Learning Discontinuous Multimodal Policies",
        "author": [
            "Shu-yuan Wang",
            "Hikaru Sasaki",
            "Takamitsu Matsubara"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01913",
        "abstract": "Learning control policies for real-world robotic tasks often involve challenges such as multimodality, local discontinuities, and the need for computational efficiency. These challenges arise from the complexity of robotic environments, where multiple solutions may coexist. To address these issues, we propose Composite Gaussian Processes Flows (CGP-Flows), a novel semi-parametric model for robotic policy. CGP-Flows integrate Overlapping Mixtures of Gaussian Processes (OMGPs) with the Continuous Normalizing Flows (CNFs), enabling them to model complex policies addressing multimodality and local discontinuities. This hybrid approach retains the computational efficiency of OMGPs while incorporating the flexibility of CNFs. Experiments conducted in both simulated and real-world robotic tasks demonstrate that CGP-flows significantly improve performance in modeling control policies. In a simulation task, we confirmed that CGP-Flows had a higher success rate compared to the baseline method, and the success rate of GCP-Flow was significantly different from the success rate of other baselines in chi-square tests.",
        "tags": [
            "Normalizing Flows"
        ]
    },
    {
        "id": "57",
        "title": "PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling",
        "author": [
            "Avery Ma",
            "Yangchen Pan",
            "Amir-massoud Farahmand"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01925",
        "abstract": "Many-shot jailbreaking circumvents the safety alignment of large language models by exploiting their ability to process long input sequences. To achieve this, the malicious target prompt is prefixed with hundreds of fabricated conversational turns between the user and the model. These fabricated exchanges are randomly sampled from a pool of malicious questions and responses, making it appear as though the model has already complied with harmful instructions. In this paper, we present PANDAS: a hybrid technique that improves many-shot jailbreaking by modifying these fabricated dialogues with positive affirmations, negative demonstrations, and an optimized adaptive sampling method tailored to the target prompt's topic. Extensive experiments on AdvBench and HarmBench, using state-of-the-art LLMs, demonstrate that PANDAS significantly outperforms baseline methods in long-context scenarios. Through an attention analysis, we provide insights on how long-context vulnerabilities are exploited and show how PANDAS further improves upon many-shot jailbreaking.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "58",
        "title": "Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs",
        "author": [
            "Angelina Wang",
            "Michelle Phan",
            "Daniel E. Ho",
            "Sanmi Koyejo"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01926",
        "abstract": "Algorithmic fairness has conventionally adopted a perspective of racial color-blindness (i.e., difference unaware treatment). We contend that in a range of important settings, group difference awareness matters. For example, differentiating between groups may be necessary in legal contexts (e.g., the U.S. compulsory draft applies to men but not women) and harm assessments (e.g., calling a girl a terrorist may be less harmful than calling a Muslim person one). In our work we first introduce an important distinction between descriptive (fact-based), normative (value-based), and correlation (association-based) benchmarks. This distinction is significant because each category requires distinct interpretation and mitigation tailored to its specific characteristics. Then, we present a benchmark suite composed of eight different scenarios for a total of 16k questions that enables us to assess difference awareness. Finally, we show results across ten models that demonstrate difference awareness is a distinct dimension of fairness where existing bias mitigation strategies may backfire.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "59",
        "title": "Distributionally Robust Direct Preference Optimization",
        "author": [
            "Zaiyan Xu",
            "Sushil Vemuri",
            "Kishan Panaganti",
            "Dileep Kalathil",
            "Rahul Jain",
            "Deepak Ramachandran"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01930",
        "abstract": "A major challenge in aligning large language models (LLMs) with human preferences is the issue of distribution shift. LLM alignment algorithms rely on static preference datasets, assuming that they accurately represent real-world user preferences. However, user preferences vary significantly across geographical regions, demographics, linguistic patterns, and evolving cultural trends. This preference distribution shift leads to catastrophic alignment failures in many real-world applications. We address this problem using the principled framework of distributionally robust optimization, and develop two novel distributionally robust direct preference optimization (DPO) algorithms, namely, Wasserstein DPO (WDPO) and Kullback-Leibler DPO (KLDPO). We characterize the sample complexity of learning the optimal policy parameters for WDPO and KLDPO. Moreover, we propose scalable gradient descent-style learning algorithms by developing suitable approximations for the challenging minimax loss functions of WDPO and KLDPO. Our empirical experiments demonstrate the superior performance of WDPO and KLDPO in substantially improving the alignment when there is a preference distribution shift.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "60",
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "author": [
            "Xiang Liu",
            "Zhenheng Tang",
            "Hong Chen",
            "Peijie Dong",
            "Zeyu Li",
            "Xiuze Zhou",
            "Bo Li",
            "Xuming Hu",
            "Xiaowen Chu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01941",
        "abstract": "This paper investigates an under-explored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. While existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain understudied. We present a comprehensive empirical study evaluating prominent KV cache compression methods across diverse tasks, spanning world knowledge, commonsense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and http://generation.Our analysis reveals that KV cache compression methods exhibit task-specific performance degradation. Arithmetic reasoning tasks prove particularly sensitive to aggressive compression, with different methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the DeepSeek R1 Distill model exhibits more robust compression tolerance compared to instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance degradation. Based on our analysis of attention patterns and cross-task compression performance, we propose ShotKV, a novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$ performance improvements on long-context generation tasks under aggressive compression ratios.",
        "tags": [
            "DeepSeek",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "61",
        "title": "DAMO: Data- and Model-aware Alignment of Multi-modal LLMs",
        "author": [
            "Jinda Lu",
            "Junkang Wu",
            "Jinghan Li",
            "Xiaojun Jia",
            "Shuo Wang",
            "YiFan Zhang",
            "Junfeng Fang",
            "Xiang Wang",
            "Xiangnan He"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01943",
        "abstract": "Direct Preference Optimization (DPO) has shown effectiveness in aligning multi-modal large language models (MLLM) with human preferences. However, existing methods exhibit an imbalanced responsiveness to the data of varying hardness, tending to overfit on the easy-to-distinguish data while underfitting on the hard-to-distinguish data. In this paper, we propose Data- and Model-aware DPO (DAMO) to dynamically adjust the optimization process from two key aspects: (1) a data-aware strategy that incorporates data hardness, and (2) a model-aware strategy that integrates real-time model responses. By combining the two strategies, DAMO enables the model to effectively adapt to data with varying levels of hardness. Extensive experiments on five benchmarks demonstrate that DAMO not only significantly enhances the trustworthiness, but also improves the effectiveness over general tasks. For instance, on the Object HalBench, our DAMO-7B reduces response-level and mentioned-level hallucination by 90.0% and 95.3%, respectively, surpassing the performance of GPT-4V.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "62",
        "title": "HeRCULES: Heterogeneous Radar Dataset in Complex Urban Environment for Multi-session Radar SLAM",
        "author": [
            "Hanjun Kim",
            "Minwoo Jung",
            "Chiyun Noh",
            "Sangwoo Jung",
            "Hyunho Song",
            "Wooseong Yang",
            "Hyesu Jang",
            "Ayoung Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01946",
        "abstract": "Recently, radars have been widely featured in robotics for their robustness in challenging weather conditions. Two commonly used radar types are spinning radars and phased-array radars, each offering distinct sensor characteristics. Existing datasets typically feature only a single type of radar, leading to the development of algorithms limited to that specific kind. In this work, we highlight that combining different radar types offers complementary advantages, which can be leveraged through a heterogeneous radar dataset. Moreover, this new dataset fosters research in multi-session and multi-robot scenarios where robots are equipped with different types of radars. In this context, we introduce the HeRCULES dataset, a comprehensive, multi-modal dataset with heterogeneous radars, FMCW LiDAR, IMU, GPS, and cameras. This is the first dataset to integrate 4D radar and spinning radar alongside FMCW LiDAR, offering unparalleled localization, mapping, and place recognition capabilities. The dataset covers diverse weather and lighting conditions and a range of urban traffic scenarios, enabling a comprehensive analysis across various environments. The sequence paths with multiple revisits and ground truth pose for each sensor enhance its suitability for place recognition research. We expect the HeRCULES dataset to facilitate odometry, mapping, place recognition, and sensor fusion research. The dataset and development tools are available at https://sites.google.com/view/herculesdataset.",
        "tags": [
            "Robot",
            "Robotics",
            "SLAM"
        ]
    },
    {
        "id": "63",
        "title": "LAYOUTDREAMER: Physics-guided Layout for Text-to-3D Compositional Scene Generation",
        "author": [
            "Yang Zhou",
            "Zongjin He",
            "Qixuan Li",
            "Chao Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01949",
        "abstract": "Recently, the field of text-guided 3D scene generation has garnered significant attention. High-quality generation that aligns with physical realism and high controllability is crucial for practical 3D scene applications. However, existing methods face fundamental limitations: (i) difficulty capturing complex relationships between multiple objects described in the text, (ii) inability to generate physically plausible scene layouts, and (iii) lack of controllability and extensibility in compositional scenes. In this paper, we introduce LayoutDreamer, a framework that leverages 3D Gaussian Splatting (3DGS) to facilitate high-quality, physically consistent compositional scene generation guided by text. Specifically, given a text prompt, we convert it into a directed scene graph and adaptively adjust the density and layout of the initial compositional 3D Gaussians. Subsequently, dynamic camera adjustments are made based on the training focal point to ensure entity-level generation quality. Finally, by extracting directed dependencies from the scene graph, we tailor physical and layout energy to ensure both realism and flexibility. Comprehensive experiments demonstrate that LayoutDreamer outperforms other compositional scene generation quality and semantic alignment methods. Specifically, it achieves state-of-the-art (SOTA) performance in the multiple objects generation metric of T3Bench.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Text-to-3D"
        ]
    },
    {
        "id": "64",
        "title": "On the Emergence of Position Bias in Transformers",
        "author": [
            "Xinyi Wu",
            "Yifei Wang",
            "Stefanie Jegelka",
            "Ali Jadbabaie"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01951",
        "abstract": "Recent studies have revealed various manifestations of position bias in transformer architectures, from the \"lost-in-the-middle\" phenomenon to attention sinks, yet a comprehensive theoretical understanding of how attention masks and positional encodings shape these biases remains elusive. This paper introduces a novel graph-theoretic framework to analyze position bias in multi-layer attention. Modeling attention masks as directed graphs, we quantify how tokens interact with contextual information based on their sequential positions. We uncover two key insights: First, causal masking inherently biases attention toward earlier positions, as tokens in deeper layers attend to increasingly more contextualized representations of earlier tokens. Second, we characterize the competing effects of the causal mask and relative positional encodings, such as the decay mask and rotary positional encoding (RoPE): while both mechanisms introduce distance-based decay within individual attention maps, their aggregate effect across multiple attention layers -- coupled with the causal mask -- leads to a trade-off between the long-term decay effects and the cumulative importance of early sequence positions. Through controlled numerical experiments, we not only validate our theoretical findings but also reproduce position biases observed in real-world LLMs. Our framework offers a principled foundation for understanding positional biases in transformers, shedding light on the complex interplay of attention mechanism components and guiding more informed architectural design.",
        "tags": [
            "LLMs",
            "Transformer"
        ]
    },
    {
        "id": "65",
        "title": "Constrained belief updates explain geometric structures in transformer representations",
        "author": [
            "Mateusz Piotrowski",
            "Paul M. Riechers",
            "Daniel Filan",
            "Adam S. Shai"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01954",
        "abstract": "What computational structures emerge in transformers trained on next-token prediction? In this work, we provide evidence that transformers implement constrained Bayesian belief updating -- a parallelized version of partial Bayesian inference shaped by architectural constraints. To do this, we integrate the model-agnostic theory of optimal prediction with mechanistic interpretability to analyze transformers trained on a tractable family of hidden Markov models that generate rich geometric patterns in neural activations. We find that attention heads carry out an algorithm with a natural interpretation in the probability simplex, and create representations with distinctive geometric structure. We show how both the algorithmic behavior and the underlying geometry of these representations can be theoretically predicted in detail -- including the attention pattern, OV-vectors, and embedding vectors -- by modifying the equations for optimal future token predictions to account for the architectural constraints of attention. Our approach provides a principled lens on how gradient descent resolves the tension between optimal prediction and architectural design.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "66",
        "title": "Memory Efficient Transformer Adapter for Dense Predictions",
        "author": [
            "Dong Zhang",
            "Rui Yan",
            "Pingcheng Dong",
            "Kwang-Ting Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01962",
        "abstract": "While current Vision Transformer (ViT) adapter methods have shown promising accuracy, their inference speed is implicitly hindered by inefficient memory access operations, e.g., standard normalization and frequent reshaping. In this work, we propose META, a simple and fast ViT adapter that can improve the model's memory efficiency and decrease memory time consumption by reducing the inefficient memory access operations. Our method features a memory-efficient adapter block that enables the common sharing of layer normalization between the self-attention and feed-forward network layers, thereby reducing the model's reliance on normalization operations. Within the proposed block, the cross-shaped self-attention is employed to reduce the model's frequent reshaping operations. Moreover, we augment the adapter block with a lightweight convolutional branch that can enhance local inductive biases, particularly beneficial for the dense prediction tasks, e.g., object detection, instance segmentation, and semantic segmentation. The adapter block is finally formulated in a cascaded manner to compute diverse head features, thereby enriching the variety of feature representations. Empirically, extensive evaluations on multiple representative datasets validate that META substantially enhances the predicted quality, while achieving a new state-of-the-art accuracy-efficiency trade-off. Theoretically, we demonstrate that META exhibits superior generalization capability and stronger adaptability.",
        "tags": [
            "Detection",
            "Segmentation",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "67",
        "title": "Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning",
        "author": [
            "Jinlong Pang",
            "Na Di",
            "Zhaowei Zhu",
            "Jiaheng Wei",
            "Hao Cheng",
            "Chen Qian",
            "Yang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01968",
        "abstract": "Recent studies show that in supervised fine-tuning (SFT) of large language models (LLMs), data quality matters more than quantity. While most data cleaning methods concentrate on filtering entire samples, the quality of individual tokens within a sample can vary significantly. After pre-training, even in high-quality samples, patterns or phrases that are not task-related can be redundant or uninformative. Continuing to fine-tune on these patterns may offer limited benefit and even degrade downstream task performance. In this paper, we investigate token quality from a noisy-label perspective and propose a generic token cleaning pipeline for SFT tasks. Our method filters out uninformative tokens while preserving those carrying key task-specific information. Specifically, we first evaluate token quality by examining the influence of model updates on each token, then apply a threshold-based separation. The token influence can be measured in a single pass with a fixed reference model or iteratively with self-evolving reference models. The benefits and limitations of both methods are analyzed theoretically by error upper bounds. Extensive experiments show that our framework consistently improves performance across multiple downstream tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "68",
        "title": "CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing",
        "author": [
            "Wenhao Zheng",
            "Yixiao Chen",
            "Weitong Zhang",
            "Souvik Kundu",
            "Yun Li",
            "Zhengzhong Liu",
            "Eric P. Xing",
            "Hongyi Wang",
            "Huaxiu Yao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01976",
        "abstract": "Large language models have achieved remarkable success in various tasks but suffer from high computational costs during inference, limiting their deployment in resource-constrained applications. To address this issue, we propose a novel CITER (\\textbf{C}ollaborative \\textbf{I}nference with \\textbf{T}oken-l\\textbf{E}vel \\textbf{R}outing) framework that enables efficient collaboration between small and large language models (SLMs & LLMs) through a token-level routing strategy. Specifically, CITER routes non-critical tokens to an SLM for efficiency and routes critical tokens to an LLM for generalization quality. We formulate router training as a policy optimization, where the router receives rewards based on both the quality of predictions and the inference costs of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate the reward evaluation process, we introduce a shortcut which significantly reduces the costs of the reward estimation and improving the practicality of our approach. Extensive experiments on five benchmark datasets demonstrate that CITER reduces the inference costs while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "69",
        "title": "AutoGUI: Scaling GUI Grounding with Automatic Functionality Annotations from LLMs",
        "author": [
            "Hongxin Li",
            "Jingfan Chen",
            "Jingran Su",
            "Yuntao Chen",
            "Qing Li",
            "Zhaoxiang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01977",
        "abstract": "User interface understanding with vision-language models has received much attention due to its potential for enabling next-generation software automation. However, existing UI datasets either only provide large-scale context-free element annotations or contextualized functional descriptions for elements at a much smaller scale. In this work, we propose the \\methodname{} pipeline for automatically annotating UI elements with detailed functionality descriptions at scale. Specifically, we leverage large language models (LLMs) to infer element functionality by comparing the UI content changes before and after simulated interactions with specific UI elements. To improve annotation quality, we propose LLM-aided rejection and verification, eliminating invalid and incorrect annotations without human labor. We construct an \\methodname{}-704k dataset using the proposed pipeline, featuring multi-resolution, multi-device screenshots, diverse data domains, and detailed functionality annotations that have never been provided by previous datasets. Human evaluation shows that the AutoGUI pipeline achieves annotation correctness comparable to trained human annotators. Extensive experimental results show that our \\methodname{}-704k dataset remarkably enhances VLM's UI grounding capabilities, exhibits significant scaling effects, and outperforms existing web pre-training data types. We envision AutoGUI as a scalable pipeline for generating massive data to build GUI-oriented VLMs. AutoGUI dataset can be viewed at this anonymous URL: https://autogui-project.github.io/.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "70",
        "title": "Gradient-Regularized Latent Space Modulation in Large Language Models for Structured Contextual Synthesis",
        "author": [
            "Derek Yotheringhay",
            "Beatrix Nightingale",
            "Maximilian Featherstone",
            "Edmund Worthington",
            "Hugo Ashdown"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01979",
        "abstract": "Generating structured textual content requires mechanisms that enforce coherence, stability, and adherence to predefined constraints while maintaining semantic fidelity. Conventional approaches often rely on rule-based heuristics or fine-tuning strategies that lack flexibility and generalizability across diverse tasks. The incorporation of Gradient-Regularized Latent Space Modulation (GRLSM) introduces a novel paradigm for guiding text generation through the application of structured constraints within the latent space. The integration of gradient-based regularization mitigates abrupt variations in latent representations, ensuring a smoother encoding process that enhances structural consistency and logical progression within generated sequences. Comparative evaluations demonstrate that latent space modulation leads to a reduction in perplexity, increased coherence scores, and improved structural alignment across multiple domains. Stability assessments further indicate that the imposition of spectral norm constraints facilitates more controlled variations in generated text, preserving semantic consistency under input perturbations. Empirical results confirm that structured latent space constraints not only refine the organization of generated outputs but also enhance interpretability through more predictable and reliable synthesis patterns. Performance metrics illustrate that the GRLSM framework substantially reduces structural inconsistencies while preserving the generative flexibility inherent in neural models.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "71",
        "title": "Generative Data Mining with Longtail-Guided Diffusion",
        "author": [
            "David S. Hayden",
            "Mao Ye",
            "Timur Garipov",
            "Gregory P. Meyer",
            "Carl Vondrick",
            "Zhao Chen",
            "Yuning Chai",
            "Eric Wolff",
            "Siddhartha S. Srinivasa"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01980",
        "abstract": "It is difficult to anticipate the myriad challenges that a predictive model will encounter once deployed. Common practice entails a reactive, cyclical approach: model deployment, data mining, and retraining. We instead develop a proactive longtail discovery process by imagining additional data during training. In particular, we develop general model-based longtail signals, including a differentiable, single forward pass formulation of epistemic uncertainty that does not impact model parameters or predictive performance but can flag rare or hard inputs. We leverage these signals as guidance to generate additional training data from a latent diffusion model in a process we call Longtail Guidance (LTG). Crucially, we can perform LTG without retraining the diffusion model or the predictive model, and we do not need to expose the predictive model to intermediate diffusion states. Data generated by LTG exhibit semantically meaningful variation, yield significant generalization improvements on image classification benchmarks, and can be analyzed to proactively discover, explain, and address conceptual gaps in a predictive model.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "72",
        "title": "DCT-Mamba3D: Spectral Decorrelation and Spatial-Spectral Feature Extraction for Hyperspectral Image Classification",
        "author": [
            "Weijia Cao",
            "Xiaofei Yang",
            "Yicong Zhou",
            "Zheng Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01986",
        "abstract": "Hyperspectral image classification presents challenges due to spectral redundancy and complex spatial-spectral dependencies. This paper proposes a novel framework, DCT-Mamba3D, for hyperspectral image classification. DCT-Mamba3D incorporates: (1) a 3D spectral-spatial decorrelation module that applies 3D discrete cosine transform basis functions to reduce both spectral and spatial redundancy, enhancing feature clarity across dimensions; (2) a 3D-Mamba module that leverages a bidirectional state-space model to capture intricate spatial-spectral dependencies; and (3) a global residual enhancement module that stabilizes feature representation, improving robustness and convergence. Extensive experiments on benchmark datasets show that our DCT-Mamba3D outperforms the state-of-the-art methods in challenging scenarios such as the same object in different spectra and different objects in the same spectra.",
        "tags": [
            "3D",
            "Mamba"
        ]
    },
    {
        "id": "73",
        "title": "T-SCEND: Test-time Scalable MCTS-enhanced Diffusion Model",
        "author": [
            "Tao Zhang",
            "Jia-Shu Pan",
            "Ruiqi Feng",
            "Tailin Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01989",
        "abstract": "We introduce Test-time Scalable MCTS-enhanced Diffusion Model (T-SCEND), a novel framework that significantly improves diffusion model's reasoning capabilities with better energy-based training and scaling up test-time computation. We first show that naÃ¯vely scaling up inference budget for diffusion models yields marginal gain. To address this, the training of T-SCEND consists of a novel linear-regression negative contrastive learning objective to improve the performance-energy consistency of the energy landscape, and a KL regularization to reduce adversarial sampling. During inference, T-SCEND integrates the denoising process with a novel hybrid Monte Carlo Tree Search (hMCTS), which sequentially performs best-of-N random search and MCTS as denoising proceeds. On challenging reasoning tasks of Maze and Sudoku, we demonstrate the effectiveness of T-SCEND's training objective and scalable inference method. In particular, trained with Maze sizes of up to $6\\times6$, our T-SCEND solves $88\\%$ of Maze problems with much larger sizes of $15\\times15$, while standard diffusion completely http://fails.Code to reproduce the experiments can be found at https://github.com/AI4Science-WestlakeU/t_scend.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "74",
        "title": "Rethinking Timesteps Samplers and Prediction Types",
        "author": [
            "Bin Xie",
            "Gady Agam"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01990",
        "abstract": "Diffusion models suffer from the huge consumption of time and resources to train. For example, diffusion models need hundreds of GPUs to train for several weeks for a high-resolution generative task to meet the requirements of an extremely large number of iterations and a large batch size. Training diffusion models become a millionaire's game. With limited resources that only fit a small batch size, training a diffusion model always fails. In this paper, we investigate the key reasons behind the difficulties of training diffusion models with limited resources. Through numerous experiments and demonstrations, we identified a major factor: the significant variation in the training losses across different timesteps, which can easily disrupt the progress made in previous iterations. Moreover, different prediction types of $x_0$ exhibit varying effectiveness depending on the task and timestep. We hypothesize that using a mixed-prediction approach to identify the most accurate $x_0$ prediction type could potentially serve as a breakthrough in addressing this issue. In this paper, we outline several challenges and insights, with the hope of inspiring further research aimed at tackling the limitations of training diffusion models with constrained resources, particularly for high-resolution tasks.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "75",
        "title": "Can LLMs Assist Annotators in Identifying Morality Frames? -- Case Study on Vaccination Debate on Social Media",
        "author": [
            "Tunazzina Islam",
            "Dan Goldwasser"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01991",
        "abstract": "Nowadays, social media is pivotal in shaping public discourse, especially on polarizing issues like vaccination, where diverse moral perspectives influence individual opinions. In NLP, data scarcity and complexity of psycholinguistic tasks such as identifying morality frames makes relying solely on human annotators costly, time-consuming, and prone to inconsistency due to cognitive load. To address these issues, we leverage large language models (LLMs), which are adept at adapting new tasks through few-shot learning, utilizing a handful of in-context examples coupled with explanations that connect examples to task principles. Our research explores LLMs' potential to assist human annotators in identifying morality frames within vaccination debates on social media. We employ a two-step process: generating concepts and explanations with LLMs, followed by human evaluation using a \"think-aloud\" tool. Our study shows that integrating LLMs into the annotation process enhances accuracy, reduces task difficulty, lowers cognitive load, suggesting a promising avenue for human-AI collaboration in complex psycholinguistic tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "76",
        "title": "One Diffusion Step to Real-World Super-Resolution via Flow Trajectory Distillation",
        "author": [
            "Jianze Li",
            "Jiezhang Cao",
            "Yong Guo",
            "Wenbo Li",
            "Yulun Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01993",
        "abstract": "Diffusion models (DMs) have significantly advanced the development of real-world image super-resolution (Real-ISR), but the computational cost of multi-step diffusion models limits their application. One-step diffusion models generate high-quality images in a one sampling step, greatly reducing computational overhead and inference latency. However, most existing one-step diffusion methods are constrained by the performance of the teacher model, where poor teacher performance results in image artifacts. To address this limitation, we propose FluxSR, a novel one-step diffusion Real-ISR technique based on flow matching models. We use the state-of-the-art diffusion model FLUX.1-dev as both the teacher model and the base model. First, we introduce Flow Trajectory Distillation (FTD) to distill a multi-step flow matching model into a one-step Real-ISR. Second, to improve image realism and address high-frequency artifact issues in generated images, we propose TV-LPIPS as a perceptual loss and introduce Attention Diversification Loss (ADL) as a regularization term to reduce token similarity in transformer, thereby eliminating high-frequency artifacts. Comprehensive experiments demonstrate that our method outperforms existing one-step diffusion-based Real-ISR methods. The code and model will be released at https://github.com/JianzeLi-114/FluxSR.",
        "tags": [
            "Diffusion",
            "FLUX",
            "Flow Matching",
            "Super Resolution",
            "Transformer"
        ]
    },
    {
        "id": "77",
        "title": "Reasoning Bias of Next Token Prediction Training",
        "author": [
            "Pengxiao Lin",
            "Zhongwang Zhang",
            "Zhi-Qin John Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02007",
        "abstract": "Since the inception of Large Language Models (LLMs), the quest to efficiently train them for superior reasoning capabilities has been a pivotal challenge. The dominant training paradigm for LLMs is based on next token prediction (NTP). Alternative methodologies, called Critical Token Prediction (CTP), focused exclusively on specific critical tokens (such as the answer in Q\\&A dataset), aiming to reduce the overfitting of extraneous information and noise. Contrary to initial assumptions, our research reveals that despite NTP's exposure to noise during training, it surpasses CTP in reasoning ability. We attribute this counterintuitive outcome to the regularizing influence of noise on the training dynamics. Our empirical analysis shows that NTP-trained models exhibit enhanced generalization and robustness across various benchmark reasoning datasets, demonstrating greater resilience to perturbations and achieving flatter loss minima. These findings illuminate that NTP is instrumental in fostering reasoning abilities during pretraining, whereas CTP is more effective for finetuning, thereby enriching our comprehension of optimal training strategies in LLM development.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "78",
        "title": "LLMSecConfig: An LLM-Based Approach for Fixing Software Container Misconfigurations",
        "author": [
            "Ziyang Ye",
            "Triet Huynh Minh Le",
            "M. Ali Babar"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02009",
        "abstract": "Security misconfigurations in Container Orchestrators (COs) can pose serious threats to software systems. While Static Analysis Tools (SATs) can effectively detect these security vulnerabilities, the industry currently lacks automated solutions capable of fixing these misconfigurations. The emergence of Large Language Models (LLMs), with their proven capabilities in code understanding and generation, presents an opportunity to address this limitation. This study introduces LLMSecConfig, an innovative framework that bridges this gap by combining SATs with LLMs. Our approach leverages advanced prompting techniques and Retrieval-Augmented Generation (RAG) to automatically repair security misconfigurations while preserving operational functionality. Evaluation of 1,000 real-world Kubernetes configurations achieved a 94\\% success rate while maintaining a low rate of introducing new misconfigurations.\nOur work makes a promising step towards automated container security management, reducing the manual effort required for configuration maintenance.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "79",
        "title": "Layer by Layer: Uncovering Hidden Representations in Language Models",
        "author": [
            "Oscar Skean",
            "Md Rifat Arefin",
            "Dan Zhao",
            "Niket Patel",
            "Jalal Naghiyev",
            "Yann LeCun",
            "Ravid Shwartz-Ziv"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02013",
        "abstract": "From extracting features to generating text, the outputs of large language models (LLMs) typically rely on their final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a wide range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each model layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layer's performance. Through extensive experiments on 32 text-embedding tasks and comparisons across model architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features. These findings challenge the standard focus on final-layer embeddings and open new directions for model analysis and optimization, including strategic use of mid-layer representations for more robust and accurate AI systems.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "State Space Models"
        ]
    },
    {
        "id": "80",
        "title": "Analytical Lyapunov Function Discovery: An RL-based Generative Approach",
        "author": [
            "Haohan Zou",
            "Jie Feng",
            "Hao Zhao",
            "Yuanyuan Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02014",
        "abstract": "Despite advances in learning-based methods, finding valid Lyapunov functions for nonlinear dynamical systems remains challenging. Current neural network approaches face two main issues: challenges in scalable verification and limited interpretability. To address these, we propose an end-to-end framework using transformers to construct analytical Lyapunov functions (local), which simplifies formal verification, enhances interpretability, and provides valuable insights for control engineers. Our framework consists of a transformer-based trainer that generates candidate Lyapunov functions and a falsifier that verifies candidate expressions and refines the model via risk-seeking policy gradient. Unlike Alfarano et al. (2024), which utilizes pre-training and seeks global Lyapunov functions for low-dimensional systems, our model is trained from scratch via reinforcement learning (RL) and succeeds in finding local Lyapunov functions for high-dimensional and non-polynomial systems. Given the analytical nature of the candidates, we employ efficient optimization methods for falsification during training and formal verification tools for the final verification. We demonstrate the efficiency of our approach on a range of nonlinear dynamical systems with up to ten dimensions and show that it can discover Lyapunov functions not previously identified in the control literature.",
        "tags": [
            "RL",
            "Transformer"
        ]
    },
    {
        "id": "81",
        "title": "A Periodic Bayesian Flow for Material Generation",
        "author": [
            "Hanlin Wu",
            "Yuxuan Song",
            "Jingjing Gong",
            "Ziyao Cao",
            "Yawen Ouyang",
            "Jianbing Zhang",
            "Hao Zhou",
            "Wei-Ying Ma",
            "Jingjing Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02016",
        "abstract": "Generative modeling of crystal data distribution is an important yet challenging task due to the unique periodic physical symmetry of crystals. Diffusion-based methods have shown early promise in modeling crystal distribution. More recently, Bayesian Flow Networks were introduced to aggregate noisy latent variables, resulting in a variance-reduced parameter space that has been shown to be advantageous for modeling Euclidean data distributions with structural constraints (Song et al., 2023). Inspired by this, we seek to unlock its potential for modeling variables located in non-Euclidean manifolds e.g. those within crystal structures, by overcoming challenging theoretical issues. We introduce CrysBFN, a novel crystal generation method by proposing a periodic Bayesian flow, which essentially differs from the original Gaussian-based BFN by exhibiting non-monotonic entropy dynamics. To successfully realize the concept of periodic Bayesian flow, CrysBFN integrates a new entropy conditioning mechanism and empirically demonstrates its significance compared to time-conditioning. Extensive experiments over both crystal ab initio generation and crystal structure prediction tasks demonstrate the superiority of CrysBFN, which consistently achieves new state-of-the-art on all benchmarks. Surprisingly, we found that CrysBFN enjoys a significant improvement in sampling efficiency, e.g., ~100x speedup 10 v.s. 2000 steps network forwards) compared with previous diffusion-based methods on MP-20 dataset. Code is available at https://github.com/wu-han-lin/CrysBFN.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "82",
        "title": "From Accidents to Insights: Leveraging Multimodal Data for Scenario-Driven ADS Testing",
        "author": [
            "Siwei Luo",
            "Yang Zhang",
            "Yao Deng",
            "Xi Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02025",
        "abstract": "The rapid advancements in Autonomous Driving Systems (ADS) have necessitated robust software testing to ensure safety and reliability. However, automating the generation of scalable and concrete test scenarios remains a significant challenge. Current scenario-based test case generation methods often face limitations, such as unrealistic scenes and inaccurate vehicle trajectories. These challenges largely result from the loss of map information during data extraction and the lack of an effective verification mechanism to mitigate hallucinations in large language models (LLMs). This paper introduces TRACE, a scenario-based ADS Test case Generation framework for Critical Scenarios. By leveraging multimodal data to extract challenging scenarios from real-world car crash reports, TRACE constructs numerous critical test cases with less data, significantly enhancing ADS bug detection efficiency. Using in-context learning, chain-of-thought prompting, and self-validation approaches, we use LLMs to extract environmental and road network information from crash reports. For vehicle trajectory planning, data containing map information and vehicle coordinates serves as a knowledge base to build a ChatGPT-based LLM with path-planning capabilities, which we named TrackMate. Based on 50 existing crash reports, our approach successfully tested three ADS models across two simulation platforms, MetaDrive and BeamNG. Of the 290 constructed test scenarios, 127 are identified as critical, as they resulted in vehicle collisions. Additionally, user feedback reveals that TRACE demonstrates superior scenario reconstruction accuracy, with 77.5% of the scenarios being rated as 'mostly or 'totally' consistent, compared to only 27% for the most related SOTA, LCTGen.",
        "tags": [
            "ChatGPT",
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "83",
        "title": "From Human Hands to Robotic Limbs: A Study in Motor Skill Embodiment for Telemanipulation",
        "author": [
            "Haoyi Shi",
            "Mingxi Su",
            "Ted Morris",
            "Vassilios Morellas",
            "Nikolaos Papanikolopoulos"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02036",
        "abstract": "This paper presents a teleoperation system for controlling a redundant degree of freedom robot manipulator using human arm gestures. We propose a GRU-based Variational Autoencoder to learn a latent representation of the manipulator's configuration space, capturing its complex joint kinematics. A fully connected neural network maps human arm configurations into this latent space, allowing the system to mimic and generate corresponding manipulator trajectories in real time through the VAE decoder. The proposed method shows promising results in teleoperating the manipulator, enabling the generation of novel manipulator configurations from human features that were not present during training.",
        "tags": [
            "Robot",
            "VAE"
        ]
    },
    {
        "id": "84",
        "title": "M2R2: Mixture of Multi-Rate Residuals for Efficient Transformer Inference",
        "author": [
            "Nikhil Bhendawade",
            "Mahyar Najibi",
            "Devang Naik",
            "Irina Belousova"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02040",
        "abstract": "Residual transformations enhance the representational depth and expressive power of large language models (LLMs). However, applying static residual transformations across all tokens in auto-regressive generation leads to a suboptimal trade-off between inference efficiency and generation fidelity. Existing methods, including Early Exiting, Skip Decoding, and Mixture-of-Depth address this by modulating the residual transformation based on token-level complexity. Nevertheless, these approaches predominantly consider the distance traversed by tokens through the model layers, neglecting the underlying velocity of residual evolution. We introduce Mixture of Multi-rate Residuals (M2R2), a framework that dynamically modulates residual velocity to improve early alignment, enhancing inference efficiency. Evaluations on reasoning oriented tasks such as Koala, Self-Instruct, WizardLM, and MT-Bench show M2R2 surpasses state-of-the-art distance-based strategies, balancing generation quality and speedup. In self-speculative decoding setup, M2R2 achieves up to 2.8x speedups on MT-Bench, outperforming methods like 2-model speculative decoding, Medusa, LookAhead Decoding, and DEED. In Mixture-of-Experts (MoE) architectures, integrating early residual alignment with ahead-of-time expert loading into high-bandwidth memory (HBM) accelerates decoding, reduces expert-switching bottlenecks, and achieves a 2.9x speedup, making it highly effective in resource-constrained environments.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "85",
        "title": "Contextual Memory Reweaving in Large Language Models Using Layered Latent State Reconstruction",
        "author": [
            "Frederick Dillon",
            "Gregor Halvorsen",
            "Simon Tattershall",
            "Magnus Rowntree",
            "Gareth Vanderpool"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02046",
        "abstract": "Memory retention challenges in deep neural architectures have ongoing limitations in the ability to process and recall extended contextual information. Token dependencies degrade as sequence length increases, leading to a decline in coherence and factual consistency across longer outputs. A structured approach is introduced to mitigate this issue through the reweaving of latent states captured at different processing layers, reinforcing token representations over extended sequences. The proposed Contextual Memory Reweaving framework incorporates a Layered Latent State Reconstruction mechanism to systematically integrate past contextual embeddings without introducing external memory modules. Experimental results demonstrate improvements in recall accuracy across a range of sequence lengths, with notable gains in the retention of rarely occurring tokens and numerical reasoning consistency. Further analysis of computational efficiency indicates that the additional processing overhead remains within acceptable thresholds, enabling scalability across different model sizes. Evaluations in long-form text generation and ambiguous query resolution highlight the capacity of memory reweaving to enhance continuity and reduce inconsistencies over extended outputs. Attention weight distributions reveal more structured allocation patterns, suggesting that reweaved latent states contribute to improved contextual awareness. The findings establish a framework for refining memory retention mechanisms in language models, addressing long-standing challenges in handling complex, multi-step reasoning tasks.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "86",
        "title": "AmaSQuAD: A Benchmark for Amharic Extractive Question Answering",
        "author": [
            "Nebiyou Daniel Hailemariam",
            "Blessed Guda",
            "Tsegazeab Tefferi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02047",
        "abstract": "This research presents a novel framework for translating extractive question-answering datasets into low-resource languages, as demonstrated by the creation of the AmaSQuAD dataset, a translation of SQuAD 2.0 into Amharic. The methodology addresses challenges related to misalignment between translated questions and answers, as well as the presence of multiple answer instances in the translated context. For this purpose, we used cosine similarity utilizing embeddings from a fine-tuned BERT-based model for Amharic and Longest Common Subsequence (LCS). Additionally, we fine-tune the XLM-R model on the AmaSQuAD synthetic dataset for Amharic Question-Answering. The results show an improvement in baseline performance, with the fine-tuned model achieving an increase in the F1 score from 36.55% to 44.41% and 50.01% to 57.5% on the AmaSQuAD development dataset. Moreover, the model demonstrates improvement on the human-curated AmQA dataset, increasing the F1 score from 67.80% to 68.80% and the exact match score from 52.50% to 52.66%.The AmaSQuAD dataset is publicly available Datasets",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "87",
        "title": "Efficient Domain Adaptation of Multimodal Embeddings using Constrastive Learning",
        "author": [
            "Georgios Margaritis",
            "Periklis Petridis",
            "Dimitris J. Bertsimas"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02048",
        "abstract": "Recent advancements in machine learning (ML), natural language processing (NLP), and foundational models have shown promise for real-life applications in critical, albeit compute-constrainted fields like healthcare.\nIn such areas, combining foundational models with supervised ML offers potential for automating tasks like diagnosis and treatment planning, but the limited availability of onsite computational resources pose significant challenges before applying these technologies effectively: Current approaches either yield subpar results when using pretrained models without task-specific adaptation, or require substantial computational resources for fine-tuning, which is often a barrier to entry in such environments.\nThis renders them inaccessible in applications where performance and quality standards are high, but computational resources are scarce.\nTo bridge the gap between best-in-class performance and accessibility, we propose a novel method for adapting foundational, multimodal embeddings to downstream tasks, without the need of expensive fine-tuning processes.\nOur method leverages frozen embeddings from Large Language Models (LLMs) and Vision Models, and uses contrastive learning to train a small, task-specific nonlinear projection that can be used in the downstream task, without having to fine-tune the original foundational models.\nWe show that this efficient procedure leads to significant performance improvements across various downstream tasks, and perhaps more importantly with minimal computational overhead, offering a practical solution for the use of advanced, foundational ML models in resource-constrained settings.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "88",
        "title": "CASIM: Composite Aware Semantic Injection for Text to Motion Generation",
        "author": [
            "Che-Jui Chang",
            "Qingze Tony Liu",
            "Honglu Zhou",
            "Vladimir Pavlovic",
            "Mubbasir Kapadia"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02063",
        "abstract": "Recent advances in generative modeling and tokenization have driven significant progress in text-to-motion generation, leading to enhanced quality and realism in generated motions. However, effectively leveraging textual information for conditional motion generation remains an open challenge. We observe that current approaches, primarily relying on fixed-length text embeddings (e.g., CLIP) for global semantic injection, struggle to capture the composite nature of human motion, resulting in suboptimal motion quality and controllability. To address this limitation, we propose the Composite Aware Semantic Injection Mechanism (CASIM), comprising a composite-aware semantic encoder and a text-motion aligner that learns the dynamic correspondence between text and motion tokens. Notably, CASIM is model and representation-agnostic, readily integrating with both autoregressive and diffusion-based methods. Experiments on HumanML3D and KIT benchmarks demonstrate that CASIM consistently improves motion quality, text-motion alignment, and retrieval scores across state-of-the-art methods. Qualitative analyses further highlight the superiority of our composite-aware approach over fixed-length semantic injection, enabling precise motion control from text prompts and stronger generalization to unseen text inputs.",
        "tags": [
            "CLIP",
            "Diffusion"
        ]
    },
    {
        "id": "89",
        "title": "Anticipate & Act : Integrating LLMs and Classical Planning for Efficient Task Execution in Household Environments",
        "author": [
            "Raghav Arora",
            "Shivam Singh",
            "Karthik Swaminathan",
            "Ahana Datta",
            "Snehasis Banerjee",
            "Brojeshwar Bhowmick",
            "Krishna Murthy Jatavallabhula",
            "Mohan Sridharan",
            "Madhava Krishna"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02066",
        "abstract": "Assistive agents performing household tasks such as making the bed or cooking breakfast often compute and execute actions that accomplish one task at a time. However, efficiency can be improved by anticipating upcoming tasks and computing an action sequence that jointly achieves these tasks. State-of-the-art methods for task anticipation use data-driven deep networks and Large Language Models (LLMs), but they do so at the level of high-level tasks and/or require many training examples. Our framework leverages the generic knowledge of LLMs through a small number of prompts to perform high-level task anticipation, using the anticipated tasks as goals in a classical planning system to compute a sequence of finer-granularity actions that jointly achieve these goals. We ground and evaluate our framework's abilities in realistic scenarios in the VirtualHome environment and demonstrate a 31% reduction in execution time compared with a system that does not consider upcoming tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "90",
        "title": "AdaptBot: Combining LLM with Knowledge Graphs and Human Input for Generic-to-Specific Task Decomposition and Knowledge Refinement",
        "author": [
            "Shivam Singh",
            "Karthik Swaminathan",
            "Nabanita Dash",
            "Ramandeep Singh",
            "Snehasis Banerjee",
            "Mohan Sridharan",
            "Madhava Krishna"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02067",
        "abstract": "Embodied agents assisting humans are often asked to complete a new task in a new scenario. An agent preparing a particular dish in the kitchen based on a known recipe may be asked to prepare a new dish or to perform cleaning tasks in the storeroom. There may not be sufficient resources, e.g., time or labeled examples, to train the agent for these new situations. Large Language Models (LLMs) trained on considerable knowledge across many domains are able to predict a sequence of abstract actions for such new tasks and scenarios, although it may not be possible for the agent to execute this action sequence due to task-, agent-, or domain-specific constraints. Our framework addresses these challenges by leveraging the generic predictions provided by LLM and the prior domain-specific knowledge encoded in a Knowledge Graph (KG), enabling an agent to quickly adapt to new tasks and scenarios. The robot also solicits and uses human input as needed to refine its existing knowledge. Based on experimental evaluation over cooking and cleaning tasks in simulation domains, we demonstrate that the interplay between LLM, KG, and human input leads to substantial performance gains compared with just using the LLM output.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Robot"
        ]
    },
    {
        "id": "91",
        "title": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models",
        "author": [
            "Yuto Kojima",
            "Jiarui Xu",
            "Xueyan Zou",
            "Xiaolong Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02069",
        "abstract": "The rapid advancements in vision-language models (VLMs), such as CLIP, have intensified the need to address distribution shifts between training and testing datasets. Although prior Test-Time Training (TTT) techniques for VLMs have demonstrated robust performance, they predominantly rely on tuning text prompts, a process that demands substantial computational resources and is heavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a novel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively to the image encoder of VLMs. By introducing LoRA and updating only its parameters during test time, our method offers a simple yet effective TTT approach, retaining the model's initial generalization capability while achieving substantial performance gains with minimal memory and runtime overhead. Additionally, we introduce a highly efficient reconstruction loss tailored for TTT. Our method can adapt to diverse domains by combining these two losses, without increasing memory consumption or runtime. Extensive experiments on two benchmarks, covering 15 datasets, demonstrate that our method improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of 5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently surpassing test-time prompt tuning, without relying on any external models or cache.",
        "tags": [
            "CLIP",
            "LoRA",
            "Low-Rank Adaptation",
            "Test-Time Training",
            "ViT"
        ]
    },
    {
        "id": "92",
        "title": "ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping",
        "author": [
            "Rajiv Bahl",
            "Venkatesan N",
            "Parimal Aglawe",
            "Aastha Sarasapalli",
            "Bhavya Kancharla",
            "Chaitanya kolukuluri",
            "Harish Mohite",
            "Japneet Hora",
            "Kiran Kakollu",
            "Rahul Diman",
            "Shubham Kapale",
            "Sri Bhagya Kathula",
            "Vamsikrishna Motru",
            "Yogeshwar Reddy"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02072",
        "abstract": "The rapid evolution of Large Language Models (LLMs) has transformed natural language processing but raises critical concerns about biases inherent in their deployment and use across diverse linguistic and sociocultural contexts. This paper presents a framework named ASCenD BDS (Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping). The framework presents approach to detecting bias, discrimination, stereotyping across various categories such as gender, caste, age, disability, socioeconomic status, linguistic variations, etc., using an approach which is Adaptive, Stochastic and Context-Aware. The existing frameworks rely heavily on usage of datasets to generate scenarios for detection of Bias, Discrimination and Stereotyping. Examples include datasets such as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ. However, such an approach provides point solutions. As a result, these datasets provide a finite number of scenarios for assessment. The current framework overcomes this limitation by having features which enable Adaptability, Stochasticity, Context Awareness. Context awareness can be customized for any nation or culture or sub-culture (for example an organization's unique culture). In this paper, context awareness in the Indian context has been established. Content has been leveraged from Indian Census 2011 to have a commonality of categorization. A framework has been developed using Category, Sub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability, Stochasticity and Context awareness. The framework has been described in detail in Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories were developed by a team of consultants at Saint Fox Consultancy Private Ltd. The concept has been tested out in SFCLabs as part of product development.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "93",
        "title": "Rethinking stance detection: A theoretically-informed research agenda for user-level inference using language models",
        "author": [
            "Prasanta Bhattacharya",
            "Hong Zhang",
            "Yiming Cao",
            "Wei Gao",
            "Brandon Siyuan Loh",
            "Joseph J.P. Simons",
            "Liang Ze Wong"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02074",
        "abstract": "Stance detection has emerged as a popular task in natural language processing research, enabled largely by the abundance of target-specific social media data. While there has been considerable research on the development of stance detection models, datasets, and application, we highlight important gaps pertaining to (i) a lack of theoretical conceptualization of stance, and (ii) the treatment of stance at an individual- or user-level, as opposed to message-level. In this paper, we first review the interdisciplinary origins of stance as an individual-level construct to highlight relevant attributes (e.g., psychological features) that might be useful to incorporate in stance detection models. Further, we argue that recent pre-trained and large language models (LLMs) might offer a way to flexibly infer such user-level attributes and/or incorporate them in modelling stance. To better illustrate this, we briefly review and synthesize the emerging corpus of studies on using LLMs for inferring stance, and specifically on incorporating user attributes in such tasks. We conclude by proposing a four-point agenda for pursuing stance detection research that is theoretically informed, inclusive, and practically impactful.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "94",
        "title": "IPO: Iterative Preference Optimization for Text-to-Video Generation",
        "author": [
            "Xiaomeng Yang",
            "Zhiyu Tan",
            "Xuecheng Nie",
            "Hao Li"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02088",
        "abstract": "Video foundation models have achieved significant advancement with the help of network upgrade as well as model scale-up. However, they are still hard to meet requirements of applications due to unsatisfied generation quality. To solve this problem, we propose to align video foundation models with human preferences from the perspective of post-training in this paper. Consequently, we introduce an Iterative Preference Optimization strategy to enhance generated video quality by incorporating human feedback. Specifically, IPO exploits a critic model to justify video generations for pairwise ranking as in Direct Preference Optimization or point-wise scoring as in Kahneman-Tversky Optimization. Given this, IPO optimizes video foundation models with guidance of signals from preference feedback, which helps improve generated video quality in subject consistency, motion smoothness and aesthetic quality, etc. In addition, IPO incorporates the critic model with the multi-modality large language model, which enables it to automatically assign preference labels without need of retraining or relabeling. In this way, IPO can efficiently perform multi-round preference optimization in an iterative manner, without the need of tediously manual labeling. Comprehensive experiments demonstrate that the proposed IPO can effectively improve the video generation quality of a pretrained model and help a model with only 2B parameters surpass the one with 5B parameters. Besides, IPO achieves new state-of-the-art performance on VBench benchmark. We will release our source codes, models as well as dataset to advance future research and applications.",
        "tags": [
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "95",
        "title": "Implicit integration factor method coupled with Pad\\'{e} approximation strategy for nonlocal Allen-Cahn equation",
        "author": [
            "Yuxin Zhang",
            "Hengfei Ding"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02089",
        "abstract": "The space nonlocal Allen-Cahn equation is a famous example of fractional reaction-diffusion equations. It is also an extension of the classical Allen-Cahn equation, which is widely used in physics to describe the phenomenon of two-phase fluid http://flows.Due to the nonlocality of the nonlocal operator, numerical solutions to these equations face considerable http://challenges.It is worth noting that whether we use low-order or high-order numerical differential formulas to approximate the operator, the corresponding matrix is always dense, which implies that the storage space and computational cost required for the former and the latter are the same. However, the higher-order formula can significantly improve the accuracy of the numerical http://scheme.Therefore, the primary goal of this paper is to construct a high-order numerical formula that approximates the nonlocal http://operator.To reduce the time step limitation in existing numerical algorithms, we employed a technique combining the compact integration factor method with the PadÃ© approximation strategy to discretize the time derivative.A novel high-order numerical scheme, which satisfies both the maximum principle and energy stability for the space nonlocal Allen-Cahn equation, is http://proposed.Furthermore, we provide a detailed error analysis of the differential scheme, which shows that its convergence order is $\\mathcal{O}\\left(\\tau^2+h^6\\right)$.Especially, it is worth mentioning that the fully implicit scheme with sixth-order accuracy in spatial has never been proven to maintain the maximum principle and energy stability http://before.Finally, some numerical experiments are carried out to demonstrate the efficiency of the proposed method.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "96",
        "title": "LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information",
        "author": [
            "Bowen Ping",
            "Jiali Zeng",
            "Fandong Meng",
            "Shuo Wang",
            "Jie Zhou",
            "Shanghang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02095",
        "abstract": "Long-form generation is crucial for academic writing papers and repo-level code generation. Despite this, current models, including GPT-4o, still exhibit unsatisfactory performance. Existing methods that utilize preference learning with outcome supervision often fail to provide detailed feedback for extended contexts. This shortcoming can lead to content that does not fully satisfy query requirements, resulting in issues like length deviations, and diminished quality. In this paper, we propose enhancing long-form generation by incorporating process supervision. We employ Monte Carlo Tree Search to gather stepwise preference pairs, utilizing a global memory pool to maintain consistency. To address the issue of suboptimal candidate selection, we integrate external critiques to refine and improve the quality of the preference pairs. Finally, we apply step-level DPO using the collected stepwise preference pairs. Experimental results show that our method improves length and quality on long-form generation benchmarks, with almost lossless performance on general benchmarks across various model backbones.",
        "tags": [
            "GPT",
            "LLMs"
        ]
    },
    {
        "id": "97",
        "title": "Topic Modeling in Marathi",
        "author": [
            "Sanket Shinde",
            "Raviraj Joshi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02100",
        "abstract": "While topic modeling in English has become a prevalent and well-explored area, venturing into topic modeling for Indic languages remains relatively rare. The limited availability of resources, diverse linguistic structures, and unique challenges posed by Indic languages contribute to the scarcity of research and applications in this domain. Despite the growing interest in natural language processing and machine learning, there exists a noticeable gap in the comprehensive exploration of topic modeling methodologies tailored specifically for languages such as Hindi, Marathi, Tamil, and others. In this paper, we examine several topic modeling approaches applied to the Marathi language. Specifically, we compare various BERT and non-BERT approaches, including multilingual and monolingual BERT models, using topic coherence and topic diversity as evaluation metrics. Our analysis provides insights into the performance of these approaches for Marathi language topic modeling. The key finding of the paper is that BERTopic, when combined with BERT models trained on Indic languages, outperforms LDA in terms of topic modeling performance.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "98",
        "title": "BRIDLE: Generalized Self-supervised Learning with Quantization",
        "author": [
            "Hoang M. Nguyen",
            "Satya N. Shukla",
            "Qiang Zhang",
            "Hanchao Yu",
            "Sreya D. Roy",
            "Taipeng Tian",
            "Lingjiong Zhu",
            "Yuchen Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02118",
        "abstract": "Self-supervised learning has been a powerful approach for learning meaningful representations from unlabeled data across various domains, reducing the reliance on large labeled datasets. Inspired by BERT's success in capturing deep bidirectional contexts in natural language processing, similar frameworks have been adapted to other modalities such as audio, with models like BEATs extending the bidirectional training paradigm to audio signals using vector quantization (VQ). However, these frameworks face challenges, notably their dependence on a single codebook for quantization, which may not capture the complex, multifaceted nature of signals. In addition, inefficiencies in codebook utilization lead to underutilized code vectors. To address these limitations, we introduce BRIDLE (Bidirectional Residual Quantization Interleaved Discrete Learning Encoder), a self-supervised encoder pretraining framework that incorporates residual quantization (RQ) into the bidirectional training process, and is generalized for pretraining with audio, image, and video. Using multiple hierarchical codebooks, RQ enables fine-grained discretization in the latent space, enhancing representation quality. BRIDLE involves an interleaved training procedure between the encoder and tokenizer. We evaluate BRIDLE on audio understanding tasks using classification benchmarks, achieving state-of-the-art results, and demonstrate competitive performance on image classification and video classification tasks, showing consistent improvements over traditional VQ methods in downstream performance.",
        "tags": [
            "BERT",
            "Vector Quantization"
        ]
    },
    {
        "id": "99",
        "title": "Risk-Aware Driving Scenario Analysis with Large Language Models",
        "author": [
            "Yuan Gao",
            "Mattia Piccinini",
            "Johannes Betz"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02145",
        "abstract": "Large Language Models (LLMs) can capture nuanced contextual relationships, reasoning, and complex problem-solving. By leveraging their ability to process and interpret large-scale information, LLMs have shown potential to address domain-specific challenges, including those in autonomous driving systems. This paper proposes a novel framework that leverages LLMs for risk-aware analysis of generated driving scenarios. We hypothesize that LLMs can effectively evaluate whether driving scenarios generated by autonomous driving testing simulators are safety-critical. To validate this hypothesis, we conducted an empirical evaluation to assess the effectiveness of LLMs in performing this task. This framework will also provide feedback to generate the new safety-critical scenario by using adversarial method to modify existing non-critical scenarios and test their effectiveness in validating motion planning algorithms. Code and scenarios are available at: https://github.com/yuangao-tum/Riskaware-Scenario-analyse",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "100",
        "title": "On the Guidance of Flow Matching",
        "author": [
            "Ruiqi Feng",
            "Tailin Wu",
            "Chenglei Yu",
            "Wenhao Deng",
            "Peiyan Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02150",
        "abstract": "Flow matching has shown state-of-the-art performance in various generative tasks, ranging from image generation to decision-making, where guided generation is pivotal. However, the guidance of flow matching is more general than and thus substantially different from that of its predecessor, diffusion models. Therefore, the challenge in guidance for general flow matching remains largely underexplored. In this paper, we propose the first framework of general guidance for flow matching. From this framework, we derive a family of guidance techniques that can be applied to general flow matching. These include a new training-free asymptotically exact guidance, novel training losses for training-based guidance, and two classes of approximate guidance that cover classical gradient guidance methods as special cases. We theoretically investigate these different methods to give a practical guideline for choosing suitable methods in different scenarios. Experiments on synthetic datasets, image inverse problems, and offline reinforcement learning demonstrate the effectiveness of our proposed guidance methods and verify the correctness of our flow matching guidance framework. Code to reproduce the experiments can be found at https://github.com/AI4Science-WestlakeU/flow_guidance.",
        "tags": [
            "Diffusion",
            "Flow Matching"
        ]
    },
    {
        "id": "101",
        "title": "EditIQ: Automated Cinematic Editing of Static Wide-Angle Videos via Dialogue Interpretation and Saliency Cues",
        "author": [
            "Rohit Girmaji",
            "Bhav Beri",
            "Ramanathan Subramanian",
            "Vineet Gandhi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02172",
        "abstract": "We present EditIQ, a completely automated framework for cinematically editing scenes captured via a stationary, large field-of-view and high-resolution camera. From the static camera feed, EditIQ initially generates multiple virtual feeds, emulating a team of cameramen. These virtual camera shots termed rushes are subsequently assembled using an automated editing algorithm, whose objective is to present the viewer with the most vivid scene content. To understand key scene elements and guide the editing process, we employ a two-pronged approach: (1) a large language model (LLM)-based dialogue understanding module to analyze conversational flow, coupled with (2) visual saliency prediction to identify meaningful scene elements and camera shots therefrom. We then formulate cinematic video editing as an energy minimization problem over shot selection, where cinematic constraints determine shot choices, transitions, and continuity. EditIQ synthesizes an aesthetically and visually compelling representation of the original narrative while maintaining cinematic coherence and a smooth viewing experience. Efficacy of EditIQ against competing baselines is demonstrated via a psychophysical study involving twenty participants on the BBC Old School dataset plus eleven theatre performance videos. Video samples from EditIQ can be found at https://editiq-ave.github.io/.",
        "tags": [
            "Video Editing"
        ]
    },
    {
        "id": "102",
        "title": "Mass-Editing Memory with Attention in Transformers: A cross-lingual exploration of knowledge",
        "author": [
            "Daniel Tamayo",
            "Aitor Gonzalez-Agirre",
            "Javier Hernando",
            "Marta Villegas"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02173",
        "abstract": "Recent research has explored methods for updating and modifying factual knowledge in large language models, often focusing on specific multi-layer perceptron blocks. This study expands on this work by examining the effectiveness of existing knowledge editing methods across languages and delving into the role of attention mechanisms in this process. Drawing from the insights gained, we propose Mass-Editing Memory with Attention in Transformers (MEMAT), a method that achieves significant improvements in all metrics while requiring minimal parameter modifications. MEMAT delivers a remarkable 10% increase in magnitude metrics, benefits languages not included in the training data and also demonstrates a high degree of portability. Our code and data are at https://github.com/dtamayo-nlp/MEMAT.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "103",
        "title": "ShapeShifter: 3D Variations Using Multiscale and Sparse Point-Voxel Diffusion",
        "author": [
            "Nissim Maruani",
            "Wang Yifan",
            "Matthew Fisher",
            "Pierre Alliez",
            "Mathieu Desbrun"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02187",
        "abstract": "This paper proposes ShapeShifter, a new 3D generative model that learns to synthesize shape variations based on a single reference model. While generative methods for 3D objects have recently attracted much attention, current techniques often lack geometric details and/or require long training times and large resources. Our approach remedies these issues by combining sparse voxel grids and point, normal, and color sampling within a multiscale neural architecture that can be trained efficiently and in parallel. We show that our resulting variations better capture the fine details of their original input and can handle more general types of surfaces than previous SDF-based methods. Moreover, we offer interactive generation of 3D shape variants, allowing more human control in the design loop if needed.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "104",
        "title": "Large language models in climate and sustainability policy: limits and opportunities",
        "author": [
            "Francesca Larosa",
            "Sergio Hoyas",
            "H. Alberto Conejero",
            "Javier Garcia-Martinez",
            "Francesco Fuso Nerini",
            "Ricardo Vinuesa"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02191",
        "abstract": "As multiple crises threaten the sustainability of our societies and pose at risk the planetary boundaries, complex challenges require timely, updated, and usable information. Natural-language processing (NLP) tools enhance and expand data collection and processing and knowledge utilization capabilities to support the definition of an inclusive, sustainable future. In this work, we apply different NLP techniques, tools and approaches to climate and sustainability documents to derive policy-relevant and actionable measures. We focus on general and domain-specific large language models (LLMs) using a combination of static and prompt-based methods. We find that the use of LLMs is successful at processing, classifying and summarizing heterogeneous text-based data. However, we also encounter challenges related to human intervention across different workflow stages and knowledge utilization for policy processes. Our work presents a critical but empirically grounded application of LLMs to complex policy problems and suggests avenues to further expand Artificial Intelligence-powered computational social sciences.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "105",
        "title": "Exploiting Ensemble Learning for Cross-View Isolated Sign Language Recognition",
        "author": [
            "Fei Wang",
            "Kun Li",
            "Yiqi Nie",
            "Zhangling Duan",
            "Peng Zou",
            "Zhiliang Wu",
            "Yuwei Wang",
            "Yanyan Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02196",
        "abstract": "In this paper, we present our solution to the Cross-View Isolated Sign Language Recognition (CV-ISLR) challenge held at WWW 2025. CV-ISLR addresses a critical issue in traditional Isolated Sign Language Recognition (ISLR), where existing datasets predominantly capture sign language videos from a frontal perspective, while real-world camera angles often vary. To accurately recognize sign language from different viewpoints, models must be capable of understanding gestures from multiple angles, making cross-view recognition challenging. To address this, we explore the advantages of ensemble learning, which enhances model robustness and generalization across diverse views. Our approach, built on a multi-dimensional Video Swin Transformer model, leverages this ensemble strategy to achieve competitive performance. Finally, our solution ranked 3rd in both the RGB-based ISLR and RGB-D-based ISLR tracks, demonstrating the effectiveness in handling the challenges of cross-view recognition. The code is available at: https://github.com/Jiafei127/CV_ISLR_WWW2025.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "106",
        "title": "When Dimensionality Hurts: The Role of LLM Embedding Compression for Noisy Regression Tasks",
        "author": [
            "Felix Drinkall",
            "Janet B. Pierrehumbert",
            "Stefan Zohren"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02199",
        "abstract": "Large language models (LLMs) have shown remarkable success in language modelling due to scaling laws found in model size and the hidden dimension of the model's text representation. Yet, we demonstrate that compressed representations of text can yield better performance in LLM-based regression tasks. In this paper, we compare the relative performance of embedding compression in three different signal-to-noise contexts: financial return prediction, writing quality assessment and review scoring. Our results show that compressing embeddings, in a minimally supervised manner using an autoencoder's hidden representation, can mitigate overfitting and improve performance on noisy tasks, such as financial return prediction; but that compression reduces performance on tasks that have high causal dependencies between the input and target data. Our results suggest that the success of interpretable compressed representations such as sentiment may be due to a regularising effect.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "107",
        "title": "From Uncertain to Safe: Conformal Fine-Tuning of Diffusion Models for Safe PDE Control",
        "author": [
            "Peiyan Hu",
            "Xiaowei Qian",
            "Wenhao Deng",
            "Rui Wang",
            "Haodong Feng",
            "Ruiqi Feng",
            "Tao Zhang",
            "Long Wei",
            "Yue Wang",
            "Zhi-Ming Ma",
            "Tailin Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02205",
        "abstract": "The application of deep learning for partial differential equation (PDE)-constrained control is gaining increasing attention. However, existing methods rarely consider safety requirements crucial in real-world applications. To address this limitation, we propose Safe Diffusion Models for PDE Control (SafeDiffCon), which introduce the uncertainty quantile as model uncertainty quantification to achieve optimal control under safety constraints through both post-training and inference phases. Firstly, our approach post-trains a pre-trained diffusion model to generate control sequences that better satisfy safety constraints while achieving improved control objectives via a reweighted diffusion loss, which incorporates the uncertainty quantile estimated using conformal prediction. Secondly, during inference, the diffusion model dynamically adjusts both its generation process and parameters through iterative guidance and fine-tuning, conditioned on control targets while simultaneously integrating the estimated uncertainty quantile. We evaluate SafeDiffCon on three control tasks: 1D Burgers' equation, 2D incompressible fluid, and controlled nuclear fusion problem. Results demonstrate that SafeDiffCon is the only method that satisfies all safety constraints, whereas other classical and deep learning baselines fail. Furthermore, while adhering to safety constraints, SafeDiffCon achieves the best control performance.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "108",
        "title": "On the Expressivity of Selective State-Space Layers: A Multivariate Polynomial Approach",
        "author": [
            "Edo Cohen-Karlik",
            "Itamar Zimerman",
            "Liane Galanti",
            "Ido Atad",
            "Amir Globerson",
            "Lior Wolf"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02209",
        "abstract": "Recent advances in efficient sequence modeling have introduced selective state-space layers, a key component of the Mamba architecture, which have demonstrated remarkable success in a wide range of NLP and vision tasks. While Mamba's empirical performance has matched or surpassed SoTA transformers on such diverse benchmarks, the theoretical foundations underlying its powerful representational capabilities remain less explored. In this work, we investigate the expressivity of selective state-space layers using multivariate polynomials, and prove that they surpass linear transformers in expressiveness. Consequently, our findings reveal that Mamba offers superior representational power over linear attention-based models for long sequences, while not sacrificing their generalization. Our theoretical insights are validated by a comprehensive set of empirical experiments on various datasets.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "109",
        "title": "InterLCM: Low-Quality Images as Intermediate States of Latent Consistency Models for Effective Blind Face Restoration",
        "author": [
            "Senmao Li",
            "Kai Wang",
            "Joost van de Weijer",
            "Fahad Shahbaz Khan",
            "Chun-Le Guo",
            "Shiqi Yang",
            "Yaxing Wang",
            "Jian Yang",
            "Ming-Ming Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02215",
        "abstract": "Diffusion priors have been used for blind face restoration (BFR) by fine-tuning diffusion models (DMs) on restoration datasets to recover low-quality images. However, the naive application of DMs presents several key limitations. (i) The diffusion prior has inferior semantic consistency (e.g., ID, structure and color.), increasing the difficulty of optimizing the BFR model; (ii) reliance on hundreds of denoising iterations, preventing the effective cooperation with perceptual losses, which is crucial for faithful restoration. Observing that the latent consistency model (LCM) learns consistency noise-to-data mappings on the ODE-trajectory and therefore shows more semantic consistency in the subject identity, structural information and color preservation, we propose InterLCM to leverage the LCM for its superior semantic consistency and efficiency to counter the above issues. Treating low-quality images as the intermediate state of LCM, InterLCM achieves a balance between fidelity and quality by starting from earlier LCM steps. LCM also allows the integration of perceptual loss during training, leading to improved restoration quality, particularly in real-world scenarios. To mitigate structural and semantic uncertainties, InterLCM incorporates a Visual Module to extract visual features and a Spatial Encoder to capture spatial details, enhancing the fidelity of restored images. Extensive experiments demonstrate that InterLCM outperforms existing approaches in both synthetic and real-world datasets while also achieving faster inference speed.",
        "tags": [
            "Consistency Models",
            "Diffusion",
            "ODE"
        ]
    },
    {
        "id": "110",
        "title": "Exploring the latent space of diffusion models directly through singular value decomposition",
        "author": [
            "Li Wang",
            "Boyan Gao",
            "Yanran Li",
            "Zhao Wang",
            "Xiaosong Yang",
            "David A. Clifton",
            "Jun Xiao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02225",
        "abstract": "Despite the groundbreaking success of diffusion models in generating high-fidelity images, their latent space remains relatively under-explored, even though it holds significant promise for enabling versatile and interpretable image editing capabilities. The complicated denoising trajectory and high dimensionality of the latent space make it extremely challenging to interpret. Existing methods mainly explore the feature space of U-Net in Diffusion Models (DMs) instead of the latent space itself. In contrast, we directly investigate the latent space via Singular Value Decomposition (SVD) and discover three useful properties that can be used to control generation results without the requirements of data collection and maintain identity fidelity generated images. Based on these properties, we propose a novel image editing framework that is capable of learning arbitrary attributes from one pair of latent codes destined by text prompts in Stable Diffusion Models. To validate our approach, extensive experiments are conducted to demonstrate its effectiveness and flexibility in image editing. We will release our codes soon to foster further research and applications in this area.",
        "tags": [
            "Diffusion",
            "Image Editing"
        ]
    },
    {
        "id": "111",
        "title": "Using ChatGPT to refine draft conceptual schemata in supply-driven design of multidimensional cubes",
        "author": [
            "Stefano Rizzi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02238",
        "abstract": "Refinement is a critical step in supply-driven conceptual design of multidimensional cubes because it can hardly be automated. In fact, it includes steps such as the labeling of attributes as descriptive and the removal of uninteresting attributes, thus relying on the end-users' requirements on the one hand, and on the semantics of measures, dimensions, and attributes on the other. As a consequence, it is normally carried out manually by designers in close collaboration with end-users. The goal of this work is to check whether LLMs can act as facilitators for the refinement task, so as to let it be carried out entirely -- or mostly -- by end-users. The Dimensional Fact Model is the target formalism for our study; as a representative LLM, we use ChatGPT's model GPT-4o. To achieve our goal, we formulate three research questions aimed at (i) understanding the basic competences of ChatGPT in multidimensional modeling; (ii) understanding the basic competences of ChatGPT in refinement; and (iii) investigating if the latter can be improved via prompt engineering. The results of our experiments show that, indeed, a careful prompt engineering can significantly improve the accuracy of refinement, and that the residual errors can quickly be fixed via one additional prompt. However, we conclude that, at present, some involvement of designers in refinement is still necessary to ensure the validity of the refined schemata.",
        "tags": [
            "ChatGPT",
            "GPT",
            "LLMs"
        ]
    },
    {
        "id": "112",
        "title": "Adversarial ML Problems Are Getting Harder to Solve and to Evaluate",
        "author": [
            "Javier Rando",
            "Jie Zhang",
            "Nicholas Carlini",
            "Florian TramÃ¨r"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02260",
        "abstract": "In the past decade, considerable research effort has been devoted to securing machine learning (ML) models that operate in adversarial settings. Yet, progress has been slow even for simple \"toy\" problems (e.g., robustness to small adversarial perturbations) and is often hindered by non-rigorous evaluations. Today, adversarial ML research has shifted towards studying larger, general-purpose language models. In this position paper, we argue that the situation is now even worse: in the era of LLMs, the field of adversarial ML studies problems that are (1) less clearly defined, (2) harder to solve, and (3) even more challenging to evaluate. As a result, we caution that yet another decade of work on adversarial ML may fail to produce meaningful progress.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "113",
        "title": "Exact Sequence Classification with Hardmax Transformers",
        "author": [
            "Albert Alcalde",
            "Giovanni Fantuzzi",
            "Enrique Zuazua"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02270",
        "abstract": "We prove that hardmax attention transformers perfectly classify datasets of $N$ labeled sequences in $\\mathbb{R}^d$, $d\\geq 2$. Specifically, given $N$ sequences with an arbitrary but finite length in $\\mathbb{R}^d$, we construct a transformer with $\\mathcal{O}(N)$ blocks and $\\mathcal{O}(Nd)$ parameters perfectly classifying this dataset. Our construction achieves the best complexity estimate to date, independent of the length of the sequences, by innovatively alternating feed-forward and self-attention layers and by capitalizing on the clustering effect inherent to the latter. Our novel constructive method also uses low-rank parameter matrices within the attention mechanism, a common practice in real-life transformer implementations. Consequently, our analysis holds twofold significance: it substantially advances the mathematical theory of transformers and it rigorously justifies their exceptional real-world performance in sequence classification tasks.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "114",
        "title": "A Revisit of Total Correlation in Disentangled Variational Auto-Encoder with Partial Disentanglement",
        "author": [
            "Chengrui Li",
            "Yunmiao Wang",
            "Yule Wang",
            "Weihan Li",
            "Dieter Jaeger",
            "Anqi Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02279",
        "abstract": "A fully disentangled variational auto-encoder (VAE) aims to identify disentangled latent components from observations. However, enforcing full independence between all latent components may be too strict for certain datasets. In some cases, multiple factors may be entangled together in a non-separable manner, or a single independent semantic meaning could be represented by multiple latent components within a higher-dimensional manifold. To address such scenarios with greater flexibility, we develop the Partially Disentangled VAE (PDisVAE), which generalizes the total correlation (TC) term in fully disentangled VAEs to a partial correlation (PC) term. This framework can handle group-wise independence and can naturally reduce to either the standard VAE or the fully disentangled VAE. Validation through three synthetic experiments demonstrates the correctness and practicality of PDisVAE. When applied to real-world datasets, PDisVAE discovers valuable information that is difficult to find using fully disentangled VAEs, implying its versatility and effectiveness.",
        "tags": [
            "VAE"
        ]
    },
    {
        "id": "115",
        "title": "GP-GS: Gaussian Processes for Enhanced Gaussian Splatting",
        "author": [
            "Zhihao Guo",
            "Jingxuan Su",
            "Shenglin Wang",
            "Jinlong Fan",
            "Jing Zhang",
            "Liangxiu Han",
            "Peng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02283",
        "abstract": "3D Gaussian Splatting has emerged as an efficient photorealistic novel view synthesis method. However, its reliance on sparse Structure-from-Motion (SfM) point clouds consistently compromises the scene reconstruction quality. To address these limitations, this paper proposes a novel 3D reconstruction framework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output Gaussian Process model is developed to achieve adaptive and uncertainty-guided densification of sparse SfM point clouds. Specifically, we propose a dynamic sampling and filtering pipeline that adaptively expands the SfM point clouds by leveraging GP-based predictions to infer new candidate points from the input 2D pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the pruning of high-variance predictions, ensuring geometric consistency and enabling the generation of dense point clouds. The densified point clouds provide high-quality initial 3D Gaussians to enhance reconstruction performance. Extensive experiments conducted on synthetic and real-world datasets across various scales validate the effectiveness and practicality of the proposed framework.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "116",
        "title": "Adaptive Resource Allocation Optimization Using Large Language Models in Dynamic Wireless Environments",
        "author": [
            "Hyeonho Noh",
            "Byonghyo Shim",
            "Hyun Jong Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02287",
        "abstract": "Deep learning (DL) has made notable progress in addressing complex radio access network control challenges that conventional analytic methods have struggled to solve. However, DL has shown limitations in solving constrained NP-hard problems often encountered in network optimization, such as those involving quality of service (QoS) or discrete variables like user indices. Current solutions rely on domain-specific architectures or heuristic techniques, and a general DL approach for constrained optimization remains undeveloped. Moreover, even minor changes in communication objectives demand time-consuming retraining, limiting their adaptability to dynamic environments where task objectives, constraints, environmental factors, and communication scenarios frequently change. To address these challenges, we propose a large language model for resource allocation optimizer (LLM-RAO), a novel approach that harnesses the capabilities of LLMs to address the complex resource allocation problem while adhering to QoS constraints. By employing a prompt-based tuning strategy to flexibly convey ever-changing task descriptions and requirements to the LLM, LLM-RAO demonstrates robust performance and seamless adaptability in dynamic environments without requiring extensive retraining. Simulation results reveal that LLM-RAO achieves up to a 40% performance enhancement compared to conventional DL methods and up to an $80$\\% improvement over analytical approaches. Moreover, in scenarios with fluctuating communication objectives, LLM-RAO attains up to 2.9 times the performance of traditional DL-based networks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "117",
        "title": "Evalita-LLM: Benchmarking Large Language Models on Italian",
        "author": [
            "Bernardo Magnini",
            "Roberto Zanoli",
            "Michele Resta",
            "Martin Cimmino",
            "Paolo Albano",
            "Marco Madeddu",
            "Viviana Patti"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02289",
        "abstract": "We describe Evalita-LLM, a new benchmark designed to evaluate Large Language Models (LLMs) on Italian tasks. The distinguishing and innovative features of Evalita-LLM are the following: (i) all tasks are native Italian, avoiding issues of translating from Italian and potential cultural biases; (ii) in addition to well established multiple-choice tasks, the benchmark includes generative tasks, enabling more natural interaction with LLMs; (iii) all tasks are evaluated against multiple prompts, this way mitigating the model sensitivity to specific prompts and allowing a fairer and objective evaluation. We propose an iterative methodology, where candidate tasks and candidate prompts are validated against a set of LLMs used for development. We report experimental results from the benchmark's development phase, and provide performance statistics for several state-of-the-art LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "118",
        "title": "UniGaze: Towards Universal Gaze Estimation via Large-scale Pre-Training",
        "author": [
            "Jiawei Qin",
            "Xucong Zhang",
            "Yusuke Sugano"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02307",
        "abstract": "Despite decades of research on data collection and model architectures, current gaze estimation models face significant challenges in generalizing across diverse data domains. While recent advances in self-supervised pre-training have shown remarkable potential for improving model generalization in various vision tasks, their effectiveness in gaze estimation remains unexplored due to the geometric nature of the gaze regression task. We propose UniGaze, which leverages large-scale, in-the-wild facial datasets through self-supervised pre-training for gaze estimation. We carefully curate multiple facial datasets that capture diverse variations in identity, lighting, background, and head poses. By directly applying Masked Autoencoder (MAE) pre-training on normalized face images with a Vision Transformer (ViT) backbone, our UniGaze learns appropriate feature representations within the specific input space required by downstream gaze estimation models. Through comprehensive experiments using challenging cross-dataset evaluation and novel protocols, including leave-one-dataset-out and joint-dataset settings, we demonstrate that UniGaze significantly improves generalization across multiple data domains while minimizing reliance on costly labeled data. The source code and pre-trained models will be released upon acceptance.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "119",
        "title": "Real-Time Operator Takeover for Visuomotor Diffusion Policy Training",
        "author": [
            "Nils Ingelhag",
            "Jesper Munkeby",
            "Michael C. Welle",
            "Marco Moletta",
            "Danica Kragic"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02308",
        "abstract": "We present a Real-Time Operator Takeover (RTOT) paradigm enabling operators to seamlessly take control of a live visuomotor diffusion policy, guiding the system back into desirable states or reinforcing specific demonstrations. We presents new insights in using the Mahalonobis distance to automaicaly identify undesirable states. Once the operator has intervened and redirected the system, the control is seamlessly returned to the policy, which resumes generating actions until further intervention is required. We demonstrate that incorporating the targeted takeover demonstrations significantly improves policy performance compared to training solely with an equivalent number of, but longer, initial demonstrations. We provide an in-depth analysis of using the Mahalanobis distance to detect out-of-distribution states, illustrating its utility for identifying critical failure points during execution. Supporting materials, including videos of initial and takeover demonstrations and all rice-scooping experiments, are available on the project website: https://operator-takeover.github.io/",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "120",
        "title": "VaiBot: Shuttle Between the Instructions and Parameters",
        "author": [
            "Wangtao Sun",
            "Haotian Xu",
            "Huanxuan Liao",
            "Xuanqing Yu",
            "Zhongtao Jiang",
            "Shizhu He",
            "Jun Zhao",
            "Kang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02315",
        "abstract": "How to interact with LLMs through \\emph{instructions} has been widely studied by researchers. However, previous studies have treated the emergence of instructions and the training of LLMs on task data as separate processes, overlooking the inherent unity between the two. This paper proposes a neural network framework, VaiBot, that integrates VAE and VIB, designed to uniformly model, learn, and infer both deduction and induction tasks under LLMs. Through experiments, we demonstrate that VaiBot performs on par with existing baseline methods in terms of deductive capabilities while significantly surpassing them in inductive capabilities. We also find that VaiBot can scale up using general instruction-following data and exhibits excellent one-shot induction abilities. We finally synergistically integrate the deductive and inductive processes of VaiBot. Through T-SNE dimensionality reduction, we observe that its inductive-deductive process significantly improves the distribution of training parameters, enabling it to outperform baseline methods in inductive reasoning tasks. The code and data for this paper can be found at https://anonymous.4open.science/r/VaiBot-021F.",
        "tags": [
            "LLMs",
            "VAE"
        ]
    },
    {
        "id": "121",
        "title": "DIME:Diffusion-Based Maximum Entropy Reinforcement Learning",
        "author": [
            "Onur Celik",
            "Zechu Li",
            "Denis Blessing",
            "Ge Li",
            "Daniel Palanicek",
            "Jan Peters",
            "Georgia Chalvatzaki",
            "Gerhard Neumann"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02316",
        "abstract": "Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard approach to RL due to its beneficial exploration properties. Traditionally, policies are parameterized using Gaussian distributions, which significantly limits their representational capacity. Diffusion-based policies offer a more expressive alternative, yet integrating them into MaxEnt-RL poses challenges--primarily due to the intractability of computing their marginal entropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL (DIME). DIME leverages recent advances in approximate inference with diffusion models to derive a lower bound on the maximum entropy objective. Additionally, we propose a policy iteration scheme that provably converges to the optimal diffusion policy. Our method enables the use of expressive diffusion-based policies while retaining the principled exploration benefits of MaxEnt-RL, significantly outperforming other diffusion-based methods on challenging high-dimensional control benchmarks. It is also competitive with state-of-the-art non-diffusion based RL methods while requiring fewer algorithmic design choices and smaller update-to-data ratios, reducing computational complexity.",
        "tags": [
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "122",
        "title": "ReSpark: Leveraging Previous Data Reports as References to Generate New Reports with LLMs",
        "author": [
            "Yuan Tian",
            "Chuhan Zhang",
            "Xiaotong Wang",
            "Sitong Pan",
            "Weiwei Cui",
            "Haidong Zhang",
            "Dazhen Deng",
            "Yingcai Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02329",
        "abstract": "Creating data reports is time-consuming, as it requires iterative exploration and understanding of data, followed by summarizing the insights. While large language models (LLMs) are powerful tools for data processing and text generation, they often struggle to produce complete data reports that fully meet user expectations. One significant challenge is effectively communicating the entire analysis logic to LLMs. Moreover, determining a comprehensive analysis logic can be mentally taxing for users. To address these challenges, we propose ReSpark, an LLM-based method that leverages existing data reports as references for creating new ones. Given a data table, ReSpark searches for similar-topic reports, parses them into interdependent segments corresponding to analytical objectives, and executes them with new data. It identifies inconsistencies and customizes the objectives, data transformations, and textual descriptions. ReSpark allows users to review real-time outputs, insert new objectives, and modify report content. Its effectiveness was evaluated through comparative and user studies.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "123",
        "title": "Event-aided Semantic Scene Completion",
        "author": [
            "Shangwei Guo",
            "Hao Shi",
            "Song Wang",
            "Xiaoting Yin",
            "Kailun Yang",
            "Kaiwei Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02334",
        "abstract": "Autonomous driving systems rely on robust 3D scene understanding. Recent advances in Semantic Scene Completion (SSC) for autonomous driving underscore the limitations of RGB-based approaches, which struggle under motion blur, poor lighting, and adverse weather. Event cameras, offering high dynamic range and low latency, address these challenges by providing asynchronous data that complements RGB inputs. We present DSEC-SSC, the first real-world benchmark specifically designed for event-aided SSC, which includes a novel 4D labeling pipeline for generating dense, visibility-aware labels that adapt dynamically to object motion. Our proposed RGB-Event fusion framework, EvSSC, introduces an Event-aided Lifting Module (ELM) that effectively bridges 2D RGB-Event features to 3D space, enhancing view transformation and the robustness of 3D volume construction across SSC models. Extensive experiments on DSEC-SSC and simulated SemanticKITTI-E demonstrate that EvSSC is adaptable to both transformer-based and LSS-based SSC architectures. Notably, evaluations on SemanticKITTI-C demonstrate that EvSSC achieves consistently improved prediction accuracy across five degradation modes and both In-domain and Out-of-domain settings, achieving up to a 52.5% relative improvement in mIoU when the image sensor partially fails. Additionally, we quantitatively and qualitatively validate the superiority of EvSSC under motion blur and extreme weather conditions, where autonomous driving is challenged. The established datasets and our codebase will be made publicly at https://github.com/Pandapan01/EvSSC.",
        "tags": [
            "3D",
            "Transformer"
        ]
    },
    {
        "id": "124",
        "title": "Identifying Large-Scale Linear Parameter Varying Systems with Dynamic Mode Decomposition Methods",
        "author": [
            "Jean Panaioti Jordanou",
            "Eduardo Camponogara",
            "Eduardo Gildin"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02336",
        "abstract": "Linear Parameter Varying (LPV) Systems are a well-established class of nonlinear systems with a rich theory for stability analysis, control, and analytical response finding, among other aspects. Although there are works on data-driven identification of such systems, the literature is quite scarce in terms of works that tackle the identification of LPV models for large-scale systems. Since large-scale systems are ubiquitous in practice, this work develops a methodology for the local and global identification of large-scale LPV systems based on nonintrusive reduced-order modeling. The developed method is coined as DMD-LPV for being inspired in the Dynamic Mode Decomposition (DMD). To validate the proposed identification method, we identify a system described by a discretized linear diffusion equation, with the diffusion gain defined by a polynomial over a parameter. The experiments show that the proposed method can easily identify a reduced-order LPV model of a given large-scale system without the need to perform identification in the full-order dimension, and with almost no performance decay over performing a reduction, given that the model structure is well-established.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "125",
        "title": "Rule-ATT&CK Mapper (RAM): Mapping SIEM Rules to TTPs Using LLMs",
        "author": [
            "Prasanna N. Wudali",
            "Moshe Kravchik",
            "Ehud Malul",
            "Parth A. Gandhi",
            "Yuval Elovici",
            "Asaf Shabtai"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02337",
        "abstract": "The growing frequency of cyberattacks has heightened the demand for accurate and efficient threat detection systems. SIEM platforms are important for analyzing log data and detecting adversarial activities through rule-based queries, also known as SIEM rules. The efficiency of the threat analysis process relies heavily on mapping these SIEM rules to the relevant attack techniques in the MITRE ATT&CK framework. Inaccurate annotation of SIEM rules can result in the misinterpretation of attacks, increasing the likelihood that threats will be overlooked. Existing solutions for annotating SIEM rules with MITRE ATT&CK technique labels have notable limitations: manual annotation of SIEM rules is both time-consuming and prone to errors, and ML-based approaches mainly focus on annotating unstructured free text sources rather than structured data like SIEM rules. Structured data often contains limited information, further complicating the annotation process and making it a challenging task. To address these challenges, we propose Rule-ATT&CK Mapper (RAM), a novel framework that leverages LLMs to automate the mapping of structured SIEM rules to MITRE ATT&CK techniques. RAM's multi-stage pipeline, which was inspired by the prompt chaining technique, enhances mapping accuracy without requiring LLM pre-training or fine-tuning. Using the Splunk Security Content dataset, we evaluate RAM's performance using several LLMs, including GPT-4-Turbo, Qwen, IBM Granite, and Mistral. Our evaluation highlights GPT-4-Turbo's superior performance, which derives from its enriched knowledge base, and an ablation study emphasizes the importance of external contextual knowledge in overcoming the limitations of LLMs' implicit knowledge for domain-specific tasks. These findings demonstrate RAM's potential in automating cybersecurity workflows and provide valuable insights for future advancements in this field.",
        "tags": [
            "Detection",
            "GPT",
            "LLMs",
            "Qwen"
        ]
    },
    {
        "id": "126",
        "title": "Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking",
        "author": [
            "Jinyang Wu",
            "Mingkuan Feng",
            "Shuai Zhang",
            "Ruihan Jin",
            "Feihu Che",
            "Zengqi Wen",
            "Jianhua Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02339",
        "abstract": "Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0$\\%$) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2$\\%$) while maintaining substantial data and computational efficiency.",
        "tags": [
            "GPT",
            "Large Language Models"
        ]
    },
    {
        "id": "127",
        "title": "MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm",
        "author": [
            "Ziyan Guo",
            "Zeyu Hu",
            "Na Zhao",
            "De Wen Soh"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02358",
        "abstract": "Human motion generation and editing are key components of computer graphics and vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. To address these limitations and provide a versatile, unified framework capable of handling both human motion generation and editing, we introduce a novel paradigm: Motion-Condition-Motion, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target http://motion.Based on this paradigm, we propose a unified framework, MotionLab, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified http://conditions.In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding} to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks. Notably, our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion. Our code and additional video results are available at: https://diouo.github.io/motionlab.github.io/.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "128",
        "title": "Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs",
        "author": [
            "Sagnik Mukherjee",
            "Abhinav Chinta",
            "Takyoung Kim",
            "Tarun Anoop Sharma",
            "Dilek Hakkani Tur"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02362",
        "abstract": "Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large language models (LLMs) by enabling detailed step-by-step solutions. However, due to the verbosity of LLMs, the resulting reasoning chains can be long, making it harder to verify the reasoning steps and trace issues resulting from dependencies between the steps that may be farther away in the sequence of steps. Importantly, mathematical reasoning allows each step to be derived from a small set of premises, which are a subset of the preceding steps in the reasoning chain. In this paper, we present a framework that identifies the premises for each step, to improve the evaluation of reasoning. We restructure conventional linear reasoning chains into Premise Augmented Reasoning Chains (PARC) by introducing premise links, resulting in a directed acyclic graph where the nodes are the steps and the edges are the premise links. Through experiments with a PARC-based dataset that we built, namely PERL (Premises and ERrors identification in LLMs), we demonstrate that LLMs can reliably identify premises within complex reasoning chains. In particular, even open-source LLMs achieve 90% recall in premise identification. We also show that PARC helps to identify errors in reasoning chains more reliably. The accuracy of error identification improves by 6% to 16% absolute when step-by-step verification is carried out in PARC under the premises. Our findings highlight the utility of premise-centric representations in addressing complex problem-solving tasks and open new avenues for improving the reliability of LLM-based reasoning evaluations.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "129",
        "title": "Evaluating the Effectiveness of LLMs in Fixing Maintainability Issues in Real-World Projects",
        "author": [
            "Henrique Nunes",
            "Eduardo Figueiredo",
            "Larissa Rocha",
            "Sarah Nadi",
            "Fischer Ferreira",
            "Geanderson Esteves"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02368",
        "abstract": "Large Language Models (LLMs) have gained attention for addressing coding problems, but their effectiveness in fixing code maintainability remains unclear. This study evaluates LLMs capability to resolve 127 maintainability issues from 10 GitHub repositories. We use zero-shot prompting for Copilot Chat and Llama 3.1, and few-shot prompting with Llama only. The LLM-generated solutions are assessed for compilation errors, test failures, and new maintainability problems. Llama with few-shot prompting successfully fixed 44.9% of the methods, while Copilot Chat and Llama zero-shot fixed 32.29% and 30%, respectively. However, most solutions introduced errors or new maintainability issues. We also conducted a human study with 45 participants to evaluate the readability of 51 LLM-generated solutions. The human study showed that 68.63% of participants observed improved readability. Overall, while LLMs show potential for fixing maintainability issues, their introduction of errors highlights their current limitations.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "130",
        "title": "MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by Continual Learning",
        "author": [
            "Shengbo Gu",
            "Yu-Kun Qiu",
            "Yu-Ming Tang",
            "Ancong Wu",
            "Wei-Shi Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02372",
        "abstract": "The generation of a virtual digital avatar is a crucial research topic in the field of computer vision. Many existing works utilize Neural Radiance Fields (NeRF) to address this issue and have achieved impressive results. However, previous works assume the images of the training person are available and fixed while the appearances and poses of a subject could constantly change and increase in real-world scenarios. How to update the human avatar but also maintain the ability to render the old appearance of the person is a practical challenge. One trivial solution is to combine the existing virtual avatar models based on NeRF with continual learning methods. However, there are some critical issues in this approach: learning new appearances and poses can cause the model to forget past information, which in turn leads to a degradation in the rendering quality of past appearances, especially color bleeding issues, and incorrect human body poses. In this work, we propose a maintainable avatar (MaintaAvatar) based on neural radiance fields by continual learning, which resolves the issues by utilizing a Global-Local Joint Storage Module and a Pose Distillation Module. Overall, our model requires only limited data collection to quickly fine-tune the model while avoiding catastrophic forgetting, thus achieving a maintainable virtual avatar. The experimental results validate the effectiveness of our MaintaAvatar model.",
        "tags": [
            "NeRF"
        ]
    },
    {
        "id": "131",
        "title": "STAIR: Improving Safety Alignment with Introspective Reasoning",
        "author": [
            "Yichi Zhang",
            "Siyuan Zhang",
            "Yao Huang",
            "Zeyu Xia",
            "Zhengwei Fang",
            "Xiao Yang",
            "Ranjie Duan",
            "Dong Yan",
            "Yinpeng Dong",
            "Jun Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02384",
        "abstract": "Ensuring the safety and harmlessness of Large Language Models (LLMs) has become equally critical as their performance in applications. However, existing safety alignment methods typically suffer from safety-performance trade-offs and the susceptibility to jailbreak attacks, primarily due to their reliance on direct refusals for malicious queries. In this paper, we propose STAIR, a novel framework that integrates SafeTy Alignment with Itrospective Reasoning. We enable LLMs to identify safety risks through step-by-step analysis by self-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR first equips the model with a structured reasoning capability and then advances safety alignment via iterative preference optimization on step-level reasoning data generated using our newly proposed Safety-Informed Monte Carlo Tree Search (SI-MCTS). We further train a process reward model on this data to guide test-time searches for improved responses. Extensive experiments show that STAIR effectively mitigates harmful outputs while better preserving helpfulness, compared to instinctive alignment strategies. With test-time scaling, STAIR achieves a safety performance comparable to Claude-3.5 against popular jailbreak attacks. Relevant resources in this work are available at https://github.com/thu-ml/STAIR.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "132",
        "title": "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning",
        "author": [
            "Jianfeng Pan",
            "Senyou Deng",
            "Shaomang Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02390",
        "abstract": "Research on LLM technologies is rapidly emerging, with most of them employing a 'fast thinking' approach to inference. Most LLMs generate the final result based solely on a single query and LLM's reasoning capabilities. However, with the advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing attention because its process is closer to the human thought process. Inspired by the human ability to constantly associate and replenish knowledge during thinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework, which introduces an innovative synergy between the Monte Carlo Tree Search (MCTS) algorithm and a dynamic mechanism for integrating new key information, termed 'associative memory'. By combining the structured exploration capabilities of MCTS with the adaptive learning capacity of associative memory, CoAT significantly expands the LLM search space, enabling our framework to explore diverse reasoning pathways and dynamically update its knowledge base in real-time. This allows the framework to not only revisit and refine earlier inferences but also adaptively incorporate evolving information, ensuring that the final output is both accurate and comprehensive. To validate the effectiveness of our framework, we conducted extensive experiments across a range of generative and reasoning tasks. These experiments demonstrated that our framework outperforms conventional inference processes on accuracy, coherence, and diversity. The framework's ability to iteratively expand its search space while retaining contextually relevant information results.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "133",
        "title": "LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models",
        "author": [
            "Tzu-Tao Chang",
            "Shivaram Venkataraman"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02406",
        "abstract": "Cross-attention is commonly adopted in multimodal large language models (MLLMs) for integrating visual information into the language backbone. However, in applications with large visual inputs, such as video understanding, processing a large number of visual tokens in cross-attention layers leads to high memory demands and often necessitates distributed computation across multiple GPUs. Existing distributed attention mechanisms face significant communication overheads, making cross-attention layers a critical bottleneck for efficient training and inference of MLLMs. To address this, we propose LV-XAttn, a distributed, exact cross-attention mechanism with minimal communication overhead. We observe that in applications involving large visual inputs the size of the query block is typically much smaller than that of the key-value blocks. Thus, in LV-XAttn we keep the large key-value blocks locally on each GPU and exchange smaller query blocks across GPUs. We also introduce an efficient activation recomputation technique enabling support for longer visual context. We theoretically analyze the communication benefits of LV-XAttn and show that it can achieve speedups for a wide range of models. Our evaluations with mPLUG-Owl3 and OpenFlamingo models find that LV-XAttn achieves up to 5.58$\\times$ end-to-end speedup compared to existing approaches.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "134",
        "title": "Avoiding spurious sharpness minimization broadens applicability of SAM",
        "author": [
            "Sidak Pal Singh",
            "Hossein Mobahi",
            "Atish Agarwala",
            "Yann Dauphin"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02407",
        "abstract": "Curvature regularization techniques like Sharpness Aware Minimization (SAM) have shown great promise in improving generalization on vision tasks. However, we find that SAM performs poorly in domains like natural language processing (NLP), often degrading performance -- even with twice the compute budget. We investigate the discrepancy across domains and find that in the NLP setting, SAM is dominated by regularization of the logit statistics -- instead of improving the geometry of the function itself. We use this observation to develop an alternative algorithm we call Functional-SAM, which regularizes curvature only through modification of the statistics of the overall function implemented by the neural network, and avoids spurious minimization through logit manipulation. Furthermore, we argue that preconditioning the SAM perturbation also prevents spurious minimization, and when combined with Functional-SAM, it gives further improvements. Our proposed algorithms show improved performance over AdamW and SAM baselines when trained for an equal number of steps, in both fixed-length and Chinchilla-style training settings, at various model scales (including billion-parameter scale). On the whole, our work highlights the importance of more precise characterizations of sharpness in broadening the applicability of curvature regularization to large language models (LLMs).",
        "tags": [
            "LLMs",
            "Large Language Models",
            "SAM"
        ]
    },
    {
        "id": "135",
        "title": "AI-Powered, But Power-Hungry? Energy Efficiency of LLM-Generated Code",
        "author": [
            "Lola Solovyeva",
            "Sophie Weidmann",
            "Fernando Castor"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02412",
        "abstract": "Large language models (LLMs) are used in software development to assist in various tasks, e.g., code generation and code completion, but empirical evaluations of the quality of the results produced by these models focus on correctness and ignore other relevant aspects, such as their performance and energy efficiency. Studying the performance of LLM-produced programs is essential to understand how well LLMs can support the construction of performance- and energy-critical software, such as operating systems, servers, and mobile applications. This paper presents the first study analyzing the energy efficiency and performance of LLM-generated code for three programming languages Python, Java, and C++, on two platforms, a Mac and a PC, leveraging three frontier LLMs, Github Copilot, GPT-4o, and the recently-released OpenAI o1-mini, and targeting ``hard'' programming problems from LeetCode. Our results show that the models are much more successful in generating Python and Java than C++ code.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "136",
        "title": "Towards Fast Graph Generation via Autoregressive Noisy Filtration Modeling",
        "author": [
            "Markus Krimmel",
            "Jenna Wiens",
            "Karsten Borgwardt",
            "Dexiong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02415",
        "abstract": "Graph generative models often face a critical trade-off between learning complex distributions and achieving fast generation speed. We introduce Autoregressive Noisy Filtration Modeling (ANFM), a novel approach that addresses both challenges. ANFM leverages filtration, a concept from topological data analysis, to transform graphs into short sequences of monotonically increasing subgraphs. This formulation extends the sequence families used in previous autoregressive models. To learn from these sequences, we propose a novel autoregressive graph mixer model. Our experiments suggest that exposure bias might represent a substantial hurdle in autoregressive graph generation and we introduce two mitigation strategies to address it: noise augmentation and a reinforcement learning approach. Incorporating these techniques leads to substantial performance gains, making ANFM competitive with state-of-the-art diffusion models across diverse synthetic and real-world datasets. Notably, ANFM produces remarkably short sequences, achieving a 100-fold speedup in generation time compared to diffusion models. This work marks a significant step toward high-throughput graph generation.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "137",
        "title": "CVKAN: Complex-Valued Kolmogorov-Arnold Networks",
        "author": [
            "Matthias Wolff",
            "Florian Eilers",
            "Xiaoyi Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02417",
        "abstract": "In this work we propose CKAN, a complex-valued KAN, to join the intrinsic interpretability of KANs and the advantages of Complex-Valued Neural Networks (CVNNs). We show how to transfer a KAN and the necessary associated mechanisms into the complex domain. To confirm that CKAN meets expectations we conduct experiments on symbolic complex-valued function fitting and physically meaningful formulae as well as on a more realistic dataset from knot theory. Our proposed CKAN is more stable and performs on par or better than real-valued KANs while requiring less parameters and a shallower network architecture, making it more explainable.",
        "tags": [
            "KAN",
            "Kolmogorov-Arnold Networks"
        ]
    },
    {
        "id": "138",
        "title": "Activation-Informed Merging of Large Language Models",
        "author": [
            "Amin Heyrani Nobari",
            "Kaveh Alimohammadi",
            "Ali ArjomandBigdeli",
            "Akash Srivastava",
            "Faez Ahmed",
            "Navid Azizan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02421",
        "abstract": "Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning~(CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40\\% increase in benchmark performance.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "139",
        "title": "TransformDAS: Mapping {\\Phi}-OTDR Signals to Riemannian Manifold for Robust Classification",
        "author": [
            "Jiaju Kang",
            "Puyu Han",
            "Yang Chun",
            "Xu Wang",
            "Luqi Gong"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02428",
        "abstract": "Phase-sensitive optical time-domain reflectometry ({\\Phi}-OTDR) is a widely used distributed fiber optic sensing system in engineering. Machine learning algorithms for {\\Phi}-OTDR event classification require high volumes and quality of datasets; however, high-quality datasets are currently extremely scarce in the field, leading to a lack of robustness in models, which is manifested by higher false alarm rates in real-world scenarios. One promising approach to address this issue is to augment existing data using generative models combined with a small amount of real-world data. We explored mapping both {\\Phi}-OTDR features in a GAN-based generative pipeline and signal features in a Transformer classifier to hyperbolic space to seek more effective model generalization. The results indicate that state-of-the-art models exhibit stronger generalization performance and lower false alarm rates in real-world scenarios when trained on augmented datasets. TransformDAS, in particular, demonstrates the best classification performance, highlighting the benefits of Riemannian manifold mapping in {\\Phi}-OTDR data generation and model classification.",
        "tags": [
            "GAN",
            "Transformer"
        ]
    },
    {
        "id": "140",
        "title": "LLMER: Crafting Interactive Extended Reality Worlds with JSON Data Generated by Large Language Models",
        "author": [
            "Jiangong Chen",
            "Xiaoyi Wu",
            "Tian Lan",
            "Bin Li"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02441",
        "abstract": "The integration of Large Language Models (LLMs) like GPT-4 with Extended Reality (XR) technologies offers the potential to build truly immersive XR environments that interact with human users through natural language, e.g., generating and animating 3D scenes from audio inputs. However, the complexity of XR environments makes it difficult to accurately extract relevant contextual data and scene/object parameters from an overwhelming volume of XR artifacts. It leads to not only increased costs with pay-per-use models, but also elevated levels of generation errors. Moreover, existing approaches focusing on coding script generation are often prone to generation errors, resulting in flawed or invalid scripts, application crashes, and ultimately a degraded user experience. To overcome these challenges, we introduce LLMER, a novel framework that creates interactive XR worlds using JSON data generated by LLMs. Unlike prior approaches focusing on coding script generation, LLMER translates natural language inputs into JSON data, significantly reducing the likelihood of application crashes and processing latency. It employs a multi-stage strategy to supply only the essential contextual information adapted to the user's request and features multiple modules designed for various XR tasks. Our preliminary user study reveals the effectiveness of the proposed system, with over 80% reduction in consumed tokens and around 60% reduction in task completion time compared to state-of-the-art approaches. The analysis of users' feedback also illuminates a series of directions for further optimization.",
        "tags": [
            "3D",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "141",
        "title": "Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models",
        "author": [
            "Haoran Ye",
            "Tianze Zhang",
            "Yuhang Xie",
            "Liyuan Zhang",
            "Yuanyi Ren",
            "Xin Zhang",
            "Guojie Song"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02444",
        "abstract": "Values are core drivers of individual and collective perception, cognition, and behavior. Value systems, such as Schwartz's Theory of Basic Human Values, delineate the hierarchy and interplay among these values, enabling cross-disciplinary investigations into decision-making and societal dynamics. Recently, the rise of Large Language Models (LLMs) has raised concerns regarding their elusive intrinsic values. Despite growing efforts in evaluating, understanding, and aligning LLM values, a psychologically grounded LLM value system remains underexplored. This study addresses the gap by introducing the Generative Psycho-Lexical Approach (GPLA), a scalable, adaptable, and theoretically informed method for constructing value systems. Leveraging GPLA, we propose a psychologically grounded five-factor value system tailored for LLMs. For systematic validation, we present three benchmarking tasks that integrate psychological principles with cutting-edge AI priorities. Our results reveal that the proposed value system meets standard psychological criteria, better captures LLM values, improves LLM safety prediction, and enhances LLM alignment, when compared to the canonical Schwartz's values.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "142",
        "title": "Sparse Data Generation Using Diffusion Models",
        "author": [
            "Phil Ostheimer",
            "Mayank Nagda",
            "Marius Kloft",
            "Sophie Fellenz"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02448",
        "abstract": "Sparse data is ubiquitous, appearing in numerous domains, from economics and recommender systems to astronomy and biomedical sciences. However, efficiently and realistically generating sparse data remains a significant challenge. We introduce Sparse Data Diffusion (SDD), a novel method for generating sparse data. SDD extends continuous state-space diffusion models by explicitly modeling sparsity through the introduction of Sparsity Bits. Empirical validation on image data from various domains-including two scientific applications, physics and biology-demonstrates that SDD achieves high fidelity in representing data sparsity while preserving the quality of the generated data.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "143",
        "title": "TUMTraffic-VideoQA: A Benchmark for Unified Spatio-Temporal Video Understanding in Traffic Scenes",
        "author": [
            "Xingcheng Zhou",
            "Konstantinos Larintzakis",
            "Hao Guo",
            "Walter Zimmer",
            "Mingyu Liu",
            "Hu Cao",
            "Jiajie Zhang",
            "Venkatnarayanan Lakshminarasimhan",
            "Leah Strand",
            "Alois C. Knoll"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02449",
        "abstract": "We present TUMTraffic-VideoQA, a novel dataset and benchmark designed for spatio-temporal video understanding in complex roadside traffic scenarios. The dataset comprises 1,000 videos, featuring 85,000 multiple-choice QA pairs, 2,300 object captioning, and 5,700 object grounding annotations, encompassing diverse real-world conditions such as adverse weather and traffic anomalies. By incorporating tuple-based spatio-temporal object expressions, TUMTraffic-VideoQA unifies three essential tasks-multiple-choice video question answering, referred object captioning, and spatio-temporal object grounding-within a cohesive evaluation framework. We further introduce the TUMTraffic-Qwen baseline model, enhanced with visual token sampling strategies, providing valuable insights into the challenges of fine-grained spatio-temporal reasoning. Extensive experiments demonstrate the dataset's complexity, highlight the limitations of existing models, and position TUMTraffic-VideoQA as a robust foundation for advancing research in intelligent transportation systems. The dataset and benchmark are publicly available to facilitate further exploration.",
        "tags": [
            "Qwen"
        ]
    },
    {
        "id": "144",
        "title": "Beyond English: Evaluating Automated Measurement of Moral Foundations in Non-English Discourse with a Chinese Case Study",
        "author": [
            "Calvin Yixiang Cheng",
            "Scott A Hale"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02451",
        "abstract": "This study explores computational approaches for measuring moral foundations (MFs) in non-English corpora. Since most resources are developed primarily for English, cross-linguistic applications of moral foundation theory remain limited. Using Chinese as a case study, this paper evaluates the effectiveness of applying English resources to machine translated text, local language lexicons, multilingual language models, and large language models (LLMs) in measuring MFs in non-English texts. The results indicate that machine translation and local lexicon approaches are insufficient for complex moral assessments, frequently resulting in a substantial loss of cultural information. In contrast, multilingual models and LLMs demonstrate reliable cross-language performance with transfer learning, with LLMs excelling in terms of data efficiency. Importantly, this study also underscores the need for human-in-the-loop validation of automated MF assessment, as the most advanced models may overlook cultural nuances in cross-language measurements. The findings highlight the potential of LLMs for cross-language MF measurements and other complex multilingual deductive coding tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "145",
        "title": "Personalization Toolkit: Training Free Personalization of Large Vision Language Models",
        "author": [
            "Soroush Seifi",
            "Vaggelis Dorovatas",
            "Daniel Olmeda Reino",
            "Rahaf Aljundi"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02452",
        "abstract": "Large Vision Language Models (LVLMs) have significant potential to deliver personalized assistance by adapting to individual users' unique needs and preferences. Personalization of LVLMs is an emerging area that involves customizing models to recognize specific object instances and provide tailored responses. However, existing approaches rely on time-consuming test-time training for each user and object, rendering them impractical. This paper proposes a novel, training-free approach to LVLM personalization by leveraging pre-trained vision foundation models to extract distinct features, retrieval-augmented generation (RAG) techniques to recognize instances in the visual input, and visual prompting methods. Our model-agnostic vision toolkit enables flexible and efficient personalization without extensive retraining. We demonstrate state-of-the-art results, outperforming conventional training-based approaches and establish a new standard for LVLM personalization.",
        "tags": [
            "RAG",
            "Test-Time Training"
        ]
    },
    {
        "id": "146",
        "title": "SAISA: Towards Multimodal Large Language Models with Both Training and Inference Efficiency",
        "author": [
            "Qianhao Yuan",
            "Yanjiang Liu",
            "Yaojie Lu",
            "Hongyu Lin",
            "Ben He",
            "Xianpei Han",
            "Le Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02458",
        "abstract": "Multimodal Large Language Models (MLLMs) mainly fall into two architectures, each involving a trade-off between training and inference efficiency: embedding space alignment (e.g., LLaVA-1.5) is inefficient during inference, while cross-attention space alignment (e.g., Flamingo) is inefficient in training. In this paper, we compare these two architectures and identify the key factors for building efficient MLLMs. A primary difference between them lies in how attention is applied to visual tokens, particularly in their interactions with each other. To investigate whether attention among visual tokens is necessary, we propose a new self-attention mechanism, NAAViT (\\textbf{N}o \\textbf{A}ttention \\textbf{A}mong \\textbf{Vi}sual \\textbf{T}okens), which eliminates this type of attention. Our pilot experiment on LLaVA-1.5 shows that attention among visual tokens is highly redundant. Based on these insights, we introduce SAISA (\\textbf{S}elf-\\textbf{A}ttention \\textbf{I}nput \\textbf{S}pace \\textbf{A}lignment), a novel architecture that enhance both training and inference efficiency. SAISA directly aligns visual features with the input spaces of NAAViT self-attention blocks, reducing computational overhead in both self-attention blocks and feed-forward networks (FFNs). Using the same configuration as LLaVA-1.5, SAISA reduces inference FLOPs by 66\\% and training budget by 26\\%, while achieving superior performance in terms of accuracy. Comprehensive ablation studies further validate the effectiveness of SAISA across various LLMs and visual encoders. The code and model will be publicly available at https://github.com/icip-cas/SAISA.",
        "tags": [
            "LLMs",
            "LLaVA",
            "Large Language Models"
        ]
    },
    {
        "id": "147",
        "title": "Towards Consistent and Controllable Image Synthesis for Face Editing",
        "author": [
            "Mengting Wei",
            "Tuomas Varanka",
            "Yante Li",
            "Xingxun Jiang",
            "Huai-Qian Khor",
            "Guoying Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02465",
        "abstract": "Current face editing methods mainly rely on GAN-based techniques, but recent focus has shifted to diffusion-based models due to their success in image reconstruction. However, diffusion models still face challenges in manipulating fine-grained attributes and preserving consistency of attributes that should remain unchanged. To address these issues and facilitate more convenient editing of face images, we propose a novel approach that leverages the power of Stable-Diffusion models and crude 3D face models to control the lighting, facial expression and head pose of a portrait photo. We observe that this task essentially involve combinations of target background, identity and different face attributes. We aim to sufficiently disentangle the control of these factors to enable high-quality of face editing. Specifically, our method, coined as RigFace, contains: 1) A Spatial Arrtibute Encoder that provides presise and decoupled conditions of background, pose, expression and lighting; 2) An Identity Encoder that transfers identity features to the denoising UNet of a pre-trained Stable-Diffusion model; 3) An Attribute Rigger that injects those conditions into the denoising UNet. Our model achieves comparable or even superior performance in both identity preservation and photorealism compared to existing face editing models.",
        "tags": [
            "3D",
            "Diffusion",
            "GAN"
        ]
    },
    {
        "id": "148",
        "title": "Mind the Gap: Evaluating Patch Embeddings from General-Purpose and Histopathology Foundation Models for Cell Segmentation and Classification",
        "author": [
            "Valentina Vadori",
            "Antonella Peruffo",
            "Jean-Marie GraÃ¯c",
            "Livio Finos",
            "Enrico Grisan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02471",
        "abstract": "Recent advancements in foundation models have transformed computer vision, driving significant performance improvements across diverse domains, including digital histopathology. However, the advantages of domain-specific histopathology foundation models over general-purpose models for specialized tasks such as cell analysis remain underexplored. This study investigates the representation learning gap between these two categories by analyzing multi-level patch embeddings applied to cell instance segmentation and classification. We implement an encoder-decoder architecture with a consistent decoder and various encoders. These include convolutional, vision transformer (ViT), and hybrid encoders pre-trained on ImageNet-22K or LVD-142M, representing general-purpose foundation models. These are compared against ViT encoders from the recently released UNI, Virchow2, and Prov-GigaPath foundation models, trained on patches extracted from hundreds of thousands of histopathology whole-slide images. The decoder integrates patch embeddings from different encoder depths via skip connections to generate semantic and distance maps. These maps are then post-processed to create instance segmentation masks where each label corresponds to an individual cell and to perform cell-type classification. All encoders remain frozen during training to assess their pre-trained feature extraction capabilities. Using the PanNuke and CoNIC histopathology datasets, and the newly introduced Nissl-stained CytoDArk0 dataset for brain cytoarchitecture studies, we evaluate instance-level detection, segmentation accuracy, and cell-type classification. This study provides insights into the comparative strengths and limitations of general-purpose vs. histopathology foundation models, offering guidance for model selection in cell-focused histopathology and brain cytoarchitecture analysis workflows.",
        "tags": [
            "Detection",
            "Segmentation",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "149",
        "title": "Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study",
        "author": [
            "Menglong Cui",
            "Pengzhi Gao",
            "Wei Liu",
            "Jian Luan",
            "BinWang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02481",
        "abstract": "Large language models (LLMs) have shown continuously improving multilingual capabilities, and even small-scale open-source models have demonstrated rapid performance enhancement. In this paper, we systematically explore the abilities of open LLMs with less than ten billion parameters to handle multilingual machine translation (MT) tasks. We conduct comprehensive evaluations on six popular LLMs and find that models like Gemma2-9B exhibit impressive multilingual translation capabilities. We then introduce the Parallel-First Monolingual-Second (PFMS) data mixing strategy in the continual pretraining stage to further enhance the MT performance and present GemmaX2-28, a 9B model achieving top-tier multilingual translation performance across 28 languages. Specifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA) models such as TowerInstruct and XALMA and achieves competitive performance with Google Translate and GPT-4-turbo.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "150",
        "title": "Distributional Diffusion Models with Scoring Rules",
        "author": [
            "Valentin De Bortoli",
            "Alexandre Galashov",
            "J. Swaroop Guntupalli",
            "Guangyao Zhou",
            "Kevin Murphy",
            "Arthur Gretton",
            "Arnaud Doucet"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02483",
        "abstract": "Diffusion models generate high-quality synthetic data. They operate by defining a continuous-time forward process which gradually adds Gaussian noise to data until fully corrupted. The corresponding reverse process progressively \"denoises\" a Gaussian sample into a sample from the data distribution. However, generating high-quality outputs requires many discretization steps to obtain a faithful approximation of the reverse process. This is expensive and has motivated the development of many acceleration methods. We propose to accomplish sample generation by learning the posterior {\\em distribution} of clean data samples given their noisy versions, instead of only the mean of this distribution. This allows us to sample from the probability transitions of the reverse process on a coarse time scale, significantly accelerating inference with minimal degradation of the quality of the output. This is accomplished by replacing the standard regression loss used to estimate conditional means with a scoring rule. We validate our method on image and robot trajectory generation, where we consistently outperform standard diffusion models at few discretization steps.",
        "tags": [
            "Diffusion",
            "Robot"
        ]
    },
    {
        "id": "151",
        "title": "Hier-EgoPack: Hierarchical Egocentric Video Understanding with Diverse Task Perspectives",
        "author": [
            "Simone Alberto Peirone",
            "Francesca Pistilli",
            "Antonio Alliegro",
            "Tatiana Tommasi",
            "Giuseppe Averta"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02487",
        "abstract": "Our comprehension of video streams depicting human activities is naturally multifaceted: in just a few moments, we can grasp what is happening, identify the relevance and interactions of objects in the scene, and forecast what will happen soon, everything all at once. To endow autonomous systems with such a holistic perception, learning how to correlate concepts, abstract knowledge across diverse tasks, and leverage tasks synergies when learning novel skills is essential. A significant step in this direction is EgoPack, a unified framework for understanding human activities across diverse tasks with minimal overhead. EgoPack promotes information sharing and collaboration among downstream tasks, essential for efficiently learning new skills. In this paper, we introduce Hier-EgoPack, which advances EgoPack by enabling reasoning also across diverse temporal granularities, which expands its applicability to a broader range of downstream tasks. To achieve this, we propose a novel hierarchical architecture for temporal reasoning equipped with a GNN layer specifically designed to tackle the challenges of multi-granularity reasoning effectively. We evaluate our approach on multiple Ego4d benchmarks involving both clip-level and frame-level reasoning, demonstrating how our hierarchical unified architecture effectively solves these diverse tasks simultaneously.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "152",
        "title": "Do Graph Diffusion Models Accurately Capture and Generate Substructure Distributions?",
        "author": [
            "Xiyuan Wang",
            "Yewei Liu",
            "Lexi Pang",
            "Siwei Chen",
            "Muhan Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02488",
        "abstract": "Diffusion models have gained popularity in graph generation tasks; however, the extent of their expressivity concerning the graph distributions they can learn is not fully understood. Unlike models in other domains, popular backbones for graph diffusion models, such as Graph Transformers, do not possess universal expressivity to accurately model the distribution scores of complex graph data. Our work addresses this limitation by focusing on the frequency of specific substructures as a key characteristic of target graph distributions. When evaluating existing models using this metric, we find that they fail to maintain the distribution of substructure counts observed in the training set when generating new graphs. To address this issue, we establish a theoretical connection between the expressivity of Graph Neural Networks (GNNs) and the overall performance of graph diffusion models, demonstrating that more expressive GNN backbones can better capture complex distribution patterns. By integrating advanced GNNs into the backbone architecture, we achieve significant improvements in substructure generation.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "153",
        "title": "VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models",
        "author": [
            "Hila Chefer",
            "Uriel Singer",
            "Amit Zohar",
            "Yuval Kirstain",
            "Adam Polyak",
            "Yaniv Taigman",
            "Lior Wolf",
            "Shelly Sheynin"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02492",
        "abstract": "Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence. To address this, we introduce VideoJAM, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation. VideoJAM is composed of two complementary units. During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation. During inference, we introduce Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model's own evolving motion prediction as a dynamic guidance signal. Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model. VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations. These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation. Project website: https://hila-chefer.github.io/videojam-paper.github.io/",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "154",
        "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization",
        "author": [
            "Yize Wu",
            "Ke Gao",
            "Yanjun Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02493",
        "abstract": "Speculative decoding is an effective and lossless method for Large Language Model (LLM) inference acceleration. It employs a smaller model to generate a draft token sequence, which is then verified by the original base model. In multi-GPU systems, inference latency can be further reduced through tensor parallelism (TP), while the optimal TP size of the draft model is typically smaller than that of the base model, leading to GPU idling during the drafting stage. To solve this problem, we propose EasySpec, a layer-parallel speculation strategy that optimizes the efficiency of multi-GPU http://utilization.EasySpec breaks the sequential execution order of layers in the drafting model, enabling multi-layer parallelization across devices, albeit with some induced approximation errors. After each drafting-and-verification iteration, the draft model's key-value (KV) cache is calibrated in a single forward pass, preventing long-term error accumulation at minimal additional latency. We evaluated EasySpec on several mainstream open-source LLMs, using smaller versions of models from the same series as drafters. The results demonstrate that EasySpec can achieve a peak speedup of 4.17x compared to vanilla decoding, while preserving the original distribution of the base LLMs. Specifically, the drafting stage can be accelerated by up to 1.62x with a maximum accuracy drop of only 7%, requiring no training or fine-tuning on the draft models.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "155",
        "title": "Unified Spatial-Temporal Edge-Enhanced Graph Networks for Pedestrian Trajectory Prediction",
        "author": [
            "Ruochen Li",
            "Tanqiu Qiao",
            "Stamos Katsigiannis",
            "Zhanxing Zhu",
            "Hubert P. H. Shum"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02504",
        "abstract": "Pedestrian trajectory prediction aims to forecast future movements based on historical paths. Spatial-temporal (ST) methods often separately model spatial interactions among pedestrians and temporal dependencies of individuals. They overlook the direct impacts of interactions among different pedestrians across various time steps (i.e., high-order cross-time interactions). This limits their ability to capture ST inter-dependencies and hinders prediction performance. To address these limitations, we propose UniEdge with three major designs. Firstly, we introduce a unified ST graph data structure that simplifies high-order cross-time interactions into first-order relationships, enabling the learning of ST inter-dependencies in a single step. This avoids the information loss caused by multi-step aggregation. Secondly, traditional GNNs focus on aggregating pedestrian node features, neglecting the propagation of implicit interaction patterns encoded in edge features. We propose the Edge-to-Edge-Node-to-Node Graph Convolution (E2E-N2N-GCN), a novel dual-graph network that jointly models explicit N2N social interactions among pedestrians and implicit E2E influence propagation across these interaction patterns. Finally, to overcome the limited receptive fields and challenges in capturing long-range dependencies of auto-regressive architectures, we introduce a transformer encoder-based predictor that enables global modeling of temporal correlation. UniEdge outperforms state-of-the-arts on multiple datasets, including ETH, UCY, and SDD.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "156",
        "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search",
        "author": [
            "Maohao Shen",
            "Guangtao Zeng",
            "Zhenting Qi",
            "Zhang-Wei Hong",
            "Zhenfang Chen",
            "Wei Lu",
            "Gregory Wornell",
            "Subhro Das",
            "David Cox",
            "Chuang Gan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02508",
        "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "157",
        "title": "Privacy Attacks on Image AutoRegressive Models",
        "author": [
            "Antoni Kowalczuk",
            "Jan DubiÅski",
            "Franziska Boenisch",
            "Adam Dziedzic"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02514",
        "abstract": "Image autoregressive (IAR) models have surpassed diffusion models (DMs) in both image quality (FID: 1.48 vs. 1.58) and generation speed. However, their privacy risks remain largely unexplored. To address this, we conduct a comprehensive privacy analysis comparing IARs to DMs. We develop a novel membership inference attack (MIA) that achieves a significantly higher success rate in detecting training images (TPR@FPR=1%: 86.38% for IARs vs. 4.91% for DMs). Using this MIA, we perform dataset inference (DI) and find that IARs require as few as six samples to detect dataset membership, compared to 200 for DMs, indicating higher information leakage. Additionally, we extract hundreds of training images from an IAR (e.g., 698 from VAR-d30). Our findings highlight a fundamental privacy-utility trade-off: while IARs excel in generation quality and speed, they are significantly more vulnerable to privacy attacks. This suggests that incorporating techniques from DMs, such as per-token probability modeling using diffusion, could help mitigate IARs' privacy risks. Our code is available at https://github.com/sprintml/privacy_attacks_against_iars.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "158",
        "title": "Brief analysis of DeepSeek R1 and it's implications for Generative AI",
        "author": [
            "Sarah Mercer",
            "Samuel Spillard",
            "Daniel P. Martin"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02523",
        "abstract": "In late January 2025, DeepSeek released their new reasoning model (DeepSeek R1); which was developed at a fraction of the cost yet remains competitive with OpenAI's models, despite the US's GPU export ban. This report discusses the model, and what its release means for the field of Generative AI more widely. We briefly discuss other models released from China in recent weeks, their similarities; innovative use of Mixture of Experts (MoE), Reinforcement Learning (RL) and clever engineering appear to be key factors in the capabilities of these models. This think piece has been written to a tight time-scale, providing broad coverage of the topic, and serves as introductory material for those looking to understand the model's technical advancements, as well as it's place in the ecosystem. Several further areas of research are identified.",
        "tags": [
            "DeepSeek",
            "RL"
        ]
    },
    {
        "id": "159",
        "title": "Diff9D: Diffusion-Based Domain-Generalized Category-Level 9-DoF Object Pose Estimation",
        "author": [
            "Jian Liu",
            "Wei Sun",
            "Hui Yang",
            "Pengchao Deng",
            "Chongpei Liu",
            "Nicu Sebe",
            "Hossein Rahmani",
            "Ajmal Mian"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02525",
        "abstract": "Nine-degrees-of-freedom (9-DoF) object pose and size estimation is crucial for enabling augmented reality and robotic manipulation. Category-level methods have received extensive research attention due to their potential for generalization to intra-class unknown objects. However, these methods require manual collection and labeling of large-scale real-world training data. To address this problem, we introduce a diffusion-based paradigm for domain-generalized category-level 9-DoF object pose estimation. Our motivation is to leverage the latent generalization ability of the diffusion model to address the domain generalization challenge in object pose estimation. This entails training the model exclusively on rendered synthetic data to achieve generalization to real-world scenes. We propose an effective diffusion model to redefine 9-DoF object pose estimation from a generative perspective. Our model does not require any 3D shape priors during training or inference. By employing the Denoising Diffusion Implicit Model, we demonstrate that the reverse diffusion process can be executed in as few as 3 steps, achieving near real-time performance. Finally, we design a robotic grasping system comprising both hardware and software components. Through comprehensive experiments on two benchmark datasets and the real-world robotic system, we show that our method achieves state-of-the-art domain generalization performance. Our code will be made public at https://github.com/CNJianLiu/Diff9D.",
        "tags": [
            "3D",
            "Diffusion",
            "Pose Estimation"
        ]
    },
    {
        "id": "160",
        "title": "Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies",
        "author": [
            "Han Zhou",
            "Xingchen Wan",
            "Ruoxi Sun",
            "Hamid Palangi",
            "Shariq Iqbal",
            "Ivan VuliÄ",
            "Anna Korhonen",
            "Sercan Ã. ArÄ±k"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02533",
        "abstract": "Large language models, employed as multiple agents that interact and collaborate with each other, have excelled at solving complex tasks. The agents are programmed with prompts that declare their functionality, along with the topologies that orchestrate interactions across agents. Designing prompts and topologies for multi-agent systems (MAS) is inherently complex. To automate the entire design process, we first conduct an in-depth analysis of the design space aiming to understand the factors behind building effective MAS. We reveal that prompts together with topologies play critical roles in enabling more effective MAS design. Based on the insights, we propose Multi-Agent System Search (MASS), a MAS optimization framework that efficiently exploits the complex MAS design space by interleaving its optimization stages, from local to global, from prompts to topologies, over three stages: 1) block-level (local) prompt optimization; 2) workflow topology optimization; 3) workflow-level (global) prompt optimization, where each stage is conditioned on the iteratively optimized prompts/topologies from former stages. We show that MASS-optimized multi-agent systems outperform a spectrum of existing alternatives by a substantial margin. Based on the MASS-found systems, we finally propose design principles behind building effective multi-agent systems.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "161",
        "title": "Adaptive Self-improvement LLM Agentic System for ML Library Development",
        "author": [
            "Genghan Zhang",
            "Weixin Liang",
            "Olivia Hsu",
            "Kunle Olukotun"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02534",
        "abstract": "ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to $3.9\\times$ over a baseline single LLM.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "162",
        "title": "Flow Q-Learning",
        "author": [
            "Seohong Park",
            "Qiyang Li",
            "Sergey Levine"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02538",
        "abstract": "We present flow Q-learning (FQL), a simple and performant offline reinforcement learning (RL) method that leverages an expressive flow-matching policy to model arbitrarily complex action distributions in data. Training a flow policy with RL is a tricky problem, due to the iterative nature of the action generation process. We address this challenge by training an expressive one-step policy with RL, rather than directly guiding an iterative flow policy to maximize values. This way, we can completely avoid unstable recursive backpropagation, eliminate costly iterative action generation at test time, yet still mostly maintain expressivity. We experimentally show that FQL leads to strong performance across 73 challenging state- and pixel-based OGBench and D4RL tasks in offline RL and offline-to-online RL. Project page: https://seohong.me/projects/fql/",
        "tags": [
            "Flow Matching",
            "RL"
        ]
    },
    {
        "id": "163",
        "title": "LLMs for Generation of Architectural Components: An Exploratory Empirical Study in the Serverless World",
        "author": [
            "Shrikara Arun",
            "Meghana Tedla",
            "Karthik Vaidhyanathan"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02539",
        "abstract": "Recently, the exponential growth in capability and pervasiveness of Large Language Models (LLMs) has led to significant work done in the field of code generation. However, this generation has been limited to code snippets. Going one step further, our desideratum is to automatically generate architectural components. This would not only speed up development time, but would also enable us to eventually completely skip the development phase, moving directly from design decisions to deployment. To this end, we conduct an exploratory study on the capability of LLMs to generate architectural components for Functions as a Service (FaaS), commonly known as serverless functions. The small size of their architectural components make this architectural style amenable for generation using current LLMs compared to other styles like monoliths and microservices. We perform the study by systematically selecting open source serverless repositories, masking a serverless function and utilizing state of the art LLMs provided with varying levels of context information about the overall system to generate the masked function. We evaluate correctness through existing tests present in the repositories and use metrics from the Software Engineering (SE) and Natural Language Processing (NLP) domains to evaluate code quality and the degree of similarity between human and LLM generated code respectively. Along with our findings, we also present a discussion on the path forward for using GenAI in architectural component generation.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "164",
        "title": "OVERTHINKING: Slowdown Attacks on Reasoning LLMs",
        "author": [
            "Abhinav Kumar",
            "Jaechul Roh",
            "Ali Naseh",
            "Marzena Karpinska",
            "Mohit Iyyer",
            "Amir Houmansadr",
            "Eugene Bagdasarian"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02542",
        "abstract": "We increase overhead for applications that rely on reasoning LLMs-we force models to spend an amplified number of reasoning tokens, i.e., \"overthink\", to respond to the user query while providing contextually correct answers. The adversary performs an OVERTHINK attack by injecting decoy reasoning problems into the public content that is used by the reasoning LLM (e.g., for RAG applications) during inference time. Due to the nature of our decoy problems (e.g., a Markov Decision Process), modified texts do not violate safety guardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini) and open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD datasets. Our results show up to 46x slowdown and high transferability of the attack across models. To protect applications, we discuss and implement defenses leveraging LLM-based and system design approaches. Finally, we discuss societal, financial, and energy impacts of OVERTHINK attack which could amplify the costs for third party applications operating reasoning models.",
        "tags": [
            "DeepSeek",
            "LLMs",
            "RAG"
        ]
    },
    {
        "id": "165",
        "title": "Learning the RoPEs: Better 2D and 3D Position Encodings with STRING",
        "author": [
            "Connor Schenck",
            "Isaac Reid",
            "Mithun George Jacob",
            "Alex Bewley",
            "Joshua Ainslie",
            "David Rendleman",
            "Deepali Jain",
            "Mohit Sharma",
            "Avinava Dubey",
            "Ayzaan Wahid",
            "Sumeet Singh",
            "Rene Wagner",
            "Tianli Ding",
            "Chuyuan Fu",
            "Arunkumar Byravan",
            "Jake Varley",
            "Alexey Gritsenko",
            "Matthias Minderer",
            "Dmitry Kalashnikov",
            "Jonathan Tompson",
            "Vikas Sindhwani",
            "Krzysztof Choromanski"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02562",
        "abstract": "We introduce STRING: Separable Translationally Invariant Position Encodings. STRING extends Rotary Position Encodings, a recently proposed and widely used algorithm in large language models, via a unifying theoretical framework. Importantly, STRING still provides exact translation invariance, including token coordinates of arbitrary dimensionality, whilst maintaining a low computational footprint. These properties are especially important in robotics, where efficient 3D token representation is key. We integrate STRING into Vision Transformers with RGB(-D) inputs (color plus optional depth), showing substantial gains, e.g. in open-vocabulary object detection and for robotics controllers. We complement our experiments with a rigorous mathematical analysis, proving the universality of our methods.",
        "tags": [
            "3D",
            "Detection",
            "Large Language Models",
            "Robotics"
        ]
    },
    {
        "id": "166",
        "title": "Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement",
        "author": [
            "Soheil Abbasloo"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02573",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem-solving, a crucial, ubiquitous, and complex domain. This paper explores the proficiency of LLMs in handling Sequential Optimization Problems (SOPs). We introduce WorldGen, a dynamic framework for generating unseen SOPs with controllable complexities, to evaluate LLM performance. Our initial observations reveal that while LLMs perform well on simple SOPs, their performance significantly degrades with increased complexity. Motivated by this, we revisit philosophical hypotheses on reasoning to enhance LLM performance. Inspired by the influential framework of Hegelian Dialectics, we propose ACE, demonstrating how the performance of LLMs in SOP contexts can be significantly improved without any retraining or further fine-tuning.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "167",
        "title": "A comparison of translation performance between DeepL and Supertext",
        "author": [
            "Alex FlÃ¼ckiger",
            "Chantal Amrhein",
            "Tim Graf",
            "Philippe SchlÃ¤pfer",
            "Florian Schottmann",
            "Samuel LÃ¤ubli"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02577",
        "abstract": "As strong machine translation (MT) systems are increasingly based on large language models (LLMs), reliable quality benchmarking requires methods that capture their ability to leverage extended context. This study compares two commercial MT systems -- DeepL and Supertext -- by assessing their performance on unsegmented texts. We evaluate translation quality across four language directions with professional translators assessing segments with full document-level context. While segment-level assessments indicate no strong preference between the systems in most cases, document-level analysis reveals a preference for Supertext in three out of four language directions, suggesting superior consistency across longer texts. We advocate for more context-sensitive evaluation methodologies to ensure that MT quality assessments reflect real-world usability. We release all evaluation data and scripts for further analysis and reproduction at https://github.com/supertext/evaluation_deepl_supertext.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "168",
        "title": "Open Materials Generation with Stochastic Interpolants",
        "author": [
            "Philipp Hoellmer",
            "Thomas Egg",
            "Maya M. Martirossyan",
            "Eric Fuemmeler",
            "Amit Gupta",
            "Zeren Shui",
            "Pawan Prakash",
            "Adrian Roitberg",
            "Mingjie Liu",
            "George Karypis",
            "Mark Transtrum",
            "Richard G. Hennig",
            "Ellad B. Tadmor",
            "Stefano Martiniani"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02582",
        "abstract": "The discovery of new materials is essential for enabling technological advancements. Computational approaches for predicting novel materials must effectively learn the manifold of stable crystal structures within an infinite design space. We introduce Open Materials Generation (OMG), a unifying framework for the generative design and discovery of inorganic crystalline materials. OMG employs stochastic interpolants (SI) to bridge an arbitrary base distribution to the target distribution of inorganic crystals via a broad class of tunable stochastic processes, encompassing both diffusion models and flow matching as special cases. In this work, we adapt the SI framework by integrating an equivariant graph representation of crystal structures and extending it to account for periodic boundary conditions in unit cell representations. Additionally, we couple the SI flow over spatial coordinates and lattice vectors with discrete flow matching for atomic species. We benchmark OMG's performance on two tasks: Crystal Structure Prediction (CSP) for specified compositions, and 'de novo' generation (DNG) aimed at discovering stable, novel, and unique structures. In our ground-up implementation of OMG, we refine and extend both CSP and DNG metrics compared to previous works. OMG establishes a new state-of-the-art in generative modeling for materials discovery, outperforming purely flow-based and diffusion-based implementations. These results underscore the importance of designing flexible deep learning frameworks to accelerate progress in materials science.",
        "tags": [
            "Diffusion",
            "Flow Matching"
        ]
    },
    {
        "id": "169",
        "title": "Calibrated Multi-Preference Optimization for Aligning Diffusion Models",
        "author": [
            "Kyungmin Lee",
            "Xiaohang Li",
            "Qifei Wang",
            "Junfeng He",
            "Junjie Ke",
            "Ming-Hsuan Yang",
            "Irfan Essa",
            "Jinwoo Shin",
            "Feng Yang",
            "Yinxiao Li"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02588",
        "abstract": "Aligning text-to-image (T2I) diffusion models with preference optimization is valuable for human-annotated datasets, but the heavy cost of manual data collection limits scalability. Using reward models offers an alternative, however, current preference optimization methods fall short in exploiting the rich information, as they only consider pairwise preference distribution. Furthermore, they lack generalization to multi-preference scenarios and struggle to handle inconsistencies between rewards. To address this, we present Calibrated Preference Optimization (CaPO), a novel method to align T2I diffusion models by incorporating the general preference from multiple reward models without human annotated data. The core of our approach involves a reward calibration method to approximate the general preference by computing the expected win-rate against the samples generated by the pretrained models. Additionally, we propose a frontier-based pair selection method that effectively manages the multi-preference distribution by selecting pairs from Pareto frontiers. Finally, we use regression loss to fine-tune diffusion models to match the difference between calibrated rewards of a selected pair. Experimental results show that CaPO consistently outperforms prior methods, such as Direct Preference Optimization (DPO), in both single and multi-reward settings validated by evaluation on T2I benchmarks, including GenEval and T2I-Compbench.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "170",
        "title": "COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for Fine-Grained Understanding and Generation",
        "author": [
            "Xueqing Deng",
            "Qihang Yu",
            "Ali Athar",
            "Chenglin Yang",
            "Linjie Yang",
            "Xiaojie Jin",
            "Xiaohui Shen",
            "Liang-Chieh Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02589",
        "abstract": "This paper introduces the COCONut-PanCap dataset, created to enhance panoptic segmentation and grounded image captioning. Building upon the COCO dataset with advanced COCONut panoptic masks, this dataset aims to overcome limitations in existing image-text datasets that often lack detailed, scene-comprehensive descriptions. The COCONut-PanCap dataset incorporates fine-grained, region-level captions grounded in panoptic segmentation masks, ensuring consistency and improving the detail of generated captions. Through human-edited, densely annotated descriptions, COCONut-PanCap supports improved training of vision-language models (VLMs) for image understanding and generative models for text-to-image tasks. Experimental results demonstrate that COCONut-PanCap significantly boosts performance across understanding and generation tasks, offering complementary benefits to large-scale datasets. This dataset sets a new benchmark for evaluating models on joint panoptic segmentation and grounded captioning tasks, addressing the need for high-quality, detailed image-text annotations in multi-modal learning.",
        "tags": [
            "Segmentation",
            "Text-to-Image"
        ]
    },
    {
        "id": "171",
        "title": "FinRLlama: A Solution to LLM-Engineered Signals Challenge at FinRL Contest 2024",
        "author": [
            "Arnav Grover"
        ],
        "pdf": "https://arxiv.org/pdf/2502.01992",
        "abstract": "In response to Task II of the FinRL Challenge at ACM ICAIF 2024, this study proposes a novel prompt framework for fine-tuning large language models (LLM) with Reinforcement Learning from Market Feedback (RLMF). Our framework incorporates market-specific features and short-term price dynamics to generate more precise trading signals. Traditional LLMs, while competent in sentiment analysis, lack contextual alignment for financial market applications. To bridge this gap, we fine-tune the LLaMA-3.2-3B-Instruct model using a custom RLMF prompt design that integrates historical market data and reward-based feedback. Our evaluation shows that this RLMF-tuned framework outperforms baseline methods in signal consistency and achieving tighter trading outcomes; awarded as winner of Task II. You can find the code for this project on GitHub.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "172",
        "title": "SurvHive: a package to consistently access multiple survival-analysis packages",
        "author": [
            "Giovanni Birolo",
            "Ivan Rossi",
            "Flavio Sartori",
            "Cesare Rollo",
            "Tiziana Sanavia",
            "Piero Fariselli"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02223",
        "abstract": "Survival analysis, a foundational tool for modeling time-to-event data, has seen growing integration with machine learning (ML) approaches to handle the complexities of censored data and time-varying risks. Despite these advances, leveraging state-of-the-art survival models remains a challenge due to the fragmented nature of existing implementations, which lack standardized interfaces and require extensive preprocessing. We introduce SurvHive, a Python-based framework designed to unify survival analysis methods within a coherent and extensible interface modeled on scikit-learn. SurvHive integrates classical statistical models with cutting-edge deep learning approaches, including transformer-based architectures and parametric survival models. Using a consistent API, SurvHive simplifies model training, evaluation, and optimization, significantly reducing the barrier to entry for ML practitioners exploring survival analysis. The package includes enhanced support for hyper-parameter tuning, time-dependent risk evaluation metrics, and cross-validation strategies tailored to censored data. With its extensibility and focus on usability, SurvHive provides a bridge between survival analysis and the broader ML community, facilitating advancements in time-to-event modeling across domains. The SurvHive code and documentation are available freely at https://github.com/compbiomed-unito/survhive.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "173",
        "title": "Asymptotic solution for three-dimensional reaction-diffusion-advection equation with periodic boundary conditions",
        "author": [
            "Aleksei Liubavin",
            "Mingkang Ni",
            "Ye Zhang",
            "Dmitrii Chaikovskii"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02263",
        "abstract": "In this study, we investigate the dynamics of moving fronts in three-dimensional spaces, which form as a result of in-situ combustion during oil production. This phenomenon is also observed in other contexts, such as various autowave models and the propagation of acoustic waves. Our analysis involves a singularly perturbed reaction-diffusion-advection type initial-boundary value problem of a general form. We employ methods from asymptotic theory to develop an approximate smooth solution with an internal layer. Using local coordinates, we focus on the transition layer, where the solution undergoes rapid changes. Once the location of the transition layer is established, we can describe the solution across the full domain of the problem. Numerical examples are provided, demonstrating the high accuracy of the asymptotic method in predicting the behaviors of moving fronts.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "174",
        "title": "Information-Theoretic Proofs for Diffusion Sampling",
        "author": [
            "Galen Reeves",
            "Henry D. Pfister"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02305",
        "abstract": "This paper provides an elementary, self-contained analysis of diffusion-based sampling methods for generative modeling. In contrast to existing approaches that rely on continuous-time processes and then discretize, our treatment works directly with discrete-time stochastic processes and yields precise non-asymptotic convergence guarantees under broad assumptions. The key insight is to couple the sampling process of interest with an idealized comparison process that has an explicit Gaussian-convolution structure. We then leverage simple identities from information theory, including the I-MMSE relationship, to bound the discrepancy (in terms of the Kullback-Leibler divergence) between these two discrete-time processes. In particular, we show that, if the diffusion step sizes are chosen sufficiently small and one can approximate certain conditional mean estimators well, then the sampling distribution is provably close to the target distribution. Our results also provide a transparent view on how to accelerate convergence by introducing additional randomness in each step to match higher order moments in the comparison process.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "175",
        "title": "Distribution Transformers: Fast Approximate Bayesian Inference With On-The-Fly Prior Adaptation",
        "author": [
            "George Whittle",
            "Juliusz Ziomek",
            "Jacob Rawling",
            "Michael A Osborne"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02463",
        "abstract": "While Bayesian inference provides a principled framework for reasoning under uncertainty, its widespread adoption is limited by the intractability of exact posterior computation, necessitating the use of approximate inference. However, existing methods are often computationally expensive, or demand costly retraining when priors change, limiting their utility, particularly in sequential inference problems such as real-time sensor fusion. To address these challenges, we introduce the Distribution Transformer -- a novel architecture that can learn arbitrary distribution-to-distribution mappings. Our method can be trained to map a prior to the corresponding posterior, conditioned on some dataset -- thus performing approximate Bayesian inference. Our novel architecture represents a prior distribution as a (universally-approximating) Gaussian Mixture Model (GMM), and transforms it into a GMM representation of the posterior. The components of the GMM attend to each other via self-attention, and to the datapoints via cross-attention. We demonstrate that Distribution Transformers both maintain flexibility to vary the prior, and significantly reduces computation times-from minutes to milliseconds-while achieving log-likelihood performance on par with or superior to existing approximate inference methods across tasks such as sequential inference, quantum system parameter inference, and Gaussian Process predictive posterior inference with hyperpriors.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "176",
        "title": "SDE Matching: Scalable and Simulation-Free Training of Latent Stochastic Differential Equations",
        "author": [
            "Grigory Bartosh",
            "Dmitry Vetrov",
            "Christian A. Naesseth"
        ],
        "pdf": "https://arxiv.org/pdf/2502.02472",
        "abstract": "The Latent Stochastic Differential Equation (SDE) is a powerful tool for time series and sequence modeling. However, training Latent SDEs typically relies on adjoint sensitivity methods, which depend on simulation and backpropagation through approximate SDE solutions, which limit scalability. In this work, we propose SDE Matching, a new simulation-free method for training Latent SDEs. Inspired by modern Score- and Flow Matching algorithms for learning generative dynamics, we extend these ideas to the domain of stochastic dynamics for time series and sequence modeling, eliminating the need for costly numerical simulations. Our results demonstrate that SDE Matching achieves performance comparable to adjoint sensitivity methods while drastically reducing computational complexity.",
        "tags": [
            "Flow Matching",
            "SDE"
        ]
    }
]